{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set the seed for reproducibility across multiple libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "train_path = \"../data/absa/id/william/train.txt\"\n",
    "val_path = \"../data/absa/id/william/dev.txt\"\n",
    "test_path = \"../data/absa/id/william/test.txt\"\n",
    "\n",
    "train = data_utils.read_data(train_path)\n",
    "val = data_utils.read_data(val_path)\n",
    "test = data_utils.read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = [\n",
    "    {\n",
    "        \"paradigm\" : \"extraction\",\n",
    "        \"se_order\" : \"oa\",\n",
    "        \"method\" : \"lego_absa\"\n",
    "    },\n",
    "    {\n",
    "        \"paradigm\" : \"extraction\",\n",
    "        \"se_order\" : \"as\",\n",
    "        \"method\" : \"lego_absa\"\n",
    "    },\n",
    "    {\n",
    "        \"paradigm\" : \"imputation\",\n",
    "        \"reduced_se_order\" : \"oa\",\n",
    "        \"se_order\" : \"oas\",\n",
    "        \"method\" : \"lego_absa\"\n",
    "    },\n",
    "    {\n",
    "        \"paradigm\" : \"imputation\",\n",
    "        \"reduced_se_order\" : \"as\",\n",
    "        \"se_order\" : \"oas\",\n",
    "        \"method\" : \"lego_absa\"\n",
    "    },\n",
    "]\n",
    "\n",
    "val_tasks = [\n",
    "    {\n",
    "        \"paradigm\" : \"extraction\",\n",
    "        \"se_order\" : \"oas\",\n",
    "        \"method\" : \"lego_absa\"\n",
    "    }\n",
    "]\n",
    "\n",
    "test_tasks = [\n",
    "    {\n",
    "        \"paradigm\" : \"extraction\",\n",
    "        \"se_order\" : \"oas\",\n",
    "        \"method\" : \"lego_absa\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [00:05<00:00, 2171.51it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 14073.33it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 14044.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = data_utils.data_gen(data=train, nt_se_order=\"aos\", tasks=train_tasks, n_fold=4, algo=\"random\", shuffle=True)\n",
    "val_ds = data_utils.data_gen(data=val, nt_se_order=\"aos\", tasks=val_tasks, n_fold=1, algo=\"round_robin\", shuffle=False)\n",
    "test_ds = data_utils.data_gen(data=test, nt_se_order=\"aos\", tasks=test_tasks, n_fold=1, algo=\"round_robin\", shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'pngen kembali lagi buat menginap .| opinion : <extra_id_0> ,aspect : <extra_id_1>', 'output': 'NULL', 'se_order': 'oa'}\n",
      "{'input': 'pngen kembali lagi buat menginap .| aspect : <extra_id_0> ,sentiment : <extra_id_1>', 'output': 'NULL', 'se_order': 'as'}\n"
     ]
    }
   ],
   "source": [
    "for el in train_ds:\n",
    "    if el[\"input\"].startswith(\"pngen kembali lagi buat menginap\"):\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tempatnya , harga , dan pelayanan sesuai dengan harga .| opinion : sesuai ,aspect : tempatnya ,sentiment : <extra_id_0> ; opinion : sesuai ,aspect : harga ,sentiment : <extra_id_1> ; opinion : sesuai ,aspect : pelayanan ,sentiment : <extra_id_2>',\n",
       " 'output': '<extra_id_0> sesuai <extra_id_1> tempatnya <extra_id_2> positive ; <extra_id_3> sesuai <extra_id_4> harga <extra_id_5> positive ; <extra_id_6> sesuai <extra_id_7> pelayanan <extra_id_8> positive',\n",
       " 'se_order': 'oas'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_list(train_ds)\n",
    "val_ds = Dataset.from_list(val_ds)\n",
    "test_ds = Dataset.from_list(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'se_order'],\n",
       "    num_rows: 8022\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'se_order'],\n",
       "    num_rows: 998\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'se_order'],\n",
       "    num_rows: 995\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 14:21:42.690312: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 14:21:42.957241: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-26 14:21:42.957268: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-26 14:21:44.288402: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-26 14:21:44.288508: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-26 14:21:44.288520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/randy/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "encoding_args = {\n",
    "    \"max_length\" : 256,\n",
    "    \"padding\" : True,\n",
    "    \"truncation\" : True,\n",
    "    \"return_tensors\" : \"pt\"\n",
    "}\n",
    "\n",
    "encode_fn = lambda x: tokenizer(x[\"input\"], text_target=x[\"output\"], **encoding_args)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8a4e1324914b57a0ff81c1d4c4c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62acf67a4ca44360a8003b5007b40f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbefff36d0a43bda40513e185aadf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tok = train_ds.map(encode_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "train_tok.set_format(\"torch\")\n",
    "\n",
    "val_tok = val_ds.map(encode_fn, batched=True, remove_columns=val_ds.column_names)\n",
    "val_tok.set_format(\"torch\")\n",
    "\n",
    "test_tok = test_ds.map(encode_fn, batched=True, remove_columns=test_ds.column_names)\n",
    "test_tok.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_strategy\" : \"epoch\",\n",
    "    \"metric_for_best_model\": \"overall_f1_score\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"adam_epsilon\": 1e-08,\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"logging_dir\" : \"./output/log\",\n",
    "    \"include_inputs_for_metrics\" : True\n",
    "}\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(**train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import compute_metrics\n",
    "\n",
    "catch_answer_fn = data_utils.AnswerCatcher().lego_absa\n",
    "decoding_args = {\n",
    "    \"skip_special_tokens\" : False\n",
    "}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = data_collator,\n",
    "        train_dataset = train_tok,\n",
    "        eval_dataset = val_tok,\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(catch_answer_fn, eval_preds, decoding_args, tokenizer, val_ds[\"se_order\"]),\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
