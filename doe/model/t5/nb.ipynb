{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "sys.path.append(\"../../src/\")\n",
    "import data_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_dir = dict(\n",
    "    lap14 = \"../../data/absa/en/peng/14lap\",\n",
    "    res14 = \"../../data/absa/en/peng/14res\",\n",
    "    res15 = \"../../data/absa/en/peng/15res\",\n",
    "    res16 = \"../../data/absa/en/peng/16res\"\n",
    ")\n",
    "\n",
    "wan_dir = dict(\n",
    "    res15 = \"../../data/absa/en/wan/interim/rest15\",\n",
    "    res16 = \"../../data/absa/en/wan/interim/rest16\"\n",
    ")\n",
    "    \n",
    "zhang_dir = dict(\n",
    "    res15 = \"../../data/absa/en/zhang/interim/interim_2/rest15\",\n",
    "    res16 = \"../../data/absa/en/zhang/interim/interim_2/rest16\"\n",
    ")\n",
    "\n",
    "william_dir = dict(\n",
    "    hotel = \"../../data/absa/id/william\"\n",
    ")\n",
    "\n",
    "peng = dict(\n",
    "    lap14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res15\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res15\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res15\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res16\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res16\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res16\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")\n",
    "\n",
    "wan = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    )\n",
    ")\n",
    "\n",
    "zhang = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    )\n",
    ")\n",
    "\n",
    "william = dict(\n",
    "    hotel = dict(\n",
    "        train = data_utils.read_data(path=william_dir[\"hotel\"] + \"/train.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=william_dir[\"hotel\"] + \"/dev.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=william_dir[\"hotel\"] + \"/test.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.SENTIMENT_ELEMENT = {'a' : \"aspect\", 'o' : \"opinion\", 's' : \"sentiment\", 'c' : \"category\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AOS (ASTE)\n",
    "    * AO\n",
    "    * AS\n",
    "    * A\n",
    "    * O\n",
    "\n",
    "2. ACS (TASD)\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * C\n",
    "\n",
    "3. ACOS\n",
    "    * AO\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * O\n",
    "    * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oas', 'oa', 'as', 'a', 'o', 'asc', 'sc', 'c', 'oasc']\n"
     ]
    }
   ],
   "source": [
    "task_tree = {\n",
    "    \"oas\" : [\"oa\",\"as\",'a','o'],\n",
    "    \"asc\" : [\"as\",\"sc\",'a','c'],\n",
    "    \"oasc\" : [\"oa\",\"as\",\"sc\",'a','o','c']\n",
    "}\n",
    "\n",
    "all_task = []\n",
    "for k,v1 in task_tree.items():\n",
    "    if k not in all_task:\n",
    "        all_task.append(k)\n",
    "    for v2 in v1:\n",
    "        if v2 not in all_task:\n",
    "            all_task.append(v2)\n",
    "\n",
    "print(all_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.remove_duplicate_targets(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"ao\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle mix may not be a must, but we'll see it later. Will be problematic if like as (UABSA / E2E ABSA) used for training AOS (ASTE) --> may be for further experiment because we will insert imputing later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good', 'sentiment': 'mixed'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.handle_mix_sentiment(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"aos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Peng (ASTE/AOS)\n",
    "peng_intermediate = dict()\n",
    "\n",
    "for domain, v1 in peng.items():\n",
    "    peng_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        peng_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = peng[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            peng_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wan (TASD/ACS)\n",
    "wan_intermediate = dict()\n",
    "\n",
    "for domain, v1 in wan.items():\n",
    "    wan_intermediate[domain] = dict()\n",
    "    for task in [\"asc\"] + task_tree[\"asc\"]:\n",
    "        wan_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = wan[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            wan_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang (ACOS)\n",
    "zhang_intermediate = dict()\n",
    "\n",
    "for domain, v1 in zhang.items():\n",
    "    zhang_intermediate[domain] = dict()\n",
    "    for task in [\"oasc\"] + task_tree[\"oasc\"]:\n",
    "        zhang_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = zhang[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            zhang_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William (AOS ID)\n",
    "william_intermediate = dict()\n",
    "\n",
    "for domain, v1 in william.items():\n",
    "    william_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        william_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = william[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            william_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"<extra_id_X>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_answer(targets,se_order):\n",
    "    result = []\n",
    "    counter = 0\n",
    "    for t in targets:\n",
    "        constructed_t = \"\"\n",
    "        for se in se_order:\n",
    "            if counter > 99:\n",
    "                raise Exception(\"Extra id more than 99!\")\n",
    "            constructed_t += ' ' + mask.replace('X',str(counter)) + ' ' + t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "            counter += 1\n",
    "        constructed_t = constructed_t.strip()\n",
    "        result.append(constructed_t)\n",
    "    result = \" ; \".join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> no <extra_id_1> GUI <extra_id_2> negative ; <extra_id_3> dark <extra_id_4> screen <extra_id_5> negative ; <extra_id_6> steady <extra_id_7> power light <extra_id_8> neutral ; <extra_id_9> steady <extra_id_10> hard drive light <extra_id_11> negative'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(text,se_order):\n",
    "    prompt = []\n",
    "    for counter, se in enumerate(se_order):\n",
    "        prompt.append(data_utils.SENTIMENT_ELEMENT[se] + \" : \" + mask.replace('X',str(counter)))\n",
    "    prompt = \" ,\".join(prompt)\n",
    "    result = text + \"| \" + prompt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One night I turned the freaking thing off after using it , the next day I turn it on , no GUI , screen all dark , power light steady , hard drive light steady and not flashing as it usually does .| opinion : <extra_id_0> ,aspect : <extra_id_1> ,sentiment : <extra_id_2>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_prompt(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"text\"],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def catch_answer(output,se_order):\n",
    "    output = output.replace(\"<pad>\",'')\n",
    "    output = output.replace(\"</s>\",'')\n",
    "    pattern = r\"\"\n",
    "    for se in se_order:\n",
    "        if se != 's':\n",
    "            pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
    "        else:\n",
    "            pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
    "    found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "    for i in range(len(found)):\n",
    "        for k, v in found[i].items():\n",
    "            found[i][k] = found[i][k].strip()\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'opinion': 'no', 'aspect': 'GUI', 'sentiment': 'negative'},\n",
       " {'opinion': 'dark', 'aspect': 'screen', 'sentiment': 'negative'},\n",
       " {'opinion': 'steady', 'aspect': 'power light', 'sentiment': 'neutral'},\n",
       " {'opinion': 'steady', 'aspect': 'hard drive light', 'sentiment': 'negative'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")\n",
    "se_order = \"oas\"\n",
    "catch_answer(output,se_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> no <extra_id_1> GUI <extra_id_2> negative ; <extra_id_3> dark <extra_id_4> screen <extra_id_5> negative ; <extra_id_6> steady <extra_id_7> power light <extra_id_8> neutral ; <extra_id_9> steady <extra_id_10> hard drive light <extra_id_11> negative'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "peng_2 = dict()\n",
    "for domain, v1 in peng_intermediate.items():\n",
    "    peng_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in peng_intermediate[domain][basic_task][\"train\"]:\n",
    "            peng_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        peng_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        peng_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    peng_2[domain][\"train\"] = Dataset.from_list(peng_2[domain][\"train\"])\n",
    "    peng_2[domain][\"val\"] = Dataset.from_list(peng_2[domain][\"val\"])\n",
    "    peng_2[domain][\"test\"] = Dataset.from_list(peng_2[domain][\"test\"])\n",
    "\n",
    "wan_2 = dict()\n",
    "for domain, v1 in wan_intermediate.items():\n",
    "    wan_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"asc\"]:\n",
    "        for el in wan_intermediate[domain][basic_task][\"train\"]:\n",
    "            wan_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"val\"]:\n",
    "        wan_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"test\"]:\n",
    "        wan_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    wan_2[domain][\"train\"] = Dataset.from_list(wan_2[domain][\"train\"])\n",
    "    wan_2[domain][\"val\"] = Dataset.from_list(wan_2[domain][\"val\"])\n",
    "    wan_2[domain][\"test\"] = Dataset.from_list(wan_2[domain][\"test\"])\n",
    "\n",
    "zhang_2 = dict()\n",
    "for domain, v1 in zhang_intermediate.items():\n",
    "    zhang_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oasc\"]:\n",
    "        for el in zhang_intermediate[domain][basic_task][\"train\"]:\n",
    "            zhang_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"val\"]:\n",
    "        zhang_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"test\"]:\n",
    "        zhang_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    zhang_2[domain][\"train\"] = Dataset.from_list(zhang_2[domain][\"train\"])\n",
    "    zhang_2[domain][\"val\"] = Dataset.from_list(zhang_2[domain][\"val\"])\n",
    "    zhang_2[domain][\"test\"] = Dataset.from_list(zhang_2[domain][\"test\"])\n",
    "\n",
    "william_2 = dict()\n",
    "for domain, v1 in william_intermediate.items():\n",
    "    william_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in william_intermediate[domain][basic_task][\"train\"]:\n",
    "            william_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in william_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        william_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in william_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        william_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    william_2[domain][\"train\"] = Dataset.from_list(william_2[domain][\"train\"])\n",
    "    william_2[domain][\"val\"] = Dataset.from_list(william_2[domain][\"val\"])\n",
    "    william_2[domain][\"test\"] = Dataset.from_list(william_2[domain][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tempat yag bagus dan nyaman untuk istirahat tetapi tolong tvnya perlu di perbaiki channelnya karena banyak semutnya digambar dan water heaternya tidak bisa jadi mandi air dingin terus .| opinion : <extra_id_0> ,aspect : <extra_id_1>',\n",
       " 'output': '<extra_id_0> bagus <extra_id_1> tempat ; <extra_id_2> nyaman <extra_id_3> tempat ; <extra_id_4> perlu di perbaiki <extra_id_5> tvnya ; <extra_id_6> tidak bisa <extra_id_7> water heaternya',\n",
       " 'task': 'oa'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "william_2[\"hotel\"][\"train\"][69]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Tokenized Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_args = {\n",
    "    \"max_length\" : 512,\n",
    "    \"padding\" : True,\n",
    "    \"truncation\" : True,\n",
    "    \"return_tensors\" : \"pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_en(dataset):\n",
    "    result = tokenizer_en(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "peng_tok = dict()\n",
    "for domain, v1 in peng_2.items():\n",
    "    peng_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            peng_tok[domain][split] = peng_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            peng_tok[domain][split] = encode_en(peng_2[domain][split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "wan_tok = dict()\n",
    "for domain, v1 in wan_2.items():\n",
    "    wan_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            wan_tok[domain][split] = wan_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            wan_tok[domain][split] = encode_en(wan_2[domain][split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "zhang_tok = dict()\n",
    "for domain, v1 in zhang_2.items():\n",
    "    zhang_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            zhang_tok[domain][split] = zhang_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            zhang_tok[domain][split] = encode_en(zhang_2[domain][split])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_id(dataset):\n",
    "    result = tokenizer_id(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "william_tok = dict()\n",
    "for domain, v1 in william_2.items():\n",
    "    william_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            william_tok[domain][split] = william_2[domain][split].map(encode_id,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            william_tok[domain][split] = encode_id(william_2[domain][split])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator_en = DataCollatorForSeq2Seq(tokenizer=tokenizer_en)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_id = DataCollatorForSeq2Seq(tokenizer=tokenizer_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from evaluation import recall, precision, f1_score, summary_score\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def seperate_target_prediction_per_task(predictions:List[List[Dict]],targets:List[List[Dict]],tasks:List) -> Tuple[Dict[str,List],Dict[str,List]]:\n",
    "    per_task_targets = {}\n",
    "    per_task_predictions = {}\n",
    "    for target, prediction, task in zip(targets,predictions,tasks):\n",
    "        if task not in per_task_targets.keys():\n",
    "            per_task_targets[task] = []\n",
    "        if task not in per_task_predictions.keys():\n",
    "            per_task_predictions[task] = []\n",
    "        per_task_targets[task].append(target)\n",
    "        per_task_predictions[task].append(prediction)\n",
    "    return per_task_targets, per_task_predictions\n",
    "\n",
    "def preprocess_eval_preds(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer):\n",
    "    input_ids = eval_preds.inputs\n",
    "    target_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(input_ids, tuple):\n",
    "        input_ids = input_ids[0]\n",
    "    if isinstance(target_ids, tuple):\n",
    "        target_ids = target_ids[0]\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    input_ids = np.argmax(input_ids,axis=-1) if len(input_ids.shape) == 3 else input_ids # in case not predict with generate\n",
    "    target_ids = np.argmax(target_ids,axis=-1) if len(target_ids.shape) == 3 else target_ids # in case not predict with generate\n",
    "    prediction_ids = np.argmax(pred_ids,axis=-1) if len(pred_ids.shape) == 3 else pred_ids # in case not predict with generate\n",
    "\n",
    "    input_ids = [[token for token in row if token != -100] for row in input_ids]\n",
    "    target_ids = [[token for token in row if token != -100] for row in target_ids]\n",
    "    prediction_ids = [[token for token in row if token != -100] for row in prediction_ids]\n",
    "\n",
    "    inputs = tokenizer.batch_decode(input_ids,**decoding_args)\n",
    "    targets = tokenizer.batch_decode(target_ids,**decoding_args)\n",
    "    predictions = tokenizer.batch_decode(prediction_ids,**decoding_args)\n",
    "\n",
    "    return inputs, targets, predictions\n",
    "\n",
    "def compute_metrics(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer,tasks:List) -> Dict[str,float]: # MAY NOT BE SUFFICIATE FOR CAUSAL LM\n",
    "        \"\"\"\n",
    "        ### DESC\n",
    "            Method to compute the metrics.\n",
    "        ### PARAMS\n",
    "        * eval_preds: EvalPrediction instance from training.\n",
    "        * decoding_args: Decoding arguments.\n",
    "        ### RETURN\n",
    "        * metrics: Dictionary of metrics.\n",
    "        \"\"\"\n",
    "        inputs, targets, predictions = preprocess_eval_preds(eval_preds,decoding_args,tokenizer)\n",
    "\n",
    "        targets = [catch_answer(text,task) for text,task in zip(targets,tasks) if task != \"non_absa\"]\n",
    "        predictions = [catch_answer(text,task) for text,task in zip(predictions,tasks) if task != \"non_absa\"]\n",
    "\n",
    "\n",
    "        per_task_targets, per_task_predictions = seperate_target_prediction_per_task(predictions, targets, tasks)\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        metrics[\"overall_recall\"] = recall(predictions,targets)\n",
    "        metrics[\"overall_precision\"] = precision(predictions,targets)\n",
    "        metrics[\"overall_f1_score\"] = f1_score(predictions,targets)\n",
    "\n",
    "        for task in per_task_targets.keys():\n",
    "            if task == \"non_absa\":\n",
    "                continue\n",
    "            metrics[f\"{task}_recall\"] = recall(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_precision\"] = precision(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_f1_score\"] = f1_score(per_task_predictions[task],per_task_targets[task])\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_strategy\" : \"epoch\",\n",
    "    \"metric_for_best_model\": \"overall_f1_score\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"adam_epsilon\": 1e-08,\n",
    "    \"output_dir\": \"./t5\",\n",
    "    \"logging_dir\" : \"./t5/log\",\n",
    "    \"include_inputs_for_metrics\" : True\n",
    "}\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(**train_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# trainer = {\n",
    "#     \"peng\" : {},\n",
    "#     \"wan\" : {},\n",
    "#     \"zhang\" : {},\n",
    "#     \"william\" : {}\n",
    "# }\n",
    "\n",
    "decoding_args = {\n",
    "    \"skip_special_tokens\" : False\n",
    "}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, targets):\n",
    "    pred_logits = logits[0] if isinstance(logits,tuple) else logits\n",
    "    pred_ids = torch.argmax(pred_logits, dim=-1)\n",
    "    return pred_ids, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_predictions(model,tokenizer,tokenized:torch.Tensor,device:torch.device=torch.device(\"cpu\"),batch_size:int=16,max_len:int=512,decoding_args:Dict={}) -> List[str]:\n",
    "    # Data loader\n",
    "    input_ids_data_loader = torch.utils.data.DataLoader(tokenized[\"input_ids\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    attention_mask_data_loader = torch.utils.data.DataLoader(tokenized[\"attention_mask\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    # Predict\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    tensor_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(zip(input_ids_data_loader,attention_mask_data_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            tensor_predictions.extend(model.generate(input_ids=input_ids,attention_mask=attention_mask,max_length=max_len,pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id).cpu())\n",
    "            input_ids = input_ids.cpu()\n",
    "            attention_mask = attention_mask.cpu()\n",
    "    tensor_predictions = [[token for token in row if token != -100] for row in tensor_predictions]\n",
    "    predictions = tokenizer.batch_decode(tensor_predictions,**decoding_args)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_result(str_preds_,preds,targets,filename):\n",
    "    result = []\n",
    "    str_preds = [el.replace(\"<pad>\",'').replace(\"</s>\",'') for el in str_preds_]\n",
    "    assert len(str_preds) == len(preds) == len(targets)\n",
    "    for i in range(len(str_preds)):\n",
    "        result.append({\n",
    "            \"str_pred\" : str_preds[i],\n",
    "            \"pred\" : preds[i],\n",
    "            \"target\" : targets[i]\n",
    "        })\n",
    "    \n",
    "    with open(filename,'w') as fp:\n",
    "        json.dump(result,fp)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Laptop 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3624\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2280\n",
      "  Number of trainable parameters = 222903552\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  32/2280 00:11 < 14:09, 2.65 it/s, Epoch 0.27/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-114\n",
      "Configuration saved in ./t5/checkpoint-114/config.json\n",
      "Model weights saved in ./t5/checkpoint-114/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-114/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-114/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-114/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-228\n",
      "Configuration saved in ./t5/checkpoint-228/config.json\n",
      "Model weights saved in ./t5/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-228/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-228/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-342\n",
      "Configuration saved in ./t5/checkpoint-342/config.json\n",
      "Model weights saved in ./t5/checkpoint-342/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-342/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-342/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-342/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-114] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-456\n",
      "Configuration saved in ./t5/checkpoint-456/config.json\n",
      "Model weights saved in ./t5/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-456/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-456/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-228] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-570\n",
      "Configuration saved in ./t5/checkpoint-570/config.json\n",
      "Model weights saved in ./t5/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-570/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-570/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-342] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-684\n",
      "Configuration saved in ./t5/checkpoint-684/config.json\n",
      "Model weights saved in ./t5/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-684/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-684/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-456] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-798\n",
      "Configuration saved in ./t5/checkpoint-798/config.json\n",
      "Model weights saved in ./t5/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-798/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-798/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-570] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-912\n",
      "Configuration saved in ./t5/checkpoint-912/config.json\n",
      "Model weights saved in ./t5/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-912/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-912/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-798] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1026\n",
      "Configuration saved in ./t5/checkpoint-1026/config.json\n",
      "Model weights saved in ./t5/checkpoint-1026/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1026/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1026/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1026/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-912] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1140\n",
      "Configuration saved in ./t5/checkpoint-1140/config.json\n",
      "Model weights saved in ./t5/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1026] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1254\n",
      "Configuration saved in ./t5/checkpoint-1254/config.json\n",
      "Model weights saved in ./t5/checkpoint-1254/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1254/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1254/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1254/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1368\n",
      "Configuration saved in ./t5/checkpoint-1368/config.json\n",
      "Model weights saved in ./t5/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1368/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1368/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1254] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1482\n",
      "Configuration saved in ./t5/checkpoint-1482/config.json\n",
      "Model weights saved in ./t5/checkpoint-1482/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1482/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1482/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1482/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1368] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1596\n",
      "Configuration saved in ./t5/checkpoint-1596/config.json\n",
      "Model weights saved in ./t5/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1596/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1596/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-684] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1710\n",
      "Configuration saved in ./t5/checkpoint-1710/config.json\n",
      "Model weights saved in ./t5/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1710/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1710/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1482] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1938\n",
      "Configuration saved in ./t5/checkpoint-1938/config.json\n",
      "Model weights saved in ./t5/checkpoint-1938/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1938/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1938/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1938/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1824] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2280, training_loss=0.03503607097817095, metrics={'train_runtime': 982.6653, 'train_samples_per_second': 73.759, 'train_steps_per_second': 2.32, 'total_flos': 8965382543769600.0, 'train_loss': 0.03503607097817095, 'epoch': 20.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"lap14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"lap14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"lap14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:08,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"lap14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"lap14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5027726432532348,\n",
       " 'precision': 0.6570048309178744,\n",
       " 'f1_score': 0.5696335078534032}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'opinion': 'fast', 'aspect': 'Boot time', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not fix', 'aspect': 'tech support', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'easy', 'aspect': 'Set up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not enjoy', 'aspect': 'Windows 8', 'sentiment': 'negative'},\n",
       "  {'opinion': 'not enjoy',\n",
       "   'aspect': 'touchscreen functions',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'lousy',\n",
       "   'aspect': 'internal speakers',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'fast', 'aspect': 'use', 'sentiment': 'positive'},\n",
       "  {'opinion': 'light', 'aspect': 'use', 'sentiment': 'positive'},\n",
       "  {'opinion': 'simple', 'aspect': 'use', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'well', 'aspect': 'Works', 'sentiment': 'positive'},\n",
       "  {'opinion': 'happy', 'aspect': 'apple OS', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not light and slim',\n",
       "   'aspect': 'features',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'pleased', 'aspect': 'log on', 'sentiment': 'positive'},\n",
       "  {'opinion': 'fast', 'aspect': 'log on', 'sentiment': 'positive'},\n",
       "  {'opinion': 'pleased', 'aspect': 'WiFi connection', 'sentiment': 'positive'},\n",
       "  {'opinion': 'speedy', 'aspect': 'WiFi connection', 'sentiment': 'positive'},\n",
       "  {'opinion': 'pleased', 'aspect': 'battery life', 'sentiment': 'positive'},\n",
       "  {'opinion': 'long', 'aspect': 'battery life', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not yet discovered',\n",
       "   'aspect': 'delete key',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'difficult', 'aspect': 'interneting', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'right', 'aspect': 'priced', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not very good',\n",
       "   'aspect': 'track pad',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'outstanding', 'aspect': 'graphics', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'slow', 'aspect': 'mountain lion', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'Strong', 'aspect': 'durability', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Strong', 'aspect': 'build', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'excellent', 'aspect': 'battery life', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'perfectly', 'aspect': 'works', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'liking', 'aspect': 'Windows 8', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'longer', 'aspect': 'baterry', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'ideal', 'aspect': 'size', 'sentiment': 'positive'},\n",
       "  {'opinion': 'acceptable', 'aspect': 'weight', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'satisfied', 'aspect': 'performance', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'much more', 'aspect': 'speed', 'sentiment': 'positive'},\n",
       "  {'opinion': 'sharp', 'aspect': 'screen', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'price', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'impressive',\n",
       "   'aspect': 'Hardware performance',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'always', 'aspect': 'works', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy', 'aspect': 'set up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'responds well', 'aspect': 'Keyboard', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'annoying', 'aspect': 'Windows 8', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'easy', 'aspect': 'setup', 'sentiment': 'positive'},\n",
       "  {'opinion': 'intuitive', 'aspect': 'setup', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy', 'aspect': 'configure', 'sentiment': 'positive'},\n",
       "  {'opinion': 'intuitive', 'aspect': 'configure', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'complaint', 'aspect': 'Windows 8', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'limited', 'aspect': 'usb ports', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'expected', 'aspect': 'features', 'sentiment': 'positive'},\n",
       "  {'opinion': 'wide', 'aspect': 'screen', 'sentiment': 'positive'},\n",
       "  {'opinion': 'roomy', 'aspect': 'keyboard', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Amazing', 'aspect': 'Performance', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'superlatives', 'aspect': 'quality', 'sentiment': 'positive'},\n",
       "  {'opinion': 'superlatives',\n",
       "   'aspect': 'performance',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'happy', 'aspect': 'OS', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'portability', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'recommend',\n",
       "   'aspect': 'portable computing',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'truly',\n",
       "   'aspect': 'portable computing',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'wonderful',\n",
       "   'aspect': 'MS Office 2011 for Mac',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'well worth',\n",
       "   'aspect': 'MS Office 2011 for Mac',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'disappointment',\n",
       "   'aspect': 'performance',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'good', 'aspect': 'look', 'sentiment': 'positive'},\n",
       "  {'opinion': 'excellent', 'aspect': 'performance', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'love', 'aspect': 'lit up keys', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Fast', 'aspect': 'lit up keys', 'sentiment': 'positive'},\n",
       "  {'opinion': 'clear', 'aspect': 'lit up keys', 'sentiment': 'positive'},\n",
       "  {'opinion': 'love', 'aspect': 'screen display', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Fast', 'aspect': 'screen display', 'sentiment': 'positive'},\n",
       "  {'opinion': 'clear', 'aspect': 'screen display', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not hard',\n",
       "   'aspect': 'Mountain Lion OS',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'familiar',\n",
       "   'aspect': 'Microsoft Windows',\n",
       "   'sentiment': 'neutral'}],\n",
       " [{'opinion': 'FAST', 'aspect': 'OSX', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'Enjoy', 'aspect': 'Microsoft Office', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Incredible', 'aspect': 'graphics', 'sentiment': 'positive'},\n",
       "  {'opinion': 'brilliant', 'aspect': 'colors', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'amazing', 'aspect': 'Built-in apps', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'beats', 'aspect': 'operating system', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'like', 'aspect': 'size', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'support',\n",
       "   'aspect': 'SquareTrade 3-Year Computer Accidental Protection Warranty',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'NOT covered', 'aspect': 'AppleCare', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'light', 'aspect': 'transport', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy', 'aspect': 'transport', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'poorly designed',\n",
       "   'aspect': 'Windows 8 Set-Up',\n",
       "   'sentiment': 'negative'},\n",
       "  {'opinion': 'frustrated',\n",
       "   'aspect': 'Windows 8 Set-Up',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'stand out',\n",
       "   'aspect': 'aluminum body',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'easy',\n",
       "   'aspect': 'integrate bluetooth devices',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'instantly', 'aspect': 'USB devices', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'underpowered',\n",
       "   'aspect': 'Intel4000 graphic chip',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'removed',\n",
       "   'aspect': 'DVD drive Firewire port',\n",
       "   'sentiment': 'neutral'},\n",
       "  {'opinion': 'silly', 'aspect': 'SDXC slot', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'worth', 'aspect': 'durability', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Well', 'aspect': 'designed', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'easy', 'aspect': 'use', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'value', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'good', 'aspect': 'specs', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'unmatched',\n",
       "   'aspect': 'product quality',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'unmatched', 'aspect': 'aesthetics', 'sentiment': 'positive'},\n",
       "  {'opinion': 'unmatched', 'aspect': 'craftmanship', 'sentiment': 'positive'},\n",
       "  {'opinion': 'unmatched',\n",
       "   'aspect': 'customer service',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'size', 'sentiment': 'positive'},\n",
       "  {'opinion': 'amazing', 'aspect': 'windows 8', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not like', 'aspect': 'Windows 8', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'long', 'aspect': 'Startup times', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'stunning', 'aspect': 'colors', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'price', 'sentiment': 'positive'},\n",
       "  {'opinion': 'free', 'aspect': 'shipping', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'terrific', 'aspect': 'mouse', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'thick', 'aspect': 'battery', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'charm', 'aspect': 'windows 7', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'price', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'delivery', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'service', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'customer service', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'hated', 'aspect': 'windows 8', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'breeze', 'aspect': 'Set up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'NOT like', 'aspect': 'Win8', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'overbearing', 'aspect': 'OSX', 'sentiment': 'negative'},\n",
       "  {'opinion': 'lack', 'aspect': 'support for games', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'nice', 'aspect': 'mobility', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'easy', 'aspect': 'set up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'nice', 'aspect': 'sound', 'sentiment': 'positive'},\n",
       "  {'opinion': 'loud', 'aspect': 'sound', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'slim', 'aspect': 'track pad', 'sentiment': 'positive'},\n",
       "  {'opinion': 'impressed', 'aspect': 'track pad', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not user-friendly',\n",
       "   'aspect': 'settings',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'goodness', 'aspect': 'OpenOffice', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Awesome', 'aspect': 'form factor', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'battery life', 'sentiment': 'positive'},\n",
       "  {'opinion': 'wonderful', 'aspect': 'UX', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'love', 'aspect': 'keyboard', 'sentiment': 'positive'},\n",
       "  {'opinion': 'love', 'aspect': 'screen', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'issues', 'aspect': 'touchpad', 'sentiment': 'negative'},\n",
       "  {'opinion': 'useless', 'aspect': 'touchpad', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'impressed', 'aspect': 'Mavericks', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Not as fast', 'aspect': 'i5', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'great', 'aspect': 'service', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'shipping', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'good', 'aspect': 'performance', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great',\n",
       "   'aspect': 'built-in applications',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'iPhoto', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'recommend',\n",
       "   'aspect': 'Samsung 830 SSD',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'scratches easily',\n",
       "   'aspect': 'aluminum',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'dirty', 'aspect': 'components', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'bad', 'aspect': 'hardware', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'nice', 'aspect': 'battery', 'sentiment': 'positive'},\n",
       "  {'opinion': 'long', 'aspect': 'battery', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'wanted', 'aspect': 'features', 'sentiment': 'positive'},\n",
       "  {'opinion': 'new', 'aspect': 'features', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'cheap', 'aspect': 'keyboard', 'sentiment': 'negative'},\n",
       "  {'opinion': 'not very sensitive',\n",
       "   'aspect': 'keyboard',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'isnt superb',\n",
       "   'aspect': 'sound quality',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'easy', 'aspect': 'navigate', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'wonderful', 'aspect': 'battery life', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Great', 'aspect': 'Performance', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Great', 'aspect': 'Quality', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'cheap', 'aspect': 'material', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'great', 'aspect': 'os', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'as anticipated', 'aspect': 'works', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'System', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Easy', 'aspect': 'customize setting', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Easy',\n",
       "   'aspect': 'create your own bookmarks',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'good', 'aspect': 'price', 'sentiment': 'positive'},\n",
       "  {'opinion': 'well', 'aspect': 'working', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'new', 'aspect': 'operating system', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Easy', 'aspect': 'set up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'quiet', 'aspect': 'hard drive', 'sentiment': 'positive'},\n",
       "  {'opinion': 'quick', 'aspect': 'boots up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'works', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'pleasurable', 'aspect': 'working', 'sentiment': 'positive'},\n",
       "  {'opinion': 'pleasurable', 'aspect': 'surfing', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'fine', 'aspect': 'works', 'sentiment': 'positive'},\n",
       "  {'opinion': 'well', 'aspect': 'software', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'performance', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'poor', 'aspect': 'Harddrive', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'obscure', 'aspect': 'on/off switch', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'lack', 'aspect': 'instructions', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'not be good', 'aspect': 'gaming', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'reliable', 'aspect': 'Audio', 'sentiment': 'positive'},\n",
       "  {'opinion': 'reliable', 'aspect': 'video', 'sentiment': 'positive'},\n",
       "  {'opinion': 'reliable', 'aspect': 'photo editing', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'bright', 'aspect': 'Screen', 'sentiment': 'positive'},\n",
       "  {'opinion': 'gorgeous', 'aspect': 'Screen', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'friendly', 'aspect': 'use', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'easy', 'aspect': 'use', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'speeds things up',\n",
       "   'aspect': 'i5 processor',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'great help',\n",
       "   'aspect': 'built in features',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'Nice', 'aspect': 'screen', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'keyboard', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'amazed', 'aspect': 'delivery', 'sentiment': 'positive'},\n",
       "  {'opinion': 'fast', 'aspect': 'delivery', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'gone', 'aspect': 'memory', 'sentiment': 'negative'},\n",
       "  {'opinion': 'not able to be used',\n",
       "   'aspect': 'memory',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'great', 'aspect': 'works', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'like', 'aspect': 'design', 'sentiment': 'positive'},\n",
       "  {'opinion': 'like', 'aspect': 'keyboard', 'sentiment': 'positive'},\n",
       "  {'opinion': 'ease of use', 'aspect': 'keyboard', 'sentiment': 'positive'},\n",
       "  {'opinion': 'plenty', 'aspect': 'ports', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'service', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'quick', 'aspect': 'Web browsing', 'sentiment': 'positive'},\n",
       "  {'opinion': 'quick', 'aspect': 'Safari browser', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'like', 'aspect': 'lighted screen', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'easy', 'aspect': 'use', 'sentiment': 'positive'},\n",
       "  {'opinion': 'quick', 'aspect': 'start up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'slow', 'aspect': 'operation', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'less expensive',\n",
       "   'aspect': 'USB3 Peripherals',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'perfect', 'aspect': 'media editing', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'elegant',\n",
       "   'aspect': 'user experience',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'fast', 'aspect': 'shipment', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'price', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Excellent', 'aspect': 'performance', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Excellent', 'aspect': 'usability', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Excellent', 'aspect': 'presentation', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Excellent',\n",
       "   'aspect': 'time response',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'smaller', 'aspect': 'size', 'sentiment': 'positive'},\n",
       "  {'opinion': 'bonus', 'aspect': 'size', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'blame', 'aspect': 'Mac OS', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'like', 'aspect': 'operating system', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'love', 'aspect': 'form factor', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'fast',\n",
       "   'aspect': 'loading the internet',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'faster', 'aspect': 'looking', 'sentiment': 'positive'},\n",
       "  {'opinion': 'sleeker', 'aspect': 'looking', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Unfortunately', 'aspect': 'XP', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'Unfortunately', 'aspect': 'support', 'sentiment': 'negative'},\n",
       "  {'opinion': 'dropping', 'aspect': 'support', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'like', 'aspect': 'OS', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy', 'aspect': 'OS', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'overall build', 'sentiment': 'positive'},\n",
       "  {'opinion': 'best', 'aspect': 'keyboard', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'solid', 'aspect': 'construction', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'nice',\n",
       "   'aspect': 'unibody construction',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'quiet', 'aspect': 'runs', 'sentiment': 'positive'},\n",
       "  {'opinion': 'cool', 'aspect': 'runs', 'sentiment': 'positive'}],\n",
       " [{'opinion': \"does n't have\",\n",
       "   'aspect': 'disk drive',\n",
       "   'sentiment': 'neutral'}],\n",
       " [{'opinion': 'no', 'aspect': 'HDMI receptacle', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'nor', 'aspect': 'SD card slot', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'perfectly', 'aspect': 'works', 'sentiment': 'positive'}],\n",
       " [{'opinion': \"do n't have\", 'aspect': 'design app', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'NOT WORKING', 'aspect': 'TRACKPAD', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'flawless', 'aspect': 'finish', 'sentiment': 'positive'},\n",
       "  {'opinion': 'solid', 'aspect': 'looks', 'sentiment': 'positive'},\n",
       "  {'opinion': 'solid', 'aspect': 'feels', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'higher', 'aspect': 'Price', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'not power down', 'aspect': 'system', 'sentiment': 'negative'},\n",
       "  {'opinion': 'not', 'aspect': 'power down', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'perfect', 'aspect': 'configuration', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'worst', 'aspect': 'speakers', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'easy', 'aspect': 'use', 'sentiment': 'positive'},\n",
       "  {'opinion': 'love', 'aspect': 'trackpad', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'smooth', 'aspect': 'Web surfuring', 'sentiment': 'positive'},\n",
       "  {'opinion': 'seamless', 'aspect': 'Web surfuring', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'pleased', 'aspect': 'interface', 'sentiment': 'positive'},\n",
       "  {'opinion': 'pleased', 'aspect': 'portability', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'well', 'aspect': 'works', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy', 'aspect': 'carry', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy', 'aspect': 'handle', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'slow', 'aspect': 'performance', 'sentiment': 'negative'},\n",
       "  {'opinion': 'shortening',\n",
       "   'aspect': 'battery lifetime',\n",
       "   'sentiment': 'negative'},\n",
       "  {'opinion': 'suffering',\n",
       "   'aspect': 'hardware ( keyboard )',\n",
       "   'sentiment': 'negative'},\n",
       "  {'opinion': 'issues',\n",
       "   'aspect': 'hardware ( keyboard )',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'good', 'aspect': 'Runs', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'small', 'aspect': 'footprint', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'gorgeous', 'aspect': 'exterior', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'high', 'aspect': 'performance', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'defective',\n",
       "   'aspect': 'intel 4000 graphics chipset',\n",
       "   'sentiment': 'neutral'},\n",
       "  {'opinion': 'defective', 'aspect': 'design', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'mistake', 'aspect': 'USB ports', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'perfect', 'aspect': 'size', 'sentiment': 'positive'},\n",
       "  {'opinion': 'perfect', 'aspect': 'speed', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'TERRIFIC',\n",
       "   'aspect': 'CUSTOMER SERVICE',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'admirably', 'aspect': 'performed', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'ZERO',\n",
       "   'aspect': 'battery cycle count',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'surprised',\n",
       "   'aspect': 'battery cycle count',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'perfectly', 'aspect': 'hardware', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'fancy', 'aspect': 'finger swipes', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'big', 'aspect': 'storage', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'nice', 'aspect': 'screen', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'like', 'aspect': 'Mac OS', 'sentiment': 'positive'},\n",
       "  {'opinion': 'lacking',\n",
       "   'aspect': 'speaker quality',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'quickly', 'aspect': 'Shipped', 'sentiment': 'positive'},\n",
       "  {'opinion': 'safely', 'aspect': 'Shipped', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'awesome',\n",
       "   'aspect': 'thunderbolt port',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'superior', 'aspect': 'performance', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'streaming video', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great',\n",
       "   'aspect': 'entertainment uses',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'like', 'aspect': 'design', 'sentiment': 'positive'},\n",
       "  {'opinion': 'needs to be improved',\n",
       "   'aspect': 'features',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'problems', 'aspect': 'Mac office', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'disappointed', 'aspect': 'spec', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'easy', 'aspect': 'use', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'looks', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'better', 'aspect': 'Performance', 'sentiment': 'positive'},\n",
       "  {'opinion': 'better', 'aspect': 'SSD', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not work',\n",
       "   'aspect': 'MagSafe accessories',\n",
       "   'sentiment': 'neutral'},\n",
       "  {'opinion': 'not work',\n",
       "   'aspect': 'MagSafe 2 connection',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'dislike', 'aspect': 'touchpad', 'sentiment': 'negative'},\n",
       "  {'opinion': 'unresponsive', 'aspect': 'touchpad', 'sentiment': 'negative'},\n",
       "  {'opinion': 'recommend', 'aspect': 'mouse', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'quiet', 'aspect': 'runs', 'sentiment': 'positive'},\n",
       "  {'opinion': \"are n't audible\", 'aspect': 'fans', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'take some time',\n",
       "   'aspect': 'Mac ecosystem',\n",
       "   'sentiment': 'neutral'}],\n",
       " [{'opinion': 'wonderful', 'aspect': 'price', 'sentiment': 'positive'},\n",
       "  {'opinion': 'worth', 'aspect': 'price', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'please', 'aspect': 'use', 'sentiment': 'positive'},\n",
       "  {'opinion': 'ease', 'aspect': 'use', 'sentiment': 'positive'},\n",
       "  {'opinion': 'please', 'aspect': 'appearance', 'sentiment': 'positive'},\n",
       "  {'opinion': 'please', 'aspect': 'functionality', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Perfect', 'aspect': 'graphic design', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not be using', 'aspect': 'slot', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'fast', 'aspect': 'OS', 'sentiment': 'positive'},\n",
       "  {'opinion': 'fluid', 'aspect': 'OS', 'sentiment': 'positive'},\n",
       "  {'opinion': 'organized', 'aspect': 'OS', 'sentiment': 'positive'},\n",
       "  {'opinion': 'beautiful', 'aspect': 'OS', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'simpler', 'aspect': 'use', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'nice', 'aspect': 'SSD', 'sentiment': 'positive'},\n",
       "  {'opinion': 'stable', 'aspect': 'SSD', 'sentiment': 'positive'},\n",
       "  {'opinion': 'fast', 'aspect': 'SSD', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'love', 'aspect': 'start up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'can not beat',\n",
       "   'aspect': 'functionality',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'properly', 'aspect': 'function', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'improved', 'aspect': 'Graphics', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'confident', 'aspect': 'Build Quality', 'sentiment': 'positive'},\n",
       "  {'opinion': \"ca n't beat\",\n",
       "   'aspect': 'unibody construction',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'more',\n",
       "   'aspect': 'flexibility for connectivity',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'help', 'aspect': 'Mac tutorials', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not helpful',\n",
       "   'aspect': 'technical support',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'new', 'aspect': 'adapter', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'no change', 'aspect': 'adapter', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'utterly fried',\n",
       "   'aspect': 'Logic board',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'cried', 'aspect': 'Logic board', 'sentiment': 'positive'},\n",
       "  {'opinion': 'laid down', 'aspect': 'Logic board', 'sentiment': 'positive'},\n",
       "  {'opinion': 'died', 'aspect': 'Logic board', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'crappy', 'aspect': 'sound', 'sentiment': 'negative'},\n",
       "  {'opinion': 'crappy', 'aspect': 'volume', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'great', 'aspect': 'OSX Lion', 'sentiment': 'positive'},\n",
       "  {'opinion': 'fast', 'aspect': 'OSX Lion', 'sentiment': 'positive'},\n",
       "  {'opinion': 'reliable', 'aspect': 'OSX Lion', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'crash', 'aspect': 'application', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'wonderful',\n",
       "   'aspect': 'physical form',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': \"has n't changed\", 'aspect': 'body', 'sentiment': 'positive'},\n",
       "  {'opinion': 'good', 'aspect': 'body', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'solid',\n",
       "   'aspect': 'unibody construction',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'slows it down',\n",
       "   'aspect': '3D rendering',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'fast', 'aspect': 'screen', 'sentiment': 'positive'},\n",
       "  {'opinion': 'great', 'aspect': 'screen', 'sentiment': 'positive'},\n",
       "  {'opinion': 'beautiful', 'aspect': 'apps', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'much needed', 'aspect': 'ports', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'neat', 'aspect': 'package', 'sentiment': 'positive'},\n",
       "  {'opinion': 'nice', 'aspect': 'package', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'a little better', 'aspect': 'cover', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'complaint', 'aspect': 'cover', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'unconsciously', 'aspect': 'gestures', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'nice', 'aspect': 'cable', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'really',\n",
       "   'aspect': '2.9ghz dual-core i7 chip',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'snappy', 'aspect': 'starts up', 'sentiment': 'positive'},\n",
       "  {'opinion': 'good', 'aspect': 'starts up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Not sure', 'aspect': 'Windows 8', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'complaint',\n",
       "   'aspect': 'internal CD drive',\n",
       "   'sentiment': 'negative'},\n",
       "  {'opinion': 'no', 'aspect': 'internal CD drive', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'no', 'aspect': 'hard drive', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'no', 'aspect': 'network lights', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'enough', 'aspect': 'storage', 'sentiment': 'positive'},\n",
       "  {'opinion': 'enough', 'aspect': 'ports', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'low', 'aspect': 'audio volume', 'sentiment': 'negative'},\n",
       "  {'opinion': 'unusable', 'aspect': 'audio volume', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'perfect', 'aspect': 'size', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'wonderful', 'aspect': 'capabilities', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'fine', 'aspect': 'settings', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'smoothly', 'aspect': 'Runs', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'slowed', 'aspect': 'Boot-up', 'sentiment': 'negative'},\n",
       "  {'opinion': 'slowed', 'aspect': 'Windows updates', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'powerful', 'aspect': 'internet', 'sentiment': 'positive'},\n",
       "  {'opinion': 'powerful',\n",
       "   'aspect': 'word processing',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'powerful', 'aspect': 'graphic design', 'sentiment': 'positive'},\n",
       "  {'opinion': 'powerful', 'aspect': 'gaming', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'like', 'aspect': 'setup', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy', 'aspect': 'setup', 'sentiment': 'positive'},\n",
       "  {'opinion': 'like', 'aspect': 'install', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy', 'aspect': 'install', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'quick', 'aspect': 'Runs', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'slim', 'aspect': 'profile', 'sentiment': 'negative'},\n",
       "  {'opinion': 'critical', 'aspect': 'profile', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'required', 'aspect': 'disk drive', 'sentiment': 'positive'},\n",
       "  {'opinion': 'required', 'aspect': 'USB ports', 'sentiment': 'positive'},\n",
       "  {'opinion': 'required',\n",
       "   'aspect': 'WiFi capability',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'recessed', 'aspect': 'SD card reader', 'sentiment': 'negative'},\n",
       "  {'opinion': 'upside down',\n",
       "   'aspect': 'SD card reader',\n",
       "   'sentiment': 'negative'},\n",
       "  {'opinion': 'can not be accessed',\n",
       "   'aspect': 'nail slot on the card',\n",
       "   'sentiment': 'negative'},\n",
       "  {'opinion': 'issue', 'aspect': 'slot', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'Soft', 'aspect': 'touch', 'sentiment': 'positive'},\n",
       "  {'opinion': 'precision',\n",
       "   'aspect': 'anodized aluminum',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'no flaws',\n",
       "   'aspect': 'anodized aluminum',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'crafted', 'aspect': 'aluminium', 'sentiment': 'positive'},\n",
       "  {'opinion': 'blow away', 'aspect': 'glass', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'nice', 'aspect': 'aluminum', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'No', 'aspect': 'HDMI port', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'impossible',\n",
       "   'aspect': 'Customization',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'surprisingly',\n",
       "   'aspect': 'two finger clicking',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'intuitive',\n",
       "   'aspect': 'right click button',\n",
       "   'sentiment': 'neutral'}],\n",
       " [{'opinion': 'quiet', 'aspect': 'SuperDrive', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'dead', 'aspect': 'battery', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'like', 'aspect': 'practicality', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'OS', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'ridiculous', 'aspect': 'Price', 'sentiment': 'negative'},\n",
       "  {'opinion': 'heavy', 'aspect': 'Price', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'not have', 'aspect': 'USB 2 ports', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'No', 'aspect': 'startup disk', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'not included',\n",
       "   'aspect': 'startup disk',\n",
       "   'sentiment': 'neutral'},\n",
       "  {'opinion': 'fault', 'aspect': 'startup disk', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'except for',\n",
       "   'aspect': 'word program',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'bigger', 'aspect': 'power switch', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'crashed', 'aspect': 'Windows 7', 'sentiment': 'negative'},\n",
       "  {'opinion': 'not want', 'aspect': 'Windows 8', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'Easy', 'aspect': 'install', 'sentiment': 'positive'},\n",
       "  {'opinion': 'easy',\n",
       "   'aspect': 'configure for ADSl cable or wifi',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'Nice', 'aspect': 'packing', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'miss', 'aspect': 'windows', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'no longer includes',\n",
       "   'aspect': 'iDVD',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'wanted', 'aspect': 'Windows 7', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'uncomfortable',\n",
       "   'aspect': 'Mac system',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'out of the box', 'aspect': 'works', 'sentiment': 'positive'},\n",
       "  {'opinion': 'cool', 'aspect': 'software', 'sentiment': 'positive'},\n",
       "  {'opinion': 'cool', 'aspect': 'OS', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'great', 'aspect': 'works', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'Premium', 'aspect': 'price', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Premium', 'aspect': 'OS', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'concerned', 'aspect': 'touch pad', 'sentiment': 'positive'},\n",
       "  {'opinion': 'fine', 'aspect': 'touch pad', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'great', 'aspect': 'OS', 'sentiment': 'positive'},\n",
       "  {'opinion': 'not become unstable', 'aspect': 'OS', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not as shown', 'aspect': 'battery', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'quick', 'aspect': 'Shipping', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'dropping',\n",
       "   'aspect': 'retina display display',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'simplest',\n",
       "   'aspect': 'compact computing',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'barely',\n",
       "   'aspect': 'ventilation system',\n",
       "   'sentiment': 'negative'},\n",
       "  {'opinion': 'hot', 'aspect': 'ventilation system', 'sentiment': 'negative'},\n",
       "  {'opinion': 'simple', 'aspect': 'watching videos', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'hot', 'aspect': 'watching videos', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'hot', 'aspect': 'playing steam games', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'corrupted',\n",
       "   'aspect': 'operating system files',\n",
       "   'sentiment': 'neutral'}],\n",
       " [{'opinion': 'struggle', 'aspect': 'keys', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'complain', 'aspect': 'OS', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'upgraded', 'aspect': 'Lion', 'sentiment': 'positive'},\n",
       "  {'opinion': 'happier', 'aspect': 'MAC OS', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'upgradeable', 'aspect': 'RAM', 'sentiment': 'positive'},\n",
       "  {'opinion': 'upgradeable', 'aspect': 'HDD', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'wanted', 'aspect': 'CD/DVD player', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'worry', 'aspect': 'os.x', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'better', 'aspect': 'OS', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'better', 'aspect': 'softwares', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'powerful', 'aspect': 'performance', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not terribly important',\n",
       "   'aspect': 'built in screen size',\n",
       "   'sentiment': 'neutral'}],\n",
       " [{'opinion': 'no', 'aspect': 'DVD slot', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'No', 'aspect': 'Cd Rom', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'less', 'aspect': 'features', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'low', 'aspect': 'volume', 'sentiment': 'negative'},\n",
       "  {'opinion': 'hate', 'aspect': 'volume', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'recommend', 'aspect': 'case', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'amazing', 'aspect': 'price', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'frustrating',\n",
       "   'aspect': 'log into the system',\n",
       "   'sentiment': 'negative'}],\n",
       " [{'opinion': 'silky smooth', 'aspect': 'set up', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'larger', 'aspect': 'case', 'sentiment': 'negative'},\n",
       "  {'opinion': 'lack',\n",
       "   'aspect': 'external power supply',\n",
       "   'sentiment': 'neutral'}],\n",
       " [{'opinion': 'hate', 'aspect': 'pad', 'sentiment': 'negative'},\n",
       "  {'opinion': 'great', 'aspect': 'works', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'wanted', 'aspect': 'features', 'sentiment': 'positive'},\n",
       "  {'opinion': 'wanted', 'aspect': 'VGA port', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'wanted', 'aspect': 'HDMI', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'wanted', 'aspect': 'ethernet', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'wanted', 'aspect': 'USB ports', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'dislike', 'aspect': 'rubber pads', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'decent', 'aspect': 'price', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'nicest', 'aspect': 'heat output', 'sentiment': 'positive'},\n",
       "  {'opinion': 'low', 'aspect': 'heat output', 'sentiment': 'positive'},\n",
       "  {'opinion': 'nicest', 'aspect': 'operation', 'sentiment': 'positive'},\n",
       "  {'opinion': 'quiet', 'aspect': 'operation', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'not have', 'aspect': 'built-in mic', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'not handle', 'aspect': 'Mac OS 10.9', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'lot of', 'aspect': 'features', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'lot of', 'aspect': 'shortcuts', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'unique', 'aspect': 'IOS system', 'sentiment': 'positive'},\n",
       "  {'opinion': 'different', 'aspect': 'IOS system', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'higher', 'aspect': 'resolution', 'sentiment': 'positive'},\n",
       "  {'opinion': 'small', 'aspect': 'fonts', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'easier', 'aspect': 'working', 'sentiment': 'positive'},\n",
       "  {'opinion': 'cool', 'aspect': 'features', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'like', 'aspect': 'brightness', 'sentiment': 'positive'},\n",
       "  {'opinion': 'like', 'aspect': 'adjustments', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'wish', 'aspect': 'CD/DVD player', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'miss', 'aspect': 'backlit keys', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'wish', 'aspect': 'Microsoft Word', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'enough', 'aspect': 'memory', 'sentiment': 'positive'},\n",
       "  {'opinion': 'enough', 'aspect': 'speed', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'excellent', 'aspect': 'battery life', 'sentiment': 'positive'},\n",
       "  {'opinion': 'excellent', 'aspect': 'display', 'sentiment': 'positive'},\n",
       "  {'opinion': 'breeze',\n",
       "   'aspect': 'downloading apps',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'smoothness',\n",
       "   'aspect': 'operating system',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'weak', 'aspect': 'bass', 'sentiment': 'negative'},\n",
       "  {'opinion': 'tinny', 'aspect': 'sound', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'good', 'aspect': 'built quality', 'sentiment': 'positive'},\n",
       "  {'opinion': 'Happy', 'aspect': 'built quality', 'sentiment': 'positive'},\n",
       "  {'opinion': 'excited', 'aspect': 'built quality', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'loving', 'aspect': 'performance', 'sentiment': 'positive'},\n",
       "  {'opinion': 'fast', 'aspect': 'performance', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'sloppy',\n",
       "   'aspect': 'Bluetooth interface',\n",
       "   'sentiment': 'negative'},\n",
       "  {'opinion': 'courtesy', 'aspect': 'Mac OS', 'sentiment': 'negative'},\n",
       "  {'opinion': 'poor', 'aspect': 'range', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'gripe', 'aspect': 'RAM', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'Fine', 'aspect': 'touch screen', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'well', 'aspect': 'built', 'sentiment': 'positive'},\n",
       "  {'opinion': 'AMAZING', 'aspect': 'battery life', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'easy', 'aspect': 'OS', 'sentiment': 'positive'},\n",
       "  {'opinion': 'surprises', 'aspect': 'OS', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'veryimportant',\n",
       "   'aspect': 'Firewire 800',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'better', 'aspect': 'Firewire 800', 'sentiment': 'positive'},\n",
       "  {'opinion': 'inferior', 'aspect': 'MAC OS', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'like', 'aspect': 'iTunes', 'sentiment': 'positive'},\n",
       "  {'opinion': 'apparent', 'aspect': 'security', 'sentiment': 'positive'},\n",
       "  {'opinion': 'nice', 'aspect': 'graphics stuff', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'flying', 'aspect': 'ssd drive', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'complained', 'aspect': 'HDMI', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'best', 'aspect': 'specs', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'perfect', 'aspect': 'packing', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'issues', 'aspect': 'mother boards', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'faulty', 'aspect': 'Mother Board', 'sentiment': 'negative'},\n",
       "  {'opinion': 'faulty', 'aspect': 'graphics card', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'better', 'aspect': 'performs', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'awesome',\n",
       "   'aspect': 'virtual home schooling',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'more', 'aspect': 'Cost', 'sentiment': 'negative'}],\n",
       " [{'opinion': 'excellent',\n",
       "   'aspect': 'operating system',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'excellent', 'aspect': 'size', 'sentiment': 'positive'},\n",
       "  {'opinion': 'excellent', 'aspect': 'weight', 'sentiment': 'positive'},\n",
       "  {'opinion': 'optimal', 'aspect': 'mobility', 'sentiment': 'positive'},\n",
       "  {'opinion': 'excellent',\n",
       "   'aspect': 'durability of the battery',\n",
       "   'sentiment': 'positive'},\n",
       "  {'opinion': 'unmatched',\n",
       "   'aspect': 'functions provided by the trackpad',\n",
       "   'sentiment': 'positive'}],\n",
       " [{'opinion': 'better', 'aspect': 'hardware', 'sentiment': 'positive'}],\n",
       " [{'opinion': 'no issues', 'aspect': 'software', 'sentiment': 'neutral'},\n",
       "  {'opinion': 'no issues', 'aspect': 'updates', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'not have', 'aspect': 'disc drive', 'sentiment': 'neutral'}],\n",
       " [{'opinion': 'complain', 'aspect': 'Screen', 'sentiment': 'positive'},\n",
       "  {'opinion': 'ridiculous', 'aspect': 'Screen', 'sentiment': 'positive'},\n",
       "  {'opinion': 'low', 'aspect': 'res', 'sentiment': 'positive'},\n",
       "  {'opinion': 'ridiculous', 'aspect': 'res', 'sentiment': 'positive'}]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_lap14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5064\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/3180 : < :, Epoch 0.01/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-159\n",
      "Configuration saved in ./t5/checkpoint-159/config.json\n",
      "Model weights saved in ./t5/checkpoint-159/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-159/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-159/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-159/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-318\n",
      "Configuration saved in ./t5/checkpoint-318/config.json\n",
      "Model weights saved in ./t5/checkpoint-318/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-318/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-318/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-318/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-477\n",
      "Configuration saved in ./t5/checkpoint-477/config.json\n",
      "Model weights saved in ./t5/checkpoint-477/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-477/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-477/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-477/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-318] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-636\n",
      "Configuration saved in ./t5/checkpoint-636/config.json\n",
      "Model weights saved in ./t5/checkpoint-636/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-636/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-636/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-636/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-477] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-795\n",
      "Configuration saved in ./t5/checkpoint-795/config.json\n",
      "Model weights saved in ./t5/checkpoint-795/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-795/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-795/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-795/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-636] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-954\n",
      "Configuration saved in ./t5/checkpoint-954/config.json\n",
      "Model weights saved in ./t5/checkpoint-954/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-954/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-954/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-954/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-795] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1113\n",
      "Configuration saved in ./t5/checkpoint-1113/config.json\n",
      "Model weights saved in ./t5/checkpoint-1113/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1113/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1113/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1113/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-954] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1272\n",
      "Configuration saved in ./t5/checkpoint-1272/config.json\n",
      "Model weights saved in ./t5/checkpoint-1272/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1272/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1272/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1272/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1113] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1431\n",
      "Configuration saved in ./t5/checkpoint-1431/config.json\n",
      "Model weights saved in ./t5/checkpoint-1431/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1431/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1431/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1431/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-159] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1590\n",
      "Configuration saved in ./t5/checkpoint-1590/config.json\n",
      "Model weights saved in ./t5/checkpoint-1590/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1590/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1590/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1590/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1749\n",
      "Configuration saved in ./t5/checkpoint-1749/config.json\n",
      "Model weights saved in ./t5/checkpoint-1749/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1749/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1749/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1749/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1590] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1908\n",
      "Configuration saved in ./t5/checkpoint-1908/config.json\n",
      "Model weights saved in ./t5/checkpoint-1908/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1908/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1908/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1908/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1749] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2067\n",
      "Configuration saved in ./t5/checkpoint-2067/config.json\n",
      "Model weights saved in ./t5/checkpoint-2067/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2067/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2067/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2067/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1908] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2226\n",
      "Configuration saved in ./t5/checkpoint-2226/config.json\n",
      "Model weights saved in ./t5/checkpoint-2226/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2226/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2226/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2226/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2067] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2385\n",
      "Configuration saved in ./t5/checkpoint-2385/config.json\n",
      "Model weights saved in ./t5/checkpoint-2385/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2385/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2385/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2385/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2226] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2544\n",
      "Configuration saved in ./t5/checkpoint-2544/config.json\n",
      "Model weights saved in ./t5/checkpoint-2544/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2544/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2544/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2544/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1431] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2703\n",
      "Configuration saved in ./t5/checkpoint-2703/config.json\n",
      "Model weights saved in ./t5/checkpoint-2703/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2703/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2703/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2703/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2385] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2862\n",
      "Configuration saved in ./t5/checkpoint-2862/config.json\n",
      "Model weights saved in ./t5/checkpoint-2862/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2862/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2862/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2862/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2703] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3021\n",
      "Configuration saved in ./t5/checkpoint-3021/config.json\n",
      "Model weights saved in ./t5/checkpoint-3021/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3021/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3021/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3021/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2862] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3180\n",
      "Configuration saved in ./t5/checkpoint-3180/config.json\n",
      "Model weights saved in ./t5/checkpoint-3180/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3180/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3180/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3180/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3021] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-2544 (score: 0.534306627258842).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3180, training_loss=0.02443723297156628, metrics={'train_runtime': 1663.3771, 'train_samples_per_second': 60.888, 'train_steps_per_second': 1.912, 'total_flos': 1.397309066698752e+16, 'train_loss': 0.02443723297156628, 'epoch': 20.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:17,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.4305835010060362,\n",
       " 'precision': 0.7210084033613445,\n",
       " 'f1_score': 0.5391742012021976}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2420\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1520\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='1520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  77/1520 00:34 < 10:57, 2.20 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-76\n",
      "Configuration saved in ./t5/checkpoint-76/config.json\n",
      "Model weights saved in ./t5/checkpoint-76/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-76/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-76/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-76/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-152\n",
      "Configuration saved in ./t5/checkpoint-152/config.json\n",
      "Model weights saved in ./t5/checkpoint-152/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-152/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-152/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-152/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-228\n",
      "Configuration saved in ./t5/checkpoint-228/config.json\n",
      "Model weights saved in ./t5/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-228/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-228/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-76] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-304\n",
      "Configuration saved in ./t5/checkpoint-304/config.json\n",
      "Model weights saved in ./t5/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-304/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-304/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-152] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-380\n",
      "Configuration saved in ./t5/checkpoint-380/config.json\n",
      "Model weights saved in ./t5/checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-380/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-380/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-304] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-456\n",
      "Configuration saved in ./t5/checkpoint-456/config.json\n",
      "Model weights saved in ./t5/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-456/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-456/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-532\n",
      "Configuration saved in ./t5/checkpoint-532/config.json\n",
      "Model weights saved in ./t5/checkpoint-532/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-532/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-532/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-532/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-228] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-608\n",
      "Configuration saved in ./t5/checkpoint-608/config.json\n",
      "Model weights saved in ./t5/checkpoint-608/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-608/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-608/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-608/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-456] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-684\n",
      "Configuration saved in ./t5/checkpoint-684/config.json\n",
      "Model weights saved in ./t5/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-684/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-684/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-608] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-760\n",
      "Configuration saved in ./t5/checkpoint-760/config.json\n",
      "Model weights saved in ./t5/checkpoint-760/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-760/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-760/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-760/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-684] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-836\n",
      "Configuration saved in ./t5/checkpoint-836/config.json\n",
      "Model weights saved in ./t5/checkpoint-836/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-836/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-836/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-836/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-760] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-912\n",
      "Configuration saved in ./t5/checkpoint-912/config.json\n",
      "Model weights saved in ./t5/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-912/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-912/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-836] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-988\n",
      "Configuration saved in ./t5/checkpoint-988/config.json\n",
      "Model weights saved in ./t5/checkpoint-988/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-988/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-988/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-988/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-912] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1064\n",
      "Configuration saved in ./t5/checkpoint-1064/config.json\n",
      "Model weights saved in ./t5/checkpoint-1064/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1064/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1064/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1064/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-988] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1140\n",
      "Configuration saved in ./t5/checkpoint-1140/config.json\n",
      "Model weights saved in ./t5/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1064] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1216\n",
      "Configuration saved in ./t5/checkpoint-1216/config.json\n",
      "Model weights saved in ./t5/checkpoint-1216/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1216/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1216/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1216/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1292\n",
      "Configuration saved in ./t5/checkpoint-1292/config.json\n",
      "Model weights saved in ./t5/checkpoint-1292/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1292/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1292/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1292/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1216] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1368\n",
      "Configuration saved in ./t5/checkpoint-1368/config.json\n",
      "Model weights saved in ./t5/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1368/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1368/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1292] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1444\n",
      "Configuration saved in ./t5/checkpoint-1444/config.json\n",
      "Model weights saved in ./t5/checkpoint-1444/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1444/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1444/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1444/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1368] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1520\n",
      "Configuration saved in ./t5/checkpoint-1520/config.json\n",
      "Model weights saved in ./t5/checkpoint-1520/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1520/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1520/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1520/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1444] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-532 (score: 0.6714377654781826).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1520, training_loss=0.03920878225104197, metrics={'train_runtime': 766.7602, 'train_samples_per_second': 63.123, 'train_steps_per_second': 1.982, 'total_flos': 7138128273408000.0, 'train_loss': 0.03920878225104197, 'epoch': 20.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:09,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5298969072164949,\n",
       " 'precision': 0.6277372262773723,\n",
       " 'f1_score': 0.5746824581702831}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3428\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2160\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 109/2160 00:44 < 14:17, 2.39 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-108\n",
      "Configuration saved in ./t5/checkpoint-108/config.json\n",
      "Model weights saved in ./t5/checkpoint-108/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-108/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-108/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-108/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-216\n",
      "Configuration saved in ./t5/checkpoint-216/config.json\n",
      "Model weights saved in ./t5/checkpoint-216/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-216/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-216/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-216/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-324\n",
      "Configuration saved in ./t5/checkpoint-324/config.json\n",
      "Model weights saved in ./t5/checkpoint-324/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-324/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-324/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-324/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-108] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-432\n",
      "Configuration saved in ./t5/checkpoint-432/config.json\n",
      "Model weights saved in ./t5/checkpoint-432/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-432/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-432/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-432/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-324] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-540\n",
      "Configuration saved in ./t5/checkpoint-540/config.json\n",
      "Model weights saved in ./t5/checkpoint-540/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-540/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-540/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-540/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-216] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-648\n",
      "Configuration saved in ./t5/checkpoint-648/config.json\n",
      "Model weights saved in ./t5/checkpoint-648/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-648/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-648/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-648/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-432] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-756\n",
      "Configuration saved in ./t5/checkpoint-756/config.json\n",
      "Model weights saved in ./t5/checkpoint-756/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-756/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-756/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-756/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-648] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-864\n",
      "Configuration saved in ./t5/checkpoint-864/config.json\n",
      "Model weights saved in ./t5/checkpoint-864/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-864/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-864/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-864/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-540] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-972\n",
      "Configuration saved in ./t5/checkpoint-972/config.json\n",
      "Model weights saved in ./t5/checkpoint-972/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-972/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-972/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-972/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-756] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1080\n",
      "Configuration saved in ./t5/checkpoint-1080/config.json\n",
      "Model weights saved in ./t5/checkpoint-1080/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1080/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1080/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1080/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-972] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1188\n",
      "Configuration saved in ./t5/checkpoint-1188/config.json\n",
      "Model weights saved in ./t5/checkpoint-1188/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1188/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1188/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1188/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1080] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1296\n",
      "Configuration saved in ./t5/checkpoint-1296/config.json\n",
      "Model weights saved in ./t5/checkpoint-1296/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1296/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1296/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1296/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1188] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1404\n",
      "Configuration saved in ./t5/checkpoint-1404/config.json\n",
      "Model weights saved in ./t5/checkpoint-1404/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1404/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1404/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1404/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1296] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1512\n",
      "Configuration saved in ./t5/checkpoint-1512/config.json\n",
      "Model weights saved in ./t5/checkpoint-1512/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1512/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1512/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1512/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1404] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1620\n",
      "Configuration saved in ./t5/checkpoint-1620/config.json\n",
      "Model weights saved in ./t5/checkpoint-1620/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1620/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1620/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1620/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1728\n",
      "Configuration saved in ./t5/checkpoint-1728/config.json\n",
      "Model weights saved in ./t5/checkpoint-1728/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1728/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1728/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1728/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1620] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1836\n",
      "Configuration saved in ./t5/checkpoint-1836/config.json\n",
      "Model weights saved in ./t5/checkpoint-1836/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1836/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1836/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1836/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1728] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1944\n",
      "Configuration saved in ./t5/checkpoint-1944/config.json\n",
      "Model weights saved in ./t5/checkpoint-1944/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1944/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1944/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1944/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1836] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2052\n",
      "Configuration saved in ./t5/checkpoint-2052/config.json\n",
      "Model weights saved in ./t5/checkpoint-2052/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2052/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2052/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2052/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1944] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2160\n",
      "Configuration saved in ./t5/checkpoint-2160/config.json\n",
      "Model weights saved in ./t5/checkpoint-2160/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2160/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2160/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2160/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2052] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-864 (score: 0.7025614218504966).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2160, training_loss=0.030442173609992972, metrics={'train_runtime': 1011.8291, 'train_samples_per_second': 67.758, 'train_steps_per_second': 2.135, 'total_flos': 1.011127478188032e+16, 'train_loss': 0.030442173609992972, 'epoch': 20.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:12,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.603112840466926,\n",
       " 'precision': 0.6813186813186813,\n",
       " 'f1_score': 0.6398348813209495}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4480\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2800\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 141/2800 01:03 < 20:18, 2.18 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-140\n",
      "Configuration saved in ./t5/checkpoint-140/config.json\n",
      "Model weights saved in ./t5/checkpoint-140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-140/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-280\n",
      "Configuration saved in ./t5/checkpoint-280/config.json\n",
      "Model weights saved in ./t5/checkpoint-280/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-280/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-280/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-280/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-420\n",
      "Configuration saved in ./t5/checkpoint-420/config.json\n",
      "Model weights saved in ./t5/checkpoint-420/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-420/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-420/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-420/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-560\n",
      "Configuration saved in ./t5/checkpoint-560/config.json\n",
      "Model weights saved in ./t5/checkpoint-560/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-560/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-560/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-560/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-700\n",
      "Configuration saved in ./t5/checkpoint-700/config.json\n",
      "Model weights saved in ./t5/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-700/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-700/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-560] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-840\n",
      "Configuration saved in ./t5/checkpoint-840/config.json\n",
      "Model weights saved in ./t5/checkpoint-840/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-840/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-840/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-840/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-980\n",
      "Configuration saved in ./t5/checkpoint-980/config.json\n",
      "Model weights saved in ./t5/checkpoint-980/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-980/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-980/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-980/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-840] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1120\n",
      "Configuration saved in ./t5/checkpoint-1120/config.json\n",
      "Model weights saved in ./t5/checkpoint-1120/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1120/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1120/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1120/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-980] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1260\n",
      "Configuration saved in ./t5/checkpoint-1260/config.json\n",
      "Model weights saved in ./t5/checkpoint-1260/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1260/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1260/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1260/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1400\n",
      "Configuration saved in ./t5/checkpoint-1400/config.json\n",
      "Model weights saved in ./t5/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1400/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1400/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1540\n",
      "Configuration saved in ./t5/checkpoint-1540/config.json\n",
      "Model weights saved in ./t5/checkpoint-1540/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1540/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1540/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1540/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1680\n",
      "Configuration saved in ./t5/checkpoint-1680/config.json\n",
      "Model weights saved in ./t5/checkpoint-1680/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1680/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1680/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1680/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1540] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1820\n",
      "Configuration saved in ./t5/checkpoint-1820/config.json\n",
      "Model weights saved in ./t5/checkpoint-1820/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1820/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1820/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1820/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1680] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1960\n",
      "Configuration saved in ./t5/checkpoint-1960/config.json\n",
      "Model weights saved in ./t5/checkpoint-1960/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1960/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1960/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1960/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1820] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2100\n",
      "Configuration saved in ./t5/checkpoint-2100/config.json\n",
      "Model weights saved in ./t5/checkpoint-2100/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2100/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2100/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2100/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1960] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2240\n",
      "Configuration saved in ./t5/checkpoint-2240/config.json\n",
      "Model weights saved in ./t5/checkpoint-2240/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2240/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2240/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2240/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2380\n",
      "Configuration saved in ./t5/checkpoint-2380/config.json\n",
      "Model weights saved in ./t5/checkpoint-2380/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2380/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2380/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2380/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2240] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-2520\n",
      "Configuration saved in ./t5/checkpoint-2520/config.json\n",
      "Model weights saved in ./t5/checkpoint-2520/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2520/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2520/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2520/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2380] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-2660\n",
      "Configuration saved in ./t5/checkpoint-2660/config.json\n",
      "Model weights saved in ./t5/checkpoint-2660/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2660/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2660/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2660/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2800\n",
      "Configuration saved in ./t5/checkpoint-2800/config.json\n",
      "Model weights saved in ./t5/checkpoint-2800/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2800/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2800/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2660] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-420 (score: 0.6956521739130435).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2800, training_loss=0.02643859689789159, metrics={'train_runtime': 1378.2213, 'train_samples_per_second': 65.011, 'train_steps_per_second': 2.032, 'total_flos': 1.438663016448e+16, 'train_loss': 0.02643859689789159, 'epoch': 20.0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:08,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.2875739644970414,\n",
       " 'precision': 0.5560640732265446,\n",
       " 'f1_score': 0.3790951638065523}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6832\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4280\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='4280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 215/4280 01:37 < 30:58, 2.19 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-214\n",
      "Configuration saved in ./t5/checkpoint-214/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-214\n",
      "Configuration saved in ./t5/checkpoint-214/config.json\n",
      "Model weights saved in ./t5/checkpoint-214/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-214/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-214/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-214/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-428\n",
      "Configuration saved in ./t5/checkpoint-428/config.json\n",
      "Model weights saved in ./t5/checkpoint-428/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-428/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-428/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-428/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-856\n",
      "Configuration saved in ./t5/checkpoint-856/config.json\n",
      "Model weights saved in ./t5/checkpoint-856/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-856/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-856/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-856/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-428] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1070\n",
      "Configuration saved in ./t5/checkpoint-1070/config.json\n",
      "Model weights saved in ./t5/checkpoint-1070/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1070/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1070/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1070/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-642] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1284\n",
      "Configuration saved in ./t5/checkpoint-1284/config.json\n",
      "Model weights saved in ./t5/checkpoint-1284/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1284/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1284/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1284/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-856] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1498\n",
      "Configuration saved in ./t5/checkpoint-1498/config.json\n",
      "Model weights saved in ./t5/checkpoint-1498/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1498/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1498/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1498/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1712\n",
      "Configuration saved in ./t5/checkpoint-1712/config.json\n",
      "Model weights saved in ./t5/checkpoint-1712/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1712/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1712/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1712/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1070] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1926\n",
      "Configuration saved in ./t5/checkpoint-1926/config.json\n",
      "Model weights saved in ./t5/checkpoint-1926/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1926/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1926/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1926/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1498] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2140\n",
      "Configuration saved in ./t5/checkpoint-2140/config.json\n",
      "Model weights saved in ./t5/checkpoint-2140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1926] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2354\n",
      "Configuration saved in ./t5/checkpoint-2354/config.json\n",
      "Model weights saved in ./t5/checkpoint-2354/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2354/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2354/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2354/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2568\n",
      "Configuration saved in ./t5/checkpoint-2568/config.json\n",
      "Model weights saved in ./t5/checkpoint-2568/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2568/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2568/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2568/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1712] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-2782\n",
      "Configuration saved in ./t5/checkpoint-2782/config.json\n",
      "Model weights saved in ./t5/checkpoint-2782/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2782/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2782/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2782/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2354] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2996\n",
      "Configuration saved in ./t5/checkpoint-2996/config.json\n",
      "Model weights saved in ./t5/checkpoint-2996/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2996/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2996/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2996/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2782] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3210\n",
      "Configuration saved in ./t5/checkpoint-3210/config.json\n",
      "Model weights saved in ./t5/checkpoint-3210/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3210/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3210/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3210/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2996] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3424\n",
      "Configuration saved in ./t5/checkpoint-3424/config.json\n",
      "Model weights saved in ./t5/checkpoint-3424/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3424/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3424/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3424/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3638\n",
      "Configuration saved in ./t5/checkpoint-3638/config.json\n",
      "Model weights saved in ./t5/checkpoint-3638/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3638/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3638/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3638/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3424] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3852\n",
      "Configuration saved in ./t5/checkpoint-3852/config.json\n",
      "Model weights saved in ./t5/checkpoint-3852/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3852/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3852/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3852/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3638] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4066\n",
      "Configuration saved in ./t5/checkpoint-4066/config.json\n",
      "Model weights saved in ./t5/checkpoint-4066/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4066/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4066/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4066/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3852] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4280\n",
      "Configuration saved in ./t5/checkpoint-4280/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-4280\n",
      "Configuration saved in ./t5/checkpoint-4280/config.json\n",
      "Model weights saved in ./t5/checkpoint-4280/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4280/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4280/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4280/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4066] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-2568 (score: 0.7341772151898734).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4280, training_loss=0.019733498087065798, metrics={'train_runtime': 2028.5889, 'train_samples_per_second': 67.357, 'train_steps_per_second': 2.11, 'total_flos': 2.1939611000832e+16, 'train_loss': 0.019733498087065798, 'epoch': 20.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:11,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.38766006984866125,\n",
       " 'precision': 0.6121323529411765,\n",
       " 'f1_score': 0.4746970776906629}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5004\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3140\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='3140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  22/3140 00:07 < 19:57, 2.60 it/s, Epoch 0.13/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-157\n",
      "Configuration saved in ./t5/checkpoint-157/config.json\n",
      "Model weights saved in ./t5/checkpoint-157/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-157/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-157/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-157/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-314\n",
      "Configuration saved in ./t5/checkpoint-314/config.json\n",
      "Model weights saved in ./t5/checkpoint-314/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-314/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-314/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-314/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-471\n",
      "Configuration saved in ./t5/checkpoint-471/config.json\n",
      "Model weights saved in ./t5/checkpoint-471/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-471/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-471/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-471/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-314] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-628\n",
      "Configuration saved in ./t5/checkpoint-628/config.json\n",
      "Model weights saved in ./t5/checkpoint-628/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-628/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-628/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-628/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-471] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-785\n",
      "Configuration saved in ./t5/checkpoint-785/config.json\n",
      "Model weights saved in ./t5/checkpoint-785/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-785/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-785/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-785/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-628] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-942\n",
      "Configuration saved in ./t5/checkpoint-942/config.json\n",
      "Model weights saved in ./t5/checkpoint-942/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-942/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-942/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-942/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-785] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1099\n",
      "Configuration saved in ./t5/checkpoint-1099/config.json\n",
      "Model weights saved in ./t5/checkpoint-1099/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1099/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1099/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1099/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-942] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1256\n",
      "Configuration saved in ./t5/checkpoint-1256/config.json\n",
      "Model weights saved in ./t5/checkpoint-1256/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1256/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1256/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1256/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1099] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1413\n",
      "Configuration saved in ./t5/checkpoint-1413/config.json\n",
      "Model weights saved in ./t5/checkpoint-1413/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1413/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1413/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1413/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1570\n",
      "Configuration saved in ./t5/checkpoint-1570/config.json\n",
      "Model weights saved in ./t5/checkpoint-1570/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1570/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1570/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1570/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1413] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1727\n",
      "Configuration saved in ./t5/checkpoint-1727/config.json\n",
      "Model weights saved in ./t5/checkpoint-1727/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1727/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1727/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1727/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1570] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1884\n",
      "Configuration saved in ./t5/checkpoint-1884/config.json\n",
      "Model weights saved in ./t5/checkpoint-1884/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1884/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1884/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1884/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1727] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2041\n",
      "Configuration saved in ./t5/checkpoint-2041/config.json\n",
      "Model weights saved in ./t5/checkpoint-2041/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2041/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2041/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2041/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1884] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2198\n",
      "Configuration saved in ./t5/checkpoint-2198/config.json\n",
      "Model weights saved in ./t5/checkpoint-2198/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2198/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2198/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2198/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2041] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2355\n",
      "Configuration saved in ./t5/checkpoint-2355/config.json\n",
      "Model weights saved in ./t5/checkpoint-2355/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2355/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2355/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2355/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2198] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2512\n",
      "Configuration saved in ./t5/checkpoint-2512/config.json\n",
      "Model weights saved in ./t5/checkpoint-2512/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2512/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2512/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2512/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2355] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2669\n",
      "Configuration saved in ./t5/checkpoint-2669/config.json\n",
      "Model weights saved in ./t5/checkpoint-2669/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2669/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2669/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2669/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2826\n",
      "Configuration saved in ./t5/checkpoint-2826/config.json\n",
      "Model weights saved in ./t5/checkpoint-2826/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2826/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2826/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2826/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2669] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2983\n",
      "Configuration saved in ./t5/checkpoint-2983/config.json\n",
      "Model weights saved in ./t5/checkpoint-2983/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2983/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2983/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2983/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2826] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3140\n",
      "Configuration saved in ./t5/checkpoint-3140/config.json\n",
      "Model weights saved in ./t5/checkpoint-3140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2983] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-157 (score: 0.12403100775193798).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3140, training_loss=0.02554370107212264, metrics={'train_runtime': 1304.0701, 'train_samples_per_second': 76.744, 'train_steps_per_second': 2.408, 'total_flos': 1.08319336280064e+16, 'train_loss': 0.02554370107212264, 'epoch': 20.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:13,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.11949685534591195,\n",
       " 'precision': 0.37109375,\n",
       " 'f1_score': 0.1807802093244529}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7584\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4740\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='4740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  76/4740 00:34 < 36:27, 2.13 it/s, Epoch 0.32/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-237\n",
      "Configuration saved in ./t5/checkpoint-237/config.json\n",
      "Model weights saved in ./t5/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-237/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-237/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-474\n",
      "Configuration saved in ./t5/checkpoint-474/config.json\n",
      "Model weights saved in ./t5/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-474/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-474/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-711\n",
      "Configuration saved in ./t5/checkpoint-711/config.json\n",
      "Model weights saved in ./t5/checkpoint-711/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-711/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-711/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-711/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-237] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-948\n",
      "Configuration saved in ./t5/checkpoint-948/config.json\n",
      "Model weights saved in ./t5/checkpoint-948/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-948/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-948/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-948/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-711] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1185\n",
      "Configuration saved in ./t5/checkpoint-1185/config.json\n",
      "Model weights saved in ./t5/checkpoint-1185/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1185/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1185/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1185/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-948] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1422\n",
      "Configuration saved in ./t5/checkpoint-1422/config.json\n",
      "Model weights saved in ./t5/checkpoint-1422/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1422/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1422/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1422/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1185] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1659\n",
      "Configuration saved in ./t5/checkpoint-1659/config.json\n",
      "Model weights saved in ./t5/checkpoint-1659/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1659/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1659/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1659/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1422] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1896\n",
      "Configuration saved in ./t5/checkpoint-1896/config.json\n",
      "Model weights saved in ./t5/checkpoint-1896/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1896/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1896/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1896/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1659] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2133\n",
      "Configuration saved in ./t5/checkpoint-2133/config.json\n",
      "Model weights saved in ./t5/checkpoint-2133/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2133/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2133/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2133/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-474] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2370\n",
      "Configuration saved in ./t5/checkpoint-2370/config.json\n",
      "Model weights saved in ./t5/checkpoint-2370/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2370/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2370/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2370/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1896] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2607\n",
      "Configuration saved in ./t5/checkpoint-2607/config.json\n",
      "Model weights saved in ./t5/checkpoint-2607/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2607/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2607/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2607/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2370] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2844\n",
      "Configuration saved in ./t5/checkpoint-2844/config.json\n",
      "Model weights saved in ./t5/checkpoint-2844/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2844/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2844/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2844/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2607] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3081\n",
      "Configuration saved in ./t5/checkpoint-3081/config.json\n",
      "Model weights saved in ./t5/checkpoint-3081/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3081/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3081/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3081/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2844] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3318\n",
      "Configuration saved in ./t5/checkpoint-3318/config.json\n",
      "Model weights saved in ./t5/checkpoint-3318/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3318/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3318/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3318/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3081] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3555\n",
      "Configuration saved in ./t5/checkpoint-3555/config.json\n",
      "Model weights saved in ./t5/checkpoint-3555/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3555/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3555/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3555/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3318] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3792\n",
      "Configuration saved in ./t5/checkpoint-3792/config.json\n",
      "Model weights saved in ./t5/checkpoint-3792/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3792/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3792/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3792/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3555] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4029\n",
      "Configuration saved in ./t5/checkpoint-4029/config.json\n",
      "Model weights saved in ./t5/checkpoint-4029/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4029/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4029/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4029/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2133] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4266\n",
      "Configuration saved in ./t5/checkpoint-4266/config.json\n",
      "Model weights saved in ./t5/checkpoint-4266/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4266/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4266/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4266/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3792] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4503\n",
      "Configuration saved in ./t5/checkpoint-4503/config.json\n",
      "Model weights saved in ./t5/checkpoint-4503/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4503/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4503/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4503/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4266] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4740\n",
      "Configuration saved in ./t5/checkpoint-4740/config.json\n",
      "Model weights saved in ./t5/checkpoint-4740/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4740/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4740/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4740/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4503] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-4029 (score: 0.30809399477806787).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4740, training_loss=0.01968533838568609, metrics={'train_runtime': 2346.1284, 'train_samples_per_second': 64.651, 'train_steps_per_second': 2.02, 'total_flos': 2.4354509635584e+16, 'train_loss': 0.01968533838568609, 'epoch': 20.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:11,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.2640801001251564,\n",
       " 'precision': 0.4678492239467849,\n",
       " 'f1_score': 0.3376}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# William Hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--Wikidepia--IndoT5-base/snapshots/da8e5576aff97b6e6e08ffa669e34bbf87ca637c/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"Wikidepia/IndoT5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--Wikidepia--IndoT5-base/snapshots/da8e5576aff97b6e6e08ffa669e34bbf87ca637c/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Wikidepia/IndoT5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7500\n",
      "  Number of trainable parameters = 247577856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='492' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 492/7500 05:39 < 1:20:57, 1.44 it/s, Epoch 1.31/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.200200</td>\n",
       "      <td>0.094703</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-375\n",
      "Configuration saved in ./t5/checkpoint-375/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-375\n",
      "Configuration saved in ./t5/checkpoint-375/config.json\n",
      "Model weights saved in ./t5/checkpoint-375/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-375/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-375/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-750\n",
      "Configuration saved in ./t5/checkpoint-750/config.json\n",
      "Model weights saved in ./t5/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-750/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-750/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1125\n",
      "Configuration saved in ./t5/checkpoint-1125/config.json\n",
      "Model weights saved in ./t5/checkpoint-1125/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1125/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1125/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1125/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1500\n",
      "Configuration saved in ./t5/checkpoint-1500/config.json\n",
      "Model weights saved in ./t5/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1500/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1125] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1875\n",
      "Configuration saved in ./t5/checkpoint-1875/config.json\n",
      "Model weights saved in ./t5/checkpoint-1875/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1875/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1875/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1875/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2250\n",
      "Configuration saved in ./t5/checkpoint-2250/config.json\n",
      "Model weights saved in ./t5/checkpoint-2250/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2250/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2250/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2250/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2625\n",
      "Configuration saved in ./t5/checkpoint-2625/config.json\n",
      "Model weights saved in ./t5/checkpoint-2625/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2625/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2625/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2625/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3000\n",
      "Configuration saved in ./t5/checkpoint-3000/config.json\n",
      "Model weights saved in ./t5/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3000/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3000/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2625] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3375\n",
      "Configuration saved in ./t5/checkpoint-3375/config.json\n",
      "Model weights saved in ./t5/checkpoint-3375/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3375/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3375/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3375/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3750\n",
      "Configuration saved in ./t5/checkpoint-3750/config.json\n",
      "Model weights saved in ./t5/checkpoint-3750/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3750/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3750/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3375] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4125\n",
      "Configuration saved in ./t5/checkpoint-4125/config.json\n",
      "Model weights saved in ./t5/checkpoint-4125/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4125/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4125/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4125/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4500\n",
      "Configuration saved in ./t5/checkpoint-4500/config.json\n",
      "Model weights saved in ./t5/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4500/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4500/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4125] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4875\n",
      "Configuration saved in ./t5/checkpoint-4875/config.json\n",
      "Model weights saved in ./t5/checkpoint-4875/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4875/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4875/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4875/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-5250\n",
      "Configuration saved in ./t5/checkpoint-5250/config.json\n",
      "Model weights saved in ./t5/checkpoint-5250/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-5250/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-5250/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-5250/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-5625\n",
      "Configuration saved in ./t5/checkpoint-5625/config.json\n",
      "Model weights saved in ./t5/checkpoint-5625/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-5625/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-5625/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-5625/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-5250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-6000\n",
      "Configuration saved in ./t5/checkpoint-6000/config.json\n",
      "Model weights saved in ./t5/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-6000/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-6000/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-5625] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-6375\n",
      "Configuration saved in ./t5/checkpoint-6375/config.json\n",
      "Model weights saved in ./t5/checkpoint-6375/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-6375/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-6375/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-6375/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-6750\n",
      "Configuration saved in ./t5/checkpoint-6750/config.json\n",
      "Model weights saved in ./t5/checkpoint-6750/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-6750/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-6750/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-6750/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-6375] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-7125\n",
      "Configuration saved in ./t5/checkpoint-7125/config.json\n",
      "Model weights saved in ./t5/checkpoint-7125/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-7125/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-7125/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-7125/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-6750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-7500\n",
      "Configuration saved in ./t5/checkpoint-7500/config.json\n",
      "Model weights saved in ./t5/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-7500/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-7500/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-7125] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-375 (score: 0).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.164492866435647, metrics={'train_runtime': 5288.9667, 'train_samples_per_second': 45.377, 'train_steps_per_second': 1.418, 'total_flos': 4.778348078451917e+16, 'train_loss': 0.164492866435647, 'epoch': 20.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_id,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = william_tok[\"hotel\"][\"train\"],\n",
    "        eval_dataset = william_tok[\"hotel\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,william_2[\"hotel\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:29,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_id, william_tok[\"hotel\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in william_2[\"hotel\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.3274270948689553,\n",
       " 'precision': 0.7598290598290598,\n",
       " 'f1_score': 0.4576449083904052}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"william_hotel.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
