{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,6\"\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "sys.path.append(\"../../../src/\")\n",
    "import data_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_dir = dict(\n",
    "    lap14 = \"../../../data/absa/en/peng/14lap\",\n",
    "    res14 = \"../../../data/absa/en/peng/14res\",\n",
    "    res15 = \"../../../data/absa/en/peng/15res\",\n",
    "    res16 = \"../../../data/absa/en/peng/16res\"\n",
    ")\n",
    "\n",
    "wan_dir = dict(\n",
    "    res15 = \"../../../data/absa/en/wan/interim/rest15\",\n",
    "    res16 = \"../../../data/absa/en/wan/interim/rest16\"\n",
    ")\n",
    "    \n",
    "zhang_dir = dict(\n",
    "    res15 = \"../../../data/absa/en/zhang/interim/interim_2/rest15\",\n",
    "    res16 = \"../../../data/absa/en/zhang/interim/interim_2/rest16\"\n",
    ")\n",
    "\n",
    "william_dir = dict(\n",
    "    hotel = \"../../../data/absa/id/william\"\n",
    ")\n",
    "\n",
    "peng = dict(\n",
    "    lap14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res15\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res15\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res15\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res16\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res16\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res16\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")\n",
    "\n",
    "wan = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    )\n",
    ")\n",
    "\n",
    "zhang = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    )\n",
    ")\n",
    "\n",
    "william = dict(\n",
    "    hotel = dict(\n",
    "        train = data_utils.read_data(path=william_dir[\"hotel\"] + \"/train.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=william_dir[\"hotel\"] + \"/dev.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=william_dir[\"hotel\"] + \"/test.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.SENTIMENT_ELEMENT = {'a' : \"aspect\", 'o' : \"opinion\", 's' : \"sentiment\", 'c' : \"category\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AOS (ASTE)\n",
    "    * AO\n",
    "    * AS\n",
    "    * A\n",
    "    * O\n",
    "\n",
    "2. ACS (TASD)\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * C\n",
    "\n",
    "3. ACOS\n",
    "    * AO\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * O\n",
    "    * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oas', 'oa', 'as', 'a', 'o', 'asc', 'sc', 'c', 'oasc']\n"
     ]
    }
   ],
   "source": [
    "task_tree = {\n",
    "    \"oas\" : [\"oas\",\"oa\",\"as\",'a','o'],\n",
    "    \"asc\" : [\"asc\",\"as\",\"sc\",'a','c'],\n",
    "    \"oasc\" : [\"oasc\",\"oa\",\"as\",\"sc\",'a','o','c']\n",
    "}\n",
    "\n",
    "all_task = []\n",
    "for k,v1 in task_tree.items():\n",
    "    if k not in all_task:\n",
    "        all_task.append(k)\n",
    "    for v2 in v1:\n",
    "        if v2 not in all_task:\n",
    "            all_task.append(v2)\n",
    "\n",
    "print(all_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.remove_duplicate_targets(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"ao\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle mix may not be a must, but we'll see it later. Will be problematic if like as (UABSA / E2E ABSA) used for training AOS (ASTE) --> may be for further experiment because we will insert imputing later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good', 'sentiment': 'mixed'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.handle_mix_sentiment(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"aos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Peng (ASTE/AOS)\n",
    "peng_intermediate = dict()\n",
    "\n",
    "for domain, v1 in peng.items():\n",
    "    peng_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        peng_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = peng[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            peng_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wan (TASD/ACS)\n",
    "wan_intermediate = dict()\n",
    "\n",
    "for domain, v1 in wan.items():\n",
    "    wan_intermediate[domain] = dict()\n",
    "    for task in [\"asc\"] + task_tree[\"asc\"]:\n",
    "        wan_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = wan[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            wan_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang (ACOS)\n",
    "zhang_intermediate = dict()\n",
    "\n",
    "for domain, v1 in zhang.items():\n",
    "    zhang_intermediate[domain] = dict()\n",
    "    for task in [\"oasc\"] + task_tree[\"oasc\"]:\n",
    "        zhang_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = zhang[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            zhang_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William (AOS ID)\n",
    "william_intermediate = dict()\n",
    "\n",
    "for domain, v1 in william.items():\n",
    "    william_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        william_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = william[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            william_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"<extra_id_X>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = {\n",
    "    ',' : \"<comma>\",\n",
    "    '(' : \"<open_bracket>\",\n",
    "    ')' : \"<close_bracket>\",\n",
    "    ';' : \"<semicolon>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_answer(targets,se_order):\n",
    "#     result = []\n",
    "#     counter = 0\n",
    "#     for t in targets:\n",
    "#         constructed_t = \"\"\n",
    "#         for se in se_order:\n",
    "#             if counter > 99:\n",
    "#                 raise Exception(\"Extra id more than 99!\")\n",
    "#             constructed_t += ' ' + mask.replace('X',str(counter)) + ' ' + t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "#             counter += 1\n",
    "#         constructed_t = constructed_t.strip()\n",
    "#         result.append(constructed_t)\n",
    "#     result = \" ; \".join(result)\n",
    "#     return result\n",
    "def construct_answer(targets,se_order):\n",
    "    result = []\n",
    "    for t in targets:\n",
    "        constructed_t = []\n",
    "        for se in se_order:\n",
    "            element = t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "            for k, v in added_tokens.items():\n",
    "                element = element.replace(k,v)\n",
    "            constructed_t.append(element)\n",
    "        constructed_t = \" , \".join(constructed_t)\n",
    "        constructed_t = f\"( {constructed_t} )\"\n",
    "        result.append(constructed_t)\n",
    "    result = \" ; \".join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( no , GUI , negative ) ; ( dark , screen , negative ) ; ( steady , power light , neutral ) ; ( steady , hard drive light , negative )'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( <open_bracket> tes3 <semicolon> tes4 <close_bracket> , tes1 <comma> tes2 , positive )'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer([{\"aspect\" : \"tes1 , tes2\", \"opinion\" : \"( tes3 ; tes4 )\", \"sentiment\" : \"positive\"}],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_prompt(text,se_order):\n",
    "#     prompt = []\n",
    "#     for counter, se in enumerate(se_order):\n",
    "#         prompt.append(data_utils.SENTIMENT_ELEMENT[se] + \" : \" + mask.replace('X',str(counter)))\n",
    "#     prompt = \" ,\".join(prompt)\n",
    "#     result = text + \"| \" + prompt\n",
    "#     return result\n",
    "def construct_prompt(text,se_order):\n",
    "    prompt = []\n",
    "    for se in se_order:\n",
    "        prompt.append(data_utils.SENTIMENT_ELEMENT[se])\n",
    "    prompt = \" , \".join(prompt)\n",
    "    prompt = f\"( {prompt} )\"\n",
    "    masked_text = text\n",
    "    for k, v in added_tokens.items():\n",
    "        masked_text = masked_text.replace(k,v)\n",
    "    result = masked_text + \" | \" + prompt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One night I turned the freaking thing off after using it <comma> the next day I turn it on <comma> no GUI <comma> screen all dark <comma> power light steady <comma> hard drive light steady and not flashing as it usually does . | ( opinion , aspect , sentiment )'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_prompt(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"text\"],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def catch_answer(output,se_order):\n",
    "#     output = output.replace(\"<pad>\",'')\n",
    "#     output = output.replace(\"</s>\",'')\n",
    "#     pattern = r\"\"\n",
    "#     for se in se_order:\n",
    "#         if se != 's':\n",
    "#             pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
    "#         else:\n",
    "#             pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
    "#     found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "#     for i in range(len(found)):\n",
    "#         for k, v in found[i].items():\n",
    "#             found[i][k] = found[i][k].strip()\n",
    "#     return found\n",
    "def catch_answer(output,se_order):\n",
    "    output = output.replace(\"<pad>\",'')\n",
    "    output = output.replace(\"</s>\",'')\n",
    "    pattern = []\n",
    "    for se in se_order:\n",
    "        if se != 's':\n",
    "            pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\")\n",
    "        else:\n",
    "            pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\")\n",
    "    pattern = ','.join(pattern)\n",
    "    pattern = f\"\\({pattern}\\)\"\n",
    "    found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "    for i in range(len(found)):\n",
    "        for k, v in found[i].items():\n",
    "            found[i][k] = found[i][k].strip()\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'opinion': 'no', 'aspect': 'GUI', 'sentiment': 'negative'},\n",
       " {'opinion': 'dark', 'aspect': 'screen', 'sentiment': 'negative'},\n",
       " {'opinion': 'steady', 'aspect': 'power light', 'sentiment': 'neutral'},\n",
       " {'opinion': 'steady', 'aspect': 'hard drive light', 'sentiment': 'negative'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")\n",
    "se_order = \"oas\"\n",
    "catch_answer(output,se_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( no , GUI , negative ) ; ( dark , screen , negative ) ; ( steady , power light , neutral ) ; ( steady , hard drive light , negative )'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "peng_2 = dict()\n",
    "for domain, v1 in peng_intermediate.items():\n",
    "    peng_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in peng_intermediate[domain][basic_task][\"train\"]:\n",
    "            peng_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        peng_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        peng_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    peng_2[domain][\"train\"] = Dataset.from_list(peng_2[domain][\"train\"])\n",
    "    peng_2[domain][\"val\"] = Dataset.from_list(peng_2[domain][\"val\"])\n",
    "    peng_2[domain][\"test\"] = Dataset.from_list(peng_2[domain][\"test\"])\n",
    "\n",
    "wan_2 = dict()\n",
    "for domain, v1 in wan_intermediate.items():\n",
    "    wan_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"asc\"]:\n",
    "        for el in wan_intermediate[domain][basic_task][\"train\"]:\n",
    "            wan_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"val\"]:\n",
    "        wan_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"test\"]:\n",
    "        wan_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    wan_2[domain][\"train\"] = Dataset.from_list(wan_2[domain][\"train\"])\n",
    "    wan_2[domain][\"val\"] = Dataset.from_list(wan_2[domain][\"val\"])\n",
    "    wan_2[domain][\"test\"] = Dataset.from_list(wan_2[domain][\"test\"])\n",
    "\n",
    "zhang_2 = dict()\n",
    "for domain, v1 in zhang_intermediate.items():\n",
    "    zhang_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oasc\"]:\n",
    "        for el in zhang_intermediate[domain][basic_task][\"train\"]:\n",
    "            zhang_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"val\"]:\n",
    "        zhang_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"test\"]:\n",
    "        zhang_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    zhang_2[domain][\"train\"] = Dataset.from_list(zhang_2[domain][\"train\"])\n",
    "    zhang_2[domain][\"val\"] = Dataset.from_list(zhang_2[domain][\"val\"])\n",
    "    zhang_2[domain][\"test\"] = Dataset.from_list(zhang_2[domain][\"test\"])\n",
    "\n",
    "william_2 = dict()\n",
    "for domain, v1 in william_intermediate.items():\n",
    "    william_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in william_intermediate[domain][basic_task][\"train\"]:\n",
    "            william_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in william_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        william_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in william_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        william_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    william_2[domain][\"train\"] = Dataset.from_list(william_2[domain][\"train\"])\n",
    "    william_2[domain][\"val\"] = Dataset.from_list(william_2[domain][\"val\"])\n",
    "    william_2[domain][\"test\"] = Dataset.from_list(william_2[domain][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tempat yag bagus dan nyaman untuk istirahat tetapi tolong tvnya perlu di perbaiki channelnya karena banyak semutnya digambar dan water heaternya tidak bisa jadi mandi air dingin terus . | ( opinion , aspect , sentiment )',\n",
       " 'output': '( bagus , tempat , positive ) ; ( nyaman , tempat , positive ) ; ( perlu di perbaiki , tvnya , positive ) ; ( tidak bisa , water heaternya , negative )',\n",
       " 'task': 'oas'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "william_2[\"hotel\"][\"train\"][69]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Tokenized Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_en.add_tokens(list(added_tokens.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_args = {\n",
    "    \"max_length\" : 512,\n",
    "    \"padding\" : True,\n",
    "    \"truncation\" : True,\n",
    "    \"return_tensors\" : \"pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_en(dataset):\n",
    "    result = tokenizer_en(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "peng_tok = dict()\n",
    "for domain, v1 in peng_2.items():\n",
    "    peng_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            peng_tok[domain][split] = peng_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            peng_tok[domain][split] = encode_en(peng_2[domain][split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "wan_tok = dict()\n",
    "for domain, v1 in wan_2.items():\n",
    "    wan_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            wan_tok[domain][split] = wan_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            wan_tok[domain][split] = encode_en(wan_2[domain][split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "zhang_tok = dict()\n",
    "for domain, v1 in zhang_2.items():\n",
    "    zhang_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            zhang_tok[domain][split] = zhang_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            zhang_tok[domain][split] = encode_en(zhang_2[domain][split])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_id = AutoTokenizer.from_pretrained(\"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_id.add_tokens(list(added_tokens.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_id(dataset):\n",
    "    result = tokenizer_id(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "william_tok = dict()\n",
    "for domain, v1 in william_2.items():\n",
    "    william_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            william_tok[domain][split] = william_2[domain][split].map(encode_id,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            william_tok[domain][split] = encode_id(william_2[domain][split])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator_en = DataCollatorForSeq2Seq(tokenizer=tokenizer_en)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_id = DataCollatorForSeq2Seq(tokenizer=tokenizer_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from evaluation import recall, precision, f1_score, summary_score\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def seperate_target_prediction_per_task(predictions:List[List[Dict]],targets:List[List[Dict]],tasks:List) -> Tuple[Dict[str,List],Dict[str,List]]:\n",
    "    per_task_targets = {}\n",
    "    per_task_predictions = {}\n",
    "    for target, prediction, task in zip(targets,predictions,tasks):\n",
    "        if task not in per_task_targets.keys():\n",
    "            per_task_targets[task] = []\n",
    "        if task not in per_task_predictions.keys():\n",
    "            per_task_predictions[task] = []\n",
    "        per_task_targets[task].append(target)\n",
    "        per_task_predictions[task].append(prediction)\n",
    "    return per_task_targets, per_task_predictions\n",
    "\n",
    "def preprocess_eval_preds(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer):\n",
    "    input_ids = eval_preds.inputs\n",
    "    target_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(input_ids, tuple):\n",
    "        input_ids = input_ids[0]\n",
    "    if isinstance(target_ids, tuple):\n",
    "        target_ids = target_ids[0]\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    input_ids = np.argmax(input_ids,axis=-1) if len(input_ids.shape) == 3 else input_ids # in case not predict with generate\n",
    "    target_ids = np.argmax(target_ids,axis=-1) if len(target_ids.shape) == 3 else target_ids # in case not predict with generate\n",
    "    prediction_ids = np.argmax(pred_ids,axis=-1) if len(pred_ids.shape) == 3 else pred_ids # in case not predict with generate\n",
    "\n",
    "    input_ids = [[token for token in row if token != -100] for row in input_ids]\n",
    "    target_ids = [[token for token in row if token != -100] for row in target_ids]\n",
    "    prediction_ids = [[token for token in row if token != -100] for row in prediction_ids]\n",
    "\n",
    "    inputs = tokenizer.batch_decode(input_ids,**decoding_args)\n",
    "    targets = tokenizer.batch_decode(target_ids,**decoding_args)\n",
    "    predictions = tokenizer.batch_decode(prediction_ids,**decoding_args)\n",
    "\n",
    "    return inputs, targets, predictions\n",
    "\n",
    "def compute_metrics(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer,tasks:List) -> Dict[str,float]: # MAY NOT BE SUFFICIATE FOR CAUSAL LM\n",
    "        \"\"\"\n",
    "        ### DESC\n",
    "            Method to compute the metrics.\n",
    "        ### PARAMS\n",
    "        * eval_preds: EvalPrediction instance from training.\n",
    "        * decoding_args: Decoding arguments.\n",
    "        ### RETURN\n",
    "        * metrics: Dictionary of metrics.\n",
    "        \"\"\"\n",
    "        inputs, targets, predictions = preprocess_eval_preds(eval_preds,decoding_args,tokenizer)\n",
    "\n",
    "        targets = [catch_answer(text,task) for text,task in zip(targets,tasks) if task != \"non_absa\"]\n",
    "        predictions = [catch_answer(text,task) for text,task in zip(predictions,tasks) if task != \"non_absa\"]\n",
    "\n",
    "\n",
    "        per_task_targets, per_task_predictions = seperate_target_prediction_per_task(predictions, targets, tasks)\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        metrics[\"overall_recall\"] = recall(predictions,targets)\n",
    "        metrics[\"overall_precision\"] = precision(predictions,targets)\n",
    "        metrics[\"overall_f1_score\"] = f1_score(predictions,targets)\n",
    "\n",
    "        for task in per_task_targets.keys():\n",
    "            if task == \"non_absa\":\n",
    "                continue\n",
    "            metrics[f\"{task}_recall\"] = recall(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_precision\"] = precision(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_f1_score\"] = f1_score(per_task_predictions[task],per_task_targets[task])\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"per_device_train_batch_size\": 16//n_gpu,\n",
    "    \"per_device_eval_batch_size\": 16//n_gpu,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_strategy\" : \"epoch\",\n",
    "    \"metric_for_best_model\": \"overall_f1_score\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"adam_epsilon\": 1e-08,\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"logging_dir\" : \"./output/log\",\n",
    "    \"include_inputs_for_metrics\" : True\n",
    "}\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(**train_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# trainer = {\n",
    "#     \"peng\" : {},\n",
    "#     \"wan\" : {},\n",
    "#     \"zhang\" : {},\n",
    "#     \"william\" : {}\n",
    "# }\n",
    "\n",
    "decoding_args = {\n",
    "    \"skip_special_tokens\" : False\n",
    "}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, targets):\n",
    "    pred_logits = logits[0] if isinstance(logits,tuple) else logits\n",
    "    pred_ids = torch.argmax(pred_logits, dim=-1)\n",
    "    return pred_ids, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_predictions(model,tokenizer,tokenized:torch.Tensor,device:torch.device=torch.device(\"cpu\"),batch_size:int=16,max_len:int=512,decoding_args:Dict={}) -> List[str]:\n",
    "    # Data loader\n",
    "    input_ids_data_loader = torch.utils.data.DataLoader(tokenized[\"input_ids\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    attention_mask_data_loader = torch.utils.data.DataLoader(tokenized[\"attention_mask\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    # Predict\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    tensor_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(zip(input_ids_data_loader,attention_mask_data_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            tensor_predictions.extend(model.generate(input_ids=input_ids,attention_mask=attention_mask,max_length=max_len,pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id).cpu())\n",
    "            input_ids = input_ids.cpu()\n",
    "            attention_mask = attention_mask.cpu()\n",
    "    tensor_predictions = [[token for token in row if token != -100] for row in tensor_predictions]\n",
    "    predictions = tokenizer.batch_decode(tensor_predictions,**decoding_args)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_result(str_preds_,preds,targets,filename):\n",
    "    result = []\n",
    "    str_preds = [el.replace(\"<pad>\",'').replace(\"</s>\",'') for el in str_preds_]\n",
    "    assert len(str_preds) == len(preds) == len(targets)\n",
    "    for i in range(len(str_preds)):\n",
    "        result.append({\n",
    "            \"str_pred\" : str_preds[i],\n",
    "            \"pred\" : preds[i],\n",
    "            \"target\" : targets[i]\n",
    "        })\n",
    "    \n",
    "    with open(filename,'w') as fp:\n",
    "        json.dump(result,fp)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Laptop 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4530\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2840\n",
      "  Number of trainable parameters = 222903552\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='702' max='2840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 702/2840 05:54 < 18:01, 1.98 it/s, Epoch 4.94/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.059005</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.559767</td>\n",
       "      <td>0.546230</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.559767</td>\n",
       "      <td>0.546230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.058904</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.576812</td>\n",
       "      <td>0.563466</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.576812</td>\n",
       "      <td>0.563466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>0.562319</td>\n",
       "      <td>0.585507</td>\n",
       "      <td>0.573679</td>\n",
       "      <td>0.562319</td>\n",
       "      <td>0.585507</td>\n",
       "      <td>0.573679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.075240</td>\n",
       "      <td>0.576812</td>\n",
       "      <td>0.598837</td>\n",
       "      <td>0.587618</td>\n",
       "      <td>0.576812</td>\n",
       "      <td>0.598837</td>\n",
       "      <td>0.587618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-142\n",
      "Configuration saved in ./t5/checkpoint-142/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-142\n",
      "Configuration saved in ./t5/checkpoint-142/config.json\n",
      "Model weights saved in ./t5/checkpoint-142/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-142/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-142/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-142/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-284\n",
      "Configuration saved in ./t5/checkpoint-284/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-284\n",
      "Configuration saved in ./t5/checkpoint-284/config.json\n",
      "Model weights saved in ./t5/checkpoint-284/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-284/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-284/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-284/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-426\n",
      "Configuration saved in ./t5/checkpoint-426/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-426\n",
      "Configuration saved in ./t5/checkpoint-426/config.json\n",
      "Model weights saved in ./t5/checkpoint-426/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-426/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-426/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-426/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-142] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-568\n",
      "Configuration saved in ./t5/checkpoint-568/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-568\n",
      "Configuration saved in ./t5/checkpoint-568/config.json\n",
      "Model weights saved in ./t5/checkpoint-568/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-568/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-568/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-568/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-710\n",
      "Configuration saved in ./t5/checkpoint-710/config.json\n",
      "Model weights saved in ./t5/checkpoint-710/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-710/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-710/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-710/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-426] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-852\n",
      "Configuration saved in ./t5/checkpoint-852/config.json\n",
      "Model weights saved in ./t5/checkpoint-852/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-852/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-852/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-852/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-568] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-994\n",
      "Configuration saved in ./t5/checkpoint-994/config.json\n",
      "Model weights saved in ./t5/checkpoint-994/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-994/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-994/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-994/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-710] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1136\n",
      "Configuration saved in ./t5/checkpoint-1136/config.json\n",
      "Model weights saved in ./t5/checkpoint-1136/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1136/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1136/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1136/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-994] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1278\n",
      "Configuration saved in ./t5/checkpoint-1278/config.json\n",
      "Model weights saved in ./t5/checkpoint-1278/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1278/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1278/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1278/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1136] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1420\n",
      "Configuration saved in ./t5/checkpoint-1420/config.json\n",
      "Model weights saved in ./t5/checkpoint-1420/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1420/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1420/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1420/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1278] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1562\n",
      "Configuration saved in ./t5/checkpoint-1562/config.json\n",
      "Model weights saved in ./t5/checkpoint-1562/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1562/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1562/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1562/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1420] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1704\n",
      "Configuration saved in ./t5/checkpoint-1704/config.json\n",
      "Model weights saved in ./t5/checkpoint-1704/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1704/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1704/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1704/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1562] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1846\n",
      "Configuration saved in ./t5/checkpoint-1846/config.json\n",
      "Model weights saved in ./t5/checkpoint-1846/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1846/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1846/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1846/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1704] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1988\n",
      "Configuration saved in ./t5/checkpoint-1988/config.json\n",
      "Model weights saved in ./t5/checkpoint-1988/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1988/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1988/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1988/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1846] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2130\n",
      "Configuration saved in ./t5/checkpoint-2130/config.json\n",
      "Model weights saved in ./t5/checkpoint-2130/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2130/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2130/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2130/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1988] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2272\n",
      "Configuration saved in ./t5/checkpoint-2272/config.json\n",
      "Model weights saved in ./t5/checkpoint-2272/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2272/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2272/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2272/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2414\n",
      "Configuration saved in ./t5/checkpoint-2414/config.json\n",
      "Model weights saved in ./t5/checkpoint-2414/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2414/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2414/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2414/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2556\n",
      "Configuration saved in ./t5/checkpoint-2556/config.json\n",
      "Model weights saved in ./t5/checkpoint-2556/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2556/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2556/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2556/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2414] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2698\n",
      "Configuration saved in ./t5/checkpoint-2698/config.json\n",
      "Model weights saved in ./t5/checkpoint-2698/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2698/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2698/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2698/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2556] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2840\n",
      "Configuration saved in ./t5/checkpoint-2840/config.json\n",
      "Model weights saved in ./t5/checkpoint-2840/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2840/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2840/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2840/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2698] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-852 (score: 0.6200731409995937).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2840, training_loss=0.020601016940207967, metrics={'train_runtime': 1453.2227, 'train_samples_per_second': 62.344, 'train_steps_per_second': 1.954, 'total_flos': 1.282296902602752e+16, 'train_loss': 0.020601016940207967, 'epoch': 20.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"lap14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"lap14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"lap14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:11,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"lap14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"lap14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5951940850277264,\n",
       " 'precision': 0.5940959409594095,\n",
       " 'f1_score': 0.5946445060018467}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_lap14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6330\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3960\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='730' max='3960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 730/3960 07:37 < 33:49, 1.59 it/s, Epoch 3.68/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.590278</td>\n",
       "      <td>0.609842</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.590278</td>\n",
       "      <td>0.609842</td>\n",
       "      <td>0.599900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.045037</td>\n",
       "      <td>0.618056</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.624744</td>\n",
       "      <td>0.618056</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.624744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.054908</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.624782</td>\n",
       "      <td>0.617871</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.624782</td>\n",
       "      <td>0.617871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-198\n",
      "Configuration saved in ./t5/checkpoint-198/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-198\n",
      "Configuration saved in ./t5/checkpoint-198/config.json\n",
      "Model weights saved in ./t5/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-198/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-198/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-396\n",
      "Configuration saved in ./t5/checkpoint-396/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-396\n",
      "Configuration saved in ./t5/checkpoint-396/config.json\n",
      "Model weights saved in ./t5/checkpoint-396/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-396/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-396/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-396/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-594\n",
      "Configuration saved in ./t5/checkpoint-594/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-594\n",
      "Configuration saved in ./t5/checkpoint-594/config.json\n",
      "Model weights saved in ./t5/checkpoint-594/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-594/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-594/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-594/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-198] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-792\n",
      "Configuration saved in ./t5/checkpoint-792/config.json\n",
      "Model weights saved in ./t5/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-792/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-792/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-594] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-990\n",
      "Configuration saved in ./t5/checkpoint-990/config.json\n",
      "Model weights saved in ./t5/checkpoint-990/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-990/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-990/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-990/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-792] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1188\n",
      "Configuration saved in ./t5/checkpoint-1188/config.json\n",
      "Model weights saved in ./t5/checkpoint-1188/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1188/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1188/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1188/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-990] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1386\n",
      "Configuration saved in ./t5/checkpoint-1386/config.json\n",
      "Model weights saved in ./t5/checkpoint-1386/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1386/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1386/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1386/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1188] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1584\n",
      "Configuration saved in ./t5/checkpoint-1584/config.json\n",
      "Model weights saved in ./t5/checkpoint-1584/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1584/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1584/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1584/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1386] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1782\n",
      "Configuration saved in ./t5/checkpoint-1782/config.json\n",
      "Model weights saved in ./t5/checkpoint-1782/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1782/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1782/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1782/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-396] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1980\n",
      "Configuration saved in ./t5/checkpoint-1980/config.json\n",
      "Model weights saved in ./t5/checkpoint-1980/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1980/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1980/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1980/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1584] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2178\n",
      "Configuration saved in ./t5/checkpoint-2178/config.json\n",
      "Model weights saved in ./t5/checkpoint-2178/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2178/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2178/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2178/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1980] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2376\n",
      "Configuration saved in ./t5/checkpoint-2376/config.json\n",
      "Model weights saved in ./t5/checkpoint-2376/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2376/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2376/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2376/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1782] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2574\n",
      "Configuration saved in ./t5/checkpoint-2574/config.json\n",
      "Model weights saved in ./t5/checkpoint-2574/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2574/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2574/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2574/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2178] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2970\n",
      "Configuration saved in ./t5/checkpoint-2970/config.json\n",
      "Model weights saved in ./t5/checkpoint-2970/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2970/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2970/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2970/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2772] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3168\n",
      "Configuration saved in ./t5/checkpoint-3168/config.json\n",
      "Model weights saved in ./t5/checkpoint-3168/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3168/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3168/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3168/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2970] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3366\n",
      "Configuration saved in ./t5/checkpoint-3366/config.json\n",
      "Model weights saved in ./t5/checkpoint-3366/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3366/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3366/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3366/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3168] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3564\n",
      "Configuration saved in ./t5/checkpoint-3564/config.json\n",
      "Model weights saved in ./t5/checkpoint-3564/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3564/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3564/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3564/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3366] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3762\n",
      "Configuration saved in ./t5/checkpoint-3762/config.json\n",
      "Model weights saved in ./t5/checkpoint-3762/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3762/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3762/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3762/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3564] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3960\n",
      "Configuration saved in ./t5/checkpoint-3960/config.json\n",
      "Model weights saved in ./t5/checkpoint-3960/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3960/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3960/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3960/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3762] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-2376 (score: 0.6492324707232905).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3960, training_loss=0.013948131309389466, metrics={'train_runtime': 2508.4329, 'train_samples_per_second': 50.47, 'train_steps_per_second': 1.579, 'total_flos': 2.33373120841728e+16, 'train_loss': 0.013948131309389466, 'epoch': 20.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:20,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.7152917505030181,\n",
       " 'precision': 0.7244174265450861,\n",
       " 'f1_score': 0.719825666723541}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3025\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1900\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='949' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 949/1900 09:37 < 09:39, 1.64 it/s, Epoch 9.98/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.062185</td>\n",
       "      <td>0.618474</td>\n",
       "      <td>0.634538</td>\n",
       "      <td>0.626403</td>\n",
       "      <td>0.618474</td>\n",
       "      <td>0.634538</td>\n",
       "      <td>0.626403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.057951</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.670659</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.670659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.061996</td>\n",
       "      <td>0.706827</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.708830</td>\n",
       "      <td>0.706827</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.708830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.071031</td>\n",
       "      <td>0.690763</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.694756</td>\n",
       "      <td>0.690763</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.694756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.071471</td>\n",
       "      <td>0.694779</td>\n",
       "      <td>0.702811</td>\n",
       "      <td>0.698772</td>\n",
       "      <td>0.694779</td>\n",
       "      <td>0.702811</td>\n",
       "      <td>0.698772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.079220</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.734940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.078953</td>\n",
       "      <td>0.702811</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.706804</td>\n",
       "      <td>0.702811</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.706804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.079194</td>\n",
       "      <td>0.730924</td>\n",
       "      <td>0.738956</td>\n",
       "      <td>0.734918</td>\n",
       "      <td>0.730924</td>\n",
       "      <td>0.738956</td>\n",
       "      <td>0.734918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.092307</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.730924</td>\n",
       "      <td>0.726885</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.730924</td>\n",
       "      <td>0.726885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-95\n",
      "Configuration saved in ./t5/checkpoint-95/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-95\n",
      "Configuration saved in ./t5/checkpoint-95/config.json\n",
      "Model weights saved in ./t5/checkpoint-95/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-95/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-95/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-95/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-190\n",
      "Configuration saved in ./t5/checkpoint-190/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-190\n",
      "Configuration saved in ./t5/checkpoint-190/config.json\n",
      "Model weights saved in ./t5/checkpoint-190/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-190/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-190/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-285\n",
      "Configuration saved in ./t5/checkpoint-285/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-285\n",
      "Configuration saved in ./t5/checkpoint-285/config.json\n",
      "Model weights saved in ./t5/checkpoint-285/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-285/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-285/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-285/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-95] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-380\n",
      "Configuration saved in ./t5/checkpoint-380/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-380\n",
      "Configuration saved in ./t5/checkpoint-380/config.json\n",
      "Model weights saved in ./t5/checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-380/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-380/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-475\n",
      "Configuration saved in ./t5/checkpoint-475/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-475\n",
      "Configuration saved in ./t5/checkpoint-475/config.json\n",
      "Model weights saved in ./t5/checkpoint-475/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-475/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-475/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-475/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-570\n",
      "Configuration saved in ./t5/checkpoint-570/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-570\n",
      "Configuration saved in ./t5/checkpoint-570/config.json\n",
      "Model weights saved in ./t5/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-570/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-570/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-285] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-665\n",
      "Configuration saved in ./t5/checkpoint-665/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-665\n",
      "Configuration saved in ./t5/checkpoint-665/config.json\n",
      "Model weights saved in ./t5/checkpoint-665/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-665/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-665/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-665/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-475] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-760\n",
      "Configuration saved in ./t5/checkpoint-760/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-760\n",
      "Configuration saved in ./t5/checkpoint-760/config.json\n",
      "Model weights saved in ./t5/checkpoint-760/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-760/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-760/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-760/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-665] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-855\n",
      "Configuration saved in ./t5/checkpoint-855/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-855\n",
      "Configuration saved in ./t5/checkpoint-855/config.json\n",
      "Model weights saved in ./t5/checkpoint-855/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-855/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-855/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-855/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-760] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-950\n",
      "Configuration saved in ./t5/checkpoint-950/config.json\n",
      "Model weights saved in ./t5/checkpoint-950/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-950/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-950/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-855] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1045\n",
      "Configuration saved in ./t5/checkpoint-1045/config.json\n",
      "Model weights saved in ./t5/checkpoint-1045/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1045/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1045/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1045/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-950] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1140\n",
      "Configuration saved in ./t5/checkpoint-1140/config.json\n",
      "Model weights saved in ./t5/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1045] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1235\n",
      "Configuration saved in ./t5/checkpoint-1235/config.json\n",
      "Model weights saved in ./t5/checkpoint-1235/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1235/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1235/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1235/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1140] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1330\n",
      "Configuration saved in ./t5/checkpoint-1330/config.json\n",
      "Model weights saved in ./t5/checkpoint-1330/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1330/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1330/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1330/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1235] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1425\n",
      "Configuration saved in ./t5/checkpoint-1425/config.json\n",
      "Model weights saved in ./t5/checkpoint-1425/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1425/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1425/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1425/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1520\n",
      "Configuration saved in ./t5/checkpoint-1520/config.json\n",
      "Model weights saved in ./t5/checkpoint-1520/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1520/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1520/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1520/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1425] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1615\n",
      "Configuration saved in ./t5/checkpoint-1615/config.json\n",
      "Model weights saved in ./t5/checkpoint-1615/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1615/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1615/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1615/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1710\n",
      "Configuration saved in ./t5/checkpoint-1710/config.json\n",
      "Model weights saved in ./t5/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1710/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1710/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1615] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1805\n",
      "Configuration saved in ./t5/checkpoint-1805/config.json\n",
      "Model weights saved in ./t5/checkpoint-1805/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1805/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1805/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1805/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1710] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1900\n",
      "Configuration saved in ./t5/checkpoint-1900/config.json\n",
      "Model weights saved in ./t5/checkpoint-1900/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1900/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1900/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1900/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1805] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-570 (score: 0.7349397590361445).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1900, training_loss=0.022991722289864954, metrics={'train_runtime': 1166.5215, 'train_samples_per_second': 51.864, 'train_steps_per_second': 1.629, 'total_flos': 1.050571298304e+16, 'train_loss': 0.022991722289864954, 'epoch': 20.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:11,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.6350515463917525,\n",
       " 'precision': 0.5849056603773585,\n",
       " 'f1_score': 0.6089479894129276}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4285\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2680\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='302' max='2680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 302/2680 02:47 < 22:10, 1.79 it/s, Epoch 2.25/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.045238</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.699003</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.699003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.048301</td>\n",
       "      <td>0.693215</td>\n",
       "      <td>0.709581</td>\n",
       "      <td>0.701303</td>\n",
       "      <td>0.693215</td>\n",
       "      <td>0.709581</td>\n",
       "      <td>0.701303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-134\n",
      "Configuration saved in ./t5/checkpoint-134/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-134\n",
      "Configuration saved in ./t5/checkpoint-134/config.json\n",
      "Model weights saved in ./t5/checkpoint-134/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-134/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-134/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-134/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-268\n",
      "Configuration saved in ./t5/checkpoint-268/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-268\n",
      "Configuration saved in ./t5/checkpoint-268/config.json\n",
      "Model weights saved in ./t5/checkpoint-268/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-268/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-268/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-268/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-402\n",
      "Configuration saved in ./t5/checkpoint-402/config.json\n",
      "Model weights saved in ./t5/checkpoint-402/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-402/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-402/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-402/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-134] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-536\n",
      "Configuration saved in ./t5/checkpoint-536/config.json\n",
      "Model weights saved in ./t5/checkpoint-536/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-536/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-536/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-536/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-670\n",
      "Configuration saved in ./t5/checkpoint-670/config.json\n",
      "Model weights saved in ./t5/checkpoint-670/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-670/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-670/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-670/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-536] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-804\n",
      "Configuration saved in ./t5/checkpoint-804/config.json\n",
      "Model weights saved in ./t5/checkpoint-804/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-804/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-804/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-804/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-670] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-938\n",
      "Configuration saved in ./t5/checkpoint-938/config.json\n",
      "Model weights saved in ./t5/checkpoint-938/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-938/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-938/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-938/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-402] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1072\n",
      "Configuration saved in ./t5/checkpoint-1072/config.json\n",
      "Model weights saved in ./t5/checkpoint-1072/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1072/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1072/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1072/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-804] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1206\n",
      "Configuration saved in ./t5/checkpoint-1206/config.json\n",
      "Model weights saved in ./t5/checkpoint-1206/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1206/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1206/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1206/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-938] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1340\n",
      "Configuration saved in ./t5/checkpoint-1340/config.json\n",
      "Model weights saved in ./t5/checkpoint-1340/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1340/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1340/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1340/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1206] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1474\n",
      "Configuration saved in ./t5/checkpoint-1474/config.json\n",
      "Model weights saved in ./t5/checkpoint-1474/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1474/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1474/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1474/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1072] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1608\n",
      "Configuration saved in ./t5/checkpoint-1608/config.json\n",
      "Model weights saved in ./t5/checkpoint-1608/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1608/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1608/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1608/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1340] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1742\n",
      "Configuration saved in ./t5/checkpoint-1742/config.json\n",
      "Model weights saved in ./t5/checkpoint-1742/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1742/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1742/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1742/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1474] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1876\n",
      "Configuration saved in ./t5/checkpoint-1876/config.json\n",
      "Model weights saved in ./t5/checkpoint-1876/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1876/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1876/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1876/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1608] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2010\n",
      "Configuration saved in ./t5/checkpoint-2010/config.json\n",
      "Model weights saved in ./t5/checkpoint-2010/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2010/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2010/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2010/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1742] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2144\n",
      "Configuration saved in ./t5/checkpoint-2144/config.json\n",
      "Model weights saved in ./t5/checkpoint-2144/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2144/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2144/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2144/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1876] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2278\n",
      "Configuration saved in ./t5/checkpoint-2278/config.json\n",
      "Model weights saved in ./t5/checkpoint-2278/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2278/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2278/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2278/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2412\n",
      "Configuration saved in ./t5/checkpoint-2412/config.json\n",
      "Model weights saved in ./t5/checkpoint-2412/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2412/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2412/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2412/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2278] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2680\n",
      "Configuration saved in ./t5/checkpoint-2680/config.json\n",
      "Model weights saved in ./t5/checkpoint-2680/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2680/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2680/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2680/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2546] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-2010 (score: 0.7695395781356316).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2680, training_loss=0.017107220226203774, metrics={'train_runtime': 1556.9113, 'train_samples_per_second': 55.045, 'train_steps_per_second': 1.721, 'total_flos': 1.4881646324736e+16, 'train_loss': 0.017107220226203774, 'epoch': 20.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:18,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.7354085603112841,\n",
       " 'precision': 0.6786355475763016,\n",
       " 'f1_score': 0.7058823529411765}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5600\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3500\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/3500 01:50 < 35:19, 1.57 it/s, Epoch 0.99/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=0.015522921905985901, metrics={'train_runtime': 2334.1643, 'train_samples_per_second': 47.983, 'train_steps_per_second': 1.499, 'total_flos': 2.091321247629312e+16, 'train_loss': 0.015522921905985901, 'epoch': 20.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:28,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5857988165680473,\n",
       " 'precision': 0.6309963099630996,\n",
       " 'f1_score': 0.6075581395348837}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8540\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='268' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 268/5340 02:51 < 54:21, 1.56 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-267\n",
      "Configuration saved in ./t5/checkpoint-267/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-267\n",
      "Configuration saved in ./t5/checkpoint-267/config.json\n",
      "Model weights saved in ./t5/checkpoint-267/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-267/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-267/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-267/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-534\n",
      "Configuration saved in ./t5/checkpoint-534/config.json\n",
      "Model weights saved in ./t5/checkpoint-534/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-534/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-534/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-534/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-801\n",
      "Configuration saved in ./t5/checkpoint-801/config.json\n",
      "Model weights saved in ./t5/checkpoint-801/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-801/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-801/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-801/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-267] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1068\n",
      "Configuration saved in ./t5/checkpoint-1068/config.json\n",
      "Model weights saved in ./t5/checkpoint-1068/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1068/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1068/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1068/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-534] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1335\n",
      "Configuration saved in ./t5/checkpoint-1335/config.json\n",
      "Model weights saved in ./t5/checkpoint-1335/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1335/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1335/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1335/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1068] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1602\n",
      "Configuration saved in ./t5/checkpoint-1602/config.json\n",
      "Model weights saved in ./t5/checkpoint-1602/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1602/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1602/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1602/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1335] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1869\n",
      "Configuration saved in ./t5/checkpoint-1869/config.json\n",
      "Model weights saved in ./t5/checkpoint-1869/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1869/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1869/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1869/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1602] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2136\n",
      "Configuration saved in ./t5/checkpoint-2136/config.json\n",
      "Model weights saved in ./t5/checkpoint-2136/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2136/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2136/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2136/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-801] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-2403\n",
      "Configuration saved in ./t5/checkpoint-2403/config.json\n",
      "Model weights saved in ./t5/checkpoint-2403/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2403/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2403/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2403/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1869] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2670\n",
      "Configuration saved in ./t5/checkpoint-2670/config.json\n",
      "Model weights saved in ./t5/checkpoint-2670/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2670/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2670/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2670/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2403] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-2937\n",
      "Configuration saved in ./t5/checkpoint-2937/config.json\n",
      "Model weights saved in ./t5/checkpoint-2937/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2937/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2937/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2937/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2670] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3204\n",
      "Configuration saved in ./t5/checkpoint-3204/config.json\n",
      "Model weights saved in ./t5/checkpoint-3204/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3204/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3204/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3204/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2937] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-3471\n",
      "Configuration saved in ./t5/checkpoint-3471/config.json\n",
      "Model weights saved in ./t5/checkpoint-3471/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3471/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3471/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3471/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3204] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3738\n",
      "Configuration saved in ./t5/checkpoint-3738/config.json\n",
      "Model weights saved in ./t5/checkpoint-3738/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3738/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3738/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3738/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3471] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-4005\n",
      "Configuration saved in ./t5/checkpoint-4005/config.json\n",
      "Model weights saved in ./t5/checkpoint-4005/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4005/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4005/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4005/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3738] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4272\n",
      "Configuration saved in ./t5/checkpoint-4272/config.json\n",
      "Model weights saved in ./t5/checkpoint-4272/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4272/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4272/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4272/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4005] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4539\n",
      "Configuration saved in ./t5/checkpoint-4539/config.json\n",
      "Model weights saved in ./t5/checkpoint-4539/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4539/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4539/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4539/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4806\n",
      "Configuration saved in ./t5/checkpoint-4806/config.json\n",
      "Model weights saved in ./t5/checkpoint-4806/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4806/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4806/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4806/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4539] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-5073\n",
      "Configuration saved in ./t5/checkpoint-5073/config.json\n",
      "Model weights saved in ./t5/checkpoint-5073/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-5073/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-5073/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-5073/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4806] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-5340\n",
      "Configuration saved in ./t5/checkpoint-5340/config.json\n",
      "Model weights saved in ./t5/checkpoint-5340/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-5340/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-5340/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-5340/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-5073] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-2136 (score: 1.0).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5340, training_loss=0.011299087121133948, metrics={'train_runtime': 3511.3328, 'train_samples_per_second': 48.642, 'train_steps_per_second': 1.521, 'total_flos': 3.188101983363072e+16, 'train_loss': 0.011299087121133948, 'epoch': 20.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:31,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.6880093131548312,\n",
       " 'precision': 0.7033096926713948,\n",
       " 'f1_score': 0.6955753735321548}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5838\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3660\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='733' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 733/3660 07:42 < 30:51, 1.58 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oasc Recall</th>\n",
       "      <th>Oasc Precision</th>\n",
       "      <th>Oasc F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.061083</td>\n",
       "      <td>0.391931</td>\n",
       "      <td>0.398256</td>\n",
       "      <td>0.395068</td>\n",
       "      <td>0.391931</td>\n",
       "      <td>0.398256</td>\n",
       "      <td>0.395068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.060917</td>\n",
       "      <td>0.458213</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.463733</td>\n",
       "      <td>0.458213</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.463733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.069981</td>\n",
       "      <td>0.469741</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.473216</td>\n",
       "      <td>0.469741</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.473216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/7 00:00 < 00:00, 4.76 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-183\n",
      "Configuration saved in ./t5/checkpoint-183/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-183\n",
      "Configuration saved in ./t5/checkpoint-183/config.json\n",
      "Model weights saved in ./t5/checkpoint-183/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-183/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-183/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-183/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-366\n",
      "Configuration saved in ./t5/checkpoint-366/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-366\n",
      "Configuration saved in ./t5/checkpoint-366/config.json\n",
      "Model weights saved in ./t5/checkpoint-366/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-366/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-366/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-366/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-549\n",
      "Configuration saved in ./t5/checkpoint-549/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-549\n",
      "Configuration saved in ./t5/checkpoint-549/config.json\n",
      "Model weights saved in ./t5/checkpoint-549/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-549/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-549/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-549/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-183] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-732\n",
      "Configuration saved in ./t5/checkpoint-732/config.json\n",
      "Model weights saved in ./t5/checkpoint-732/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-732/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-732/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-732/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-366] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-915\n",
      "Configuration saved in ./t5/checkpoint-915/config.json\n",
      "Model weights saved in ./t5/checkpoint-915/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-915/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-915/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-915/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-549] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1098\n",
      "Configuration saved in ./t5/checkpoint-1098/config.json\n",
      "Model weights saved in ./t5/checkpoint-1098/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1098/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1098/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1098/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-915] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1281\n",
      "Configuration saved in ./t5/checkpoint-1281/config.json\n",
      "Model weights saved in ./t5/checkpoint-1281/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1281/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1281/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1281/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1098] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1464\n",
      "Configuration saved in ./t5/checkpoint-1464/config.json\n",
      "Model weights saved in ./t5/checkpoint-1464/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1464/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1464/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1464/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1281] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1647\n",
      "Configuration saved in ./t5/checkpoint-1647/config.json\n",
      "Model weights saved in ./t5/checkpoint-1647/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1647/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1647/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1647/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1464] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1830\n",
      "Configuration saved in ./t5/checkpoint-1830/config.json\n",
      "Model weights saved in ./t5/checkpoint-1830/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1830/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1830/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1830/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1647] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2013\n",
      "Configuration saved in ./t5/checkpoint-2013/config.json\n",
      "Model weights saved in ./t5/checkpoint-2013/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2013/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2013/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2013/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1830] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2196\n",
      "Configuration saved in ./t5/checkpoint-2196/config.json\n",
      "Model weights saved in ./t5/checkpoint-2196/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2196/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2196/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2196/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2013] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2379\n",
      "Configuration saved in ./t5/checkpoint-2379/config.json\n",
      "Model weights saved in ./t5/checkpoint-2379/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2379/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2379/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2379/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2196] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2562\n",
      "Configuration saved in ./t5/checkpoint-2562/config.json\n",
      "Model weights saved in ./t5/checkpoint-2562/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2562/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2562/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2562/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2379] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2745\n",
      "Configuration saved in ./t5/checkpoint-2745/config.json\n",
      "Model weights saved in ./t5/checkpoint-2745/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2745/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2745/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2745/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2562] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2928\n",
      "Configuration saved in ./t5/checkpoint-2928/config.json\n",
      "Model weights saved in ./t5/checkpoint-2928/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2928/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2928/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2928/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2745] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3111\n",
      "Configuration saved in ./t5/checkpoint-3111/config.json\n",
      "Model weights saved in ./t5/checkpoint-3111/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3111/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3111/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3111/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2928] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3294\n",
      "Configuration saved in ./t5/checkpoint-3294/config.json\n",
      "Model weights saved in ./t5/checkpoint-3294/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3294/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3294/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3294/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3111] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3477\n",
      "Configuration saved in ./t5/checkpoint-3477/config.json\n",
      "Model weights saved in ./t5/checkpoint-3477/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3477/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3477/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3477/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3294] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3660\n",
      "Configuration saved in ./t5/checkpoint-3660/config.json\n",
      "Model weights saved in ./t5/checkpoint-3660/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3660/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3660/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3660/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3477] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-732 (score: 0.5224209673927859).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3660, training_loss=0.0151078967784679, metrics={'train_runtime': 2349.8352, 'train_samples_per_second': 49.689, 'train_steps_per_second': 1.558, 'total_flos': 1.555141912713216e+16, 'train_loss': 0.0151078967784679, 'epoch': 20.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:42,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.4641509433962264,\n",
       " 'precision': 0.462111801242236,\n",
       " 'f1_score': 0.46312912776133}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8848\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5540\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1386' max='5540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1386/5540 16:26 < 49:21, 1.40 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oasc Recall</th>\n",
       "      <th>Oasc Precision</th>\n",
       "      <th>Oasc F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.054857</td>\n",
       "      <td>0.529644</td>\n",
       "      <td>0.534791</td>\n",
       "      <td>0.532205</td>\n",
       "      <td>0.529644</td>\n",
       "      <td>0.534791</td>\n",
       "      <td>0.532205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.066848</td>\n",
       "      <td>0.527668</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.529699</td>\n",
       "      <td>0.527668</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.529699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.072750</td>\n",
       "      <td>0.561265</td>\n",
       "      <td>0.565737</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.561265</td>\n",
       "      <td>0.565737</td>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.079248</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.567323</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.567323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7/10 00:00 < 00:00, 6.61 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-277\n",
      "Configuration saved in ./t5/checkpoint-277/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-277\n",
      "Configuration saved in ./t5/checkpoint-277/config.json\n",
      "Model weights saved in ./t5/checkpoint-277/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-277/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-277/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-277/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-554\n",
      "Configuration saved in ./t5/checkpoint-554/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-554\n",
      "Configuration saved in ./t5/checkpoint-554/config.json\n",
      "Model weights saved in ./t5/checkpoint-554/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-554/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-554/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-554/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-831\n",
      "Configuration saved in ./t5/checkpoint-831/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-831\n",
      "Configuration saved in ./t5/checkpoint-831/config.json\n",
      "Model weights saved in ./t5/checkpoint-831/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-831/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-831/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-831/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-277] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1108\n",
      "Configuration saved in ./t5/checkpoint-1108/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-1108\n",
      "Configuration saved in ./t5/checkpoint-1108/config.json\n",
      "Model weights saved in ./t5/checkpoint-1108/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1108/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1108/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1108/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-554] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1385\n",
      "Configuration saved in ./t5/checkpoint-1385/config.json\n",
      "Model weights saved in ./t5/checkpoint-1385/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1385/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1385/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1385/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-831] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1662\n",
      "Configuration saved in ./t5/checkpoint-1662/config.json\n",
      "Model weights saved in ./t5/checkpoint-1662/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1662/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1662/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1662/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1108] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1939\n",
      "Configuration saved in ./t5/checkpoint-1939/config.json\n",
      "Model weights saved in ./t5/checkpoint-1939/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1939/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1939/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1939/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1385] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2216\n",
      "Configuration saved in ./t5/checkpoint-2216/config.json\n",
      "Model weights saved in ./t5/checkpoint-2216/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2216/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2216/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2216/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1662] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2493\n",
      "Configuration saved in ./t5/checkpoint-2493/config.json\n",
      "Model weights saved in ./t5/checkpoint-2493/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2493/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2493/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2493/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1939] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2770\n",
      "Configuration saved in ./t5/checkpoint-2770/config.json\n",
      "Model weights saved in ./t5/checkpoint-2770/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2770/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2770/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2770/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2493] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3047\n",
      "Configuration saved in ./t5/checkpoint-3047/config.json\n",
      "Model weights saved in ./t5/checkpoint-3047/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3047/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3047/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3047/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2770] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3324\n",
      "Configuration saved in ./t5/checkpoint-3324/config.json\n",
      "Model weights saved in ./t5/checkpoint-3324/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3324/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3324/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3324/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3047] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3601\n",
      "Configuration saved in ./t5/checkpoint-3601/config.json\n",
      "Model weights saved in ./t5/checkpoint-3601/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3601/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3601/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3601/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2216] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3878\n",
      "Configuration saved in ./t5/checkpoint-3878/config.json\n",
      "Model weights saved in ./t5/checkpoint-3878/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3878/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3878/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3878/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3324] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4155\n",
      "Configuration saved in ./t5/checkpoint-4155/config.json\n",
      "Model weights saved in ./t5/checkpoint-4155/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4155/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4155/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4155/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3878] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4432\n",
      "Configuration saved in ./t5/checkpoint-4432/config.json\n",
      "Model weights saved in ./t5/checkpoint-4432/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4432/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4432/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4432/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4155] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4709\n",
      "Configuration saved in ./t5/checkpoint-4709/config.json\n",
      "Model weights saved in ./t5/checkpoint-4709/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4709/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4709/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4709/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4432] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4986\n",
      "Configuration saved in ./t5/checkpoint-4986/config.json\n",
      "Model weights saved in ./t5/checkpoint-4986/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4986/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4986/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4986/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4709] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-5263\n",
      "Configuration saved in ./t5/checkpoint-5263/config.json\n",
      "Model weights saved in ./t5/checkpoint-5263/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-5263/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-5263/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-5263/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4986] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-5540\n",
      "Configuration saved in ./t5/checkpoint-5540/config.json\n",
      "Model weights saved in ./t5/checkpoint-5540/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-5540/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-5540/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-5540/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-5263] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-3601 (score: 0.5991919059194535).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5540, training_loss=0.011752677950268403, metrics={'train_runtime': 3969.6364, 'train_samples_per_second': 44.578, 'train_steps_per_second': 1.396, 'total_flos': 3.364751152447488e+16, 'train_loss': 0.011752677950268403, 'epoch': 20.0})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [01:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5857321652065082,\n",
       " 'precision': 0.5707317073170731,\n",
       " 'f1_score': 0.5781346510191475}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# William Hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 9380\n",
      "  Number of trainable parameters = 582401280\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4222' max='9380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4222/9380 1:44:56 < 2:08:16, 0.67 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.892100</td>\n",
       "      <td>0.103867</td>\n",
       "      <td>0.051710</td>\n",
       "      <td>0.091962</td>\n",
       "      <td>0.066197</td>\n",
       "      <td>0.051710</td>\n",
       "      <td>0.091962</td>\n",
       "      <td>0.066197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.021838</td>\n",
       "      <td>0.700326</td>\n",
       "      <td>0.731749</td>\n",
       "      <td>0.715692</td>\n",
       "      <td>0.700326</td>\n",
       "      <td>0.731749</td>\n",
       "      <td>0.715692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.019236</td>\n",
       "      <td>0.740635</td>\n",
       "      <td>0.772920</td>\n",
       "      <td>0.756433</td>\n",
       "      <td>0.740635</td>\n",
       "      <td>0.772920</td>\n",
       "      <td>0.756433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.787866</td>\n",
       "      <td>0.815579</td>\n",
       "      <td>0.801483</td>\n",
       "      <td>0.787866</td>\n",
       "      <td>0.815579</td>\n",
       "      <td>0.801483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.017104</td>\n",
       "      <td>0.802932</td>\n",
       "      <td>0.830109</td>\n",
       "      <td>0.816294</td>\n",
       "      <td>0.802932</td>\n",
       "      <td>0.830109</td>\n",
       "      <td>0.816294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.018516</td>\n",
       "      <td>0.799674</td>\n",
       "      <td>0.824517</td>\n",
       "      <td>0.811906</td>\n",
       "      <td>0.799674</td>\n",
       "      <td>0.824517</td>\n",
       "      <td>0.811906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.018313</td>\n",
       "      <td>0.820033</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.831919</td>\n",
       "      <td>0.820033</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.831919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.019332</td>\n",
       "      <td>0.815961</td>\n",
       "      <td>0.839615</td>\n",
       "      <td>0.827619</td>\n",
       "      <td>0.815961</td>\n",
       "      <td>0.839615</td>\n",
       "      <td>0.827619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/63 00:19 < 00:01, 3.00 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-469\n",
      "Configuration saved in ./output/checkpoint-469/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-469\n",
      "Configuration saved in ./output/checkpoint-469/config.json\n",
      "Model weights saved in ./output/checkpoint-469/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-469/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-469/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-469/spiece.model\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-938\n",
      "Configuration saved in ./output/checkpoint-938/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-938\n",
      "Configuration saved in ./output/checkpoint-938/config.json\n",
      "Model weights saved in ./output/checkpoint-938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-938/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-938/spiece.model\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1407\n",
      "Configuration saved in ./output/checkpoint-1407/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-1407\n",
      "Configuration saved in ./output/checkpoint-1407/config.json\n",
      "Model weights saved in ./output/checkpoint-1407/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1407/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1407/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-1407/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-469] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1876\n",
      "Configuration saved in ./output/checkpoint-1876/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-1876\n",
      "Configuration saved in ./output/checkpoint-1876/config.json\n",
      "Model weights saved in ./output/checkpoint-1876/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1876/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1876/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-1876/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-938] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-2345\n",
      "Configuration saved in ./output/checkpoint-2345/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-2345\n",
      "Configuration saved in ./output/checkpoint-2345/config.json\n",
      "Model weights saved in ./output/checkpoint-2345/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2345/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2345/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-2345/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-1407] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-2814\n",
      "Configuration saved in ./output/checkpoint-2814/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-2814\n",
      "Configuration saved in ./output/checkpoint-2814/config.json\n",
      "Model weights saved in ./output/checkpoint-2814/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2814/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2814/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-2814/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-1876] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3283\n",
      "Configuration saved in ./output/checkpoint-3283/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-3283\n",
      "Configuration saved in ./output/checkpoint-3283/config.json\n",
      "Model weights saved in ./output/checkpoint-3283/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3283/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3283/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-3283/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-2345] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3752\n",
      "Configuration saved in ./output/checkpoint-3752/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-3752\n",
      "Configuration saved in ./output/checkpoint-3752/config.json\n",
      "Model weights saved in ./output/checkpoint-3752/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3752/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3752/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-3752/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-2814] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-4221\n",
      "Configuration saved in ./output/checkpoint-4221/config.json\n",
      "Model weights saved in ./output/checkpoint-4221/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4221/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4221/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-4221/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-3752] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-4690\n",
      "Configuration saved in ./output/checkpoint-4690/config.json\n",
      "Model weights saved in ./output/checkpoint-4690/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4690/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4690/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-4690/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-4221] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-5159\n",
      "Configuration saved in ./output/checkpoint-5159/config.json\n",
      "Model weights saved in ./output/checkpoint-5159/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5159/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5159/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-5159/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-4690] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-5628\n",
      "Configuration saved in ./output/checkpoint-5628/config.json\n",
      "Model weights saved in ./output/checkpoint-5628/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5628/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5628/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-5628/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-3283] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-6097\n",
      "Configuration saved in ./output/checkpoint-6097/config.json\n",
      "Model weights saved in ./output/checkpoint-6097/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-6097/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6097/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-6097/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-5159] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-6566\n",
      "Configuration saved in ./output/checkpoint-6566/config.json\n",
      "Model weights saved in ./output/checkpoint-6566/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-6566/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6566/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-6566/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-5628] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-7035\n",
      "Configuration saved in ./output/checkpoint-7035/config.json\n",
      "Model weights saved in ./output/checkpoint-7035/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-7035/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7035/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-7035/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-6097] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-7504\n",
      "Configuration saved in ./output/checkpoint-7504/config.json\n",
      "Model weights saved in ./output/checkpoint-7504/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-7504/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7504/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-7504/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-6566] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-7973\n",
      "Configuration saved in ./output/checkpoint-7973/config.json\n",
      "Model weights saved in ./output/checkpoint-7973/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-7973/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7973/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-7973/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-7035] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-8442\n",
      "Configuration saved in ./output/checkpoint-8442/config.json\n",
      "Model weights saved in ./output/checkpoint-8442/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-8442/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8442/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-8442/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-7504] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-8911\n",
      "Configuration saved in ./output/checkpoint-8911/config.json\n",
      "Model weights saved in ./output/checkpoint-8911/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-8911/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8911/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-8911/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-8442] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-9380\n",
      "Configuration saved in ./output/checkpoint-9380/config.json\n",
      "Model weights saved in ./output/checkpoint-9380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-9380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9380/special_tokens_map.json\n",
      "Copy vocab file to ./output/checkpoint-9380/spiece.model\n",
      "Deleting older checkpoint [output/checkpoint-8911] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/checkpoint-7973 (score: 0.8465826966756145).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9380, training_loss=0.1555819763525971, metrics={'train_runtime': 14081.3567, 'train_samples_per_second': 21.305, 'train_steps_per_second': 0.666, 'total_flos': 1.2467638586821018e+17, 'train_loss': 0.1555819763525971, 'epoch': 20.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_id,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = william_tok[\"hotel\"][\"train\"],\n",
    "        eval_dataset = william_tok[\"hotel\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_id,william_2[\"hotel\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:34,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_id, william_tok[\"hotel\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in william_2[\"hotel\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.7692875599852345,\n",
       " 'precision': 0.817149569303054,\n",
       " 'f1_score': 0.79249657828438}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"william_hotel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
