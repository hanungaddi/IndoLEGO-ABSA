{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "sys.path.append(\"../../src/\")\n",
    "import data_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_dir = dict(\n",
    "    lap14 = \"../../data/absa/en/peng/14lap\",\n",
    "    res14 = \"../../data/absa/en/peng/14res\",\n",
    "    res15 = \"../../data/absa/en/peng/15res\",\n",
    "    res16 = \"../../data/absa/en/peng/16res\"\n",
    ")\n",
    "\n",
    "wan_dir = dict(\n",
    "    res15 = \"../../data/absa/en/wan/interim/rest15\",\n",
    "    res16 = \"../../data/absa/en/wan/interim/rest16\"\n",
    ")\n",
    "    \n",
    "zhang_dir = dict(\n",
    "    res15 = \"../../data/absa/en/zhang/interim/interim_2/rest15\",\n",
    "    res16 = \"../../data/absa/en/zhang/interim/interim_2/rest16\"\n",
    ")\n",
    "\n",
    "william_dir = dict(\n",
    "    hotel = \"../../data/absa/id/william\"\n",
    ")\n",
    "\n",
    "peng = dict(\n",
    "    lap14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res15\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res15\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res15\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res16\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res16\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res16\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")\n",
    "\n",
    "wan = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    )\n",
    ")\n",
    "\n",
    "zhang = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    )\n",
    ")\n",
    "\n",
    "william = dict(\n",
    "    hotel = dict(\n",
    "        train = data_utils.read_data(path=william_dir[\"hotel\"] + \"/train.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=william_dir[\"hotel\"] + \"/dev.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=william_dir[\"hotel\"] + \"/test.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.SENTIMENT_ELEMENT = {'a' : \"aspect\", 'o' : \"opinion\", 's' : \"sentiment\", 'c' : \"category\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AOS (ASTE)\n",
    "    * AO\n",
    "    * AS\n",
    "    * A\n",
    "    * O\n",
    "\n",
    "2. ACS (TASD)\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * C\n",
    "\n",
    "3. ACOS\n",
    "    * AO\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * O\n",
    "    * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oas', 'oa', 'as', 'a', 'o', 'asc', 'sc', 'c', 'oasc']\n"
     ]
    }
   ],
   "source": [
    "task_tree = {\n",
    "    \"oas\" : [\"oa\",\"as\",'a','o'],\n",
    "    \"asc\" : [\"as\",\"sc\",'a','c'],\n",
    "    \"oasc\" : [\"oa\",\"as\",\"sc\",'a','o','c']\n",
    "}\n",
    "\n",
    "all_task = []\n",
    "for k,v1 in task_tree.items():\n",
    "    if k not in all_task:\n",
    "        all_task.append(k)\n",
    "    for v2 in v1:\n",
    "        if v2 not in all_task:\n",
    "            all_task.append(v2)\n",
    "\n",
    "print(all_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.remove_duplicate_targets(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"ao\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle mix may not be a must, but we'll see it later. Will be problematic if like as (UABSA / E2E ABSA) used for training AOS (ASTE) --> may be for further experiment because we will insert imputing later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good', 'sentiment': 'mixed'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.handle_mix_sentiment(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"aos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Peng (ASTE/AOS)\n",
    "peng_intermediate = dict()\n",
    "\n",
    "for domain, v1 in peng.items():\n",
    "    peng_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        peng_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = peng[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            peng_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wan (TASD/ACS)\n",
    "wan_intermediate = dict()\n",
    "\n",
    "for domain, v1 in wan.items():\n",
    "    wan_intermediate[domain] = dict()\n",
    "    for task in [\"asc\"] + task_tree[\"asc\"]:\n",
    "        wan_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = wan[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            wan_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang (ACOS)\n",
    "zhang_intermediate = dict()\n",
    "\n",
    "for domain, v1 in zhang.items():\n",
    "    zhang_intermediate[domain] = dict()\n",
    "    for task in [\"oasc\"] + task_tree[\"oasc\"]:\n",
    "        zhang_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = zhang[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            zhang_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William (AOS ID)\n",
    "william_intermediate = dict()\n",
    "\n",
    "for domain, v1 in william.items():\n",
    "    william_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        william_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = william[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            william_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"<extra_id_X>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_answer(targets,se_order):\n",
    "    result = []\n",
    "    counter = 0\n",
    "    for t in targets:\n",
    "        constructed_t = \"\"\n",
    "        for se in se_order:\n",
    "            if counter > 99:\n",
    "                raise Exception(\"Extra id more than 99!\")\n",
    "            constructed_t += ' ' + mask.replace('X',str(counter)) + ' ' + t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "            counter += 1\n",
    "        constructed_t = constructed_t.strip()\n",
    "        result.append(constructed_t)\n",
    "    result = \" ; \".join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> no <extra_id_1> GUI <extra_id_2> negative ; <extra_id_3> dark <extra_id_4> screen <extra_id_5> negative ; <extra_id_6> steady <extra_id_7> power light <extra_id_8> neutral ; <extra_id_9> steady <extra_id_10> hard drive light <extra_id_11> negative'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(text,se_order):\n",
    "    prompt = []\n",
    "    for counter, se in enumerate(se_order):\n",
    "        prompt.append(data_utils.SENTIMENT_ELEMENT[se] + \" : \" + mask.replace('X',str(counter)))\n",
    "    prompt = \" ,\".join(prompt)\n",
    "    result = text + \"| \" + prompt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One night I turned the freaking thing off after using it , the next day I turn it on , no GUI , screen all dark , power light steady , hard drive light steady and not flashing as it usually does .| opinion : <extra_id_0> ,aspect : <extra_id_1> ,sentiment : <extra_id_2>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_prompt(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"text\"],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def catch_answer(output,se_order):\n",
    "    output = output.replace(\"<pad>\",'')\n",
    "    output = output.replace(\"</s>\",'')\n",
    "    pattern = r\"\"\n",
    "    for se in se_order:\n",
    "        if se != 's':\n",
    "            pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
    "        else:\n",
    "            pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
    "    found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "    for i in range(len(found)):\n",
    "        for k, v in found[i].items():\n",
    "            found[i][k] = found[i][k].strip()\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'opinion': 'no', 'aspect': 'GUI', 'sentiment': 'negative'},\n",
       " {'opinion': 'dark', 'aspect': 'screen', 'sentiment': 'negative'},\n",
       " {'opinion': 'steady', 'aspect': 'power light', 'sentiment': 'neutral'},\n",
       " {'opinion': 'steady', 'aspect': 'hard drive light', 'sentiment': 'negative'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")\n",
    "se_order = \"oas\"\n",
    "catch_answer(output,se_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> no <extra_id_1> GUI <extra_id_2> negative ; <extra_id_3> dark <extra_id_4> screen <extra_id_5> negative ; <extra_id_6> steady <extra_id_7> power light <extra_id_8> neutral ; <extra_id_9> steady <extra_id_10> hard drive light <extra_id_11> negative'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "peng_2 = dict()\n",
    "for domain, v1 in peng_intermediate.items():\n",
    "    peng_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in peng_intermediate[domain][basic_task][\"train\"]:\n",
    "            peng_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        peng_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        peng_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    peng_2[domain][\"train\"] = Dataset.from_list(peng_2[domain][\"train\"])\n",
    "    peng_2[domain][\"val\"] = Dataset.from_list(peng_2[domain][\"val\"])\n",
    "    peng_2[domain][\"test\"] = Dataset.from_list(peng_2[domain][\"test\"])\n",
    "\n",
    "wan_2 = dict()\n",
    "for domain, v1 in wan_intermediate.items():\n",
    "    wan_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"asc\"]:\n",
    "        for el in wan_intermediate[domain][basic_task][\"train\"]:\n",
    "            wan_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"val\"]:\n",
    "        wan_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"test\"]:\n",
    "        wan_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    wan_2[domain][\"train\"] = Dataset.from_list(wan_2[domain][\"train\"])\n",
    "    wan_2[domain][\"val\"] = Dataset.from_list(wan_2[domain][\"val\"])\n",
    "    wan_2[domain][\"test\"] = Dataset.from_list(wan_2[domain][\"test\"])\n",
    "\n",
    "zhang_2 = dict()\n",
    "for domain, v1 in zhang_intermediate.items():\n",
    "    zhang_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oasc\"]:\n",
    "        for el in zhang_intermediate[domain][basic_task][\"train\"]:\n",
    "            zhang_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"val\"]:\n",
    "        zhang_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"test\"]:\n",
    "        zhang_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    zhang_2[domain][\"train\"] = Dataset.from_list(zhang_2[domain][\"train\"])\n",
    "    zhang_2[domain][\"val\"] = Dataset.from_list(zhang_2[domain][\"val\"])\n",
    "    zhang_2[domain][\"test\"] = Dataset.from_list(zhang_2[domain][\"test\"])\n",
    "\n",
    "william_2 = dict()\n",
    "for domain, v1 in william_intermediate.items():\n",
    "    william_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in william_intermediate[domain][basic_task][\"train\"]:\n",
    "            william_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in william_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        william_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in william_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        william_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    william_2[domain][\"train\"] = Dataset.from_list(william_2[domain][\"train\"])\n",
    "    william_2[domain][\"val\"] = Dataset.from_list(william_2[domain][\"val\"])\n",
    "    william_2[domain][\"test\"] = Dataset.from_list(william_2[domain][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tempat yag bagus dan nyaman untuk istirahat tetapi tolong tvnya perlu di perbaiki channelnya karena banyak semutnya digambar dan water heaternya tidak bisa jadi mandi air dingin terus .| opinion : <extra_id_0> ,aspect : <extra_id_1>',\n",
       " 'output': '<extra_id_0> bagus <extra_id_1> tempat ; <extra_id_2> nyaman <extra_id_3> tempat ; <extra_id_4> perlu di perbaiki <extra_id_5> tvnya ; <extra_id_6> tidak bisa <extra_id_7> water heaternya',\n",
       " 'task': 'oa'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "william_2[\"hotel\"][\"train\"][69]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Tokenized Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_args = {\n",
    "    \"max_length\" : 512,\n",
    "    \"padding\" : True,\n",
    "    \"truncation\" : True,\n",
    "    \"return_tensors\" : \"pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_en(dataset):\n",
    "    result = tokenizer_en(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "peng_tok = dict()\n",
    "for domain, v1 in peng_2.items():\n",
    "    peng_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            peng_tok[domain][split] = peng_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            peng_tok[domain][split] = encode_en(peng_2[domain][split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "wan_tok = dict()\n",
    "for domain, v1 in wan_2.items():\n",
    "    wan_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            wan_tok[domain][split] = wan_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            wan_tok[domain][split] = encode_en(wan_2[domain][split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "zhang_tok = dict()\n",
    "for domain, v1 in zhang_2.items():\n",
    "    zhang_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            zhang_tok[domain][split] = zhang_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            zhang_tok[domain][split] = encode_en(zhang_2[domain][split])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_id(dataset):\n",
    "    result = tokenizer_id(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "william_tok = dict()\n",
    "for domain, v1 in william_2.items():\n",
    "    william_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            william_tok[domain][split] = william_2[domain][split].map(encode_id,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            william_tok[domain][split] = encode_id(william_2[domain][split])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator_en = DataCollatorForSeq2Seq(tokenizer=tokenizer_en)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_id = DataCollatorForSeq2Seq(tokenizer=tokenizer_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from evaluation import recall, precision, f1_score, summary_score\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def seperate_target_prediction_per_task(predictions:List[List[Dict]],targets:List[List[Dict]],tasks:List) -> Tuple[Dict[str,List],Dict[str,List]]:\n",
    "    per_task_targets = {}\n",
    "    per_task_predictions = {}\n",
    "    for target, prediction, task in zip(targets,predictions,tasks):\n",
    "        if task not in per_task_targets.keys():\n",
    "            per_task_targets[task] = []\n",
    "        if task not in per_task_predictions.keys():\n",
    "            per_task_predictions[task] = []\n",
    "        per_task_targets[task].append(target)\n",
    "        per_task_predictions[task].append(prediction)\n",
    "    return per_task_targets, per_task_predictions\n",
    "\n",
    "def preprocess_eval_preds(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer):\n",
    "    input_ids = eval_preds.inputs\n",
    "    target_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(input_ids, tuple):\n",
    "        input_ids = input_ids[0]\n",
    "    if isinstance(target_ids, tuple):\n",
    "        target_ids = target_ids[0]\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    input_ids = np.argmax(input_ids,axis=-1) if len(input_ids.shape) == 3 else input_ids # in case not predict with generate\n",
    "    target_ids = np.argmax(target_ids,axis=-1) if len(target_ids.shape) == 3 else target_ids # in case not predict with generate\n",
    "    prediction_ids = np.argmax(pred_ids,axis=-1) if len(pred_ids.shape) == 3 else pred_ids # in case not predict with generate\n",
    "\n",
    "    input_ids = [[token for token in row if token != -100] for row in input_ids]\n",
    "    target_ids = [[token for token in row if token != -100] for row in target_ids]\n",
    "    prediction_ids = [[token for token in row if token != -100] for row in prediction_ids]\n",
    "\n",
    "    inputs = tokenizer.batch_decode(input_ids,**decoding_args)\n",
    "    targets = tokenizer.batch_decode(target_ids,**decoding_args)\n",
    "    predictions = tokenizer.batch_decode(prediction_ids,**decoding_args)\n",
    "\n",
    "    return inputs, targets, predictions\n",
    "\n",
    "def compute_metrics(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer,tasks:List) -> Dict[str,float]: # MAY NOT BE SUFFICIATE FOR CAUSAL LM\n",
    "        \"\"\"\n",
    "        ### DESC\n",
    "            Method to compute the metrics.\n",
    "        ### PARAMS\n",
    "        * eval_preds: EvalPrediction instance from training.\n",
    "        * decoding_args: Decoding arguments.\n",
    "        ### RETURN\n",
    "        * metrics: Dictionary of metrics.\n",
    "        \"\"\"\n",
    "        inputs, targets, predictions = preprocess_eval_preds(eval_preds,decoding_args,tokenizer)\n",
    "\n",
    "        targets = [catch_answer(text,task) for text,task in zip(targets,tasks) if task != \"non_absa\"]\n",
    "        predictions = [catch_answer(text,task) for text,task in zip(predictions,tasks) if task != \"non_absa\"]\n",
    "\n",
    "\n",
    "        per_task_targets, per_task_predictions = seperate_target_prediction_per_task(predictions, targets, tasks)\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        metrics[\"overall_recall\"] = recall(predictions,targets)\n",
    "        metrics[\"overall_precision\"] = precision(predictions,targets)\n",
    "        metrics[\"overall_f1_score\"] = f1_score(predictions,targets)\n",
    "\n",
    "        for task in per_task_targets.keys():\n",
    "            if task == \"non_absa\":\n",
    "                continue\n",
    "            metrics[f\"{task}_recall\"] = recall(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_precision\"] = precision(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_f1_score\"] = f1_score(per_task_predictions[task],per_task_targets[task])\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_strategy\" : \"epoch\",\n",
    "    \"metric_for_best_model\": \"overall_f1_score\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"adam_epsilon\": 1e-08,\n",
    "    \"output_dir\": \"./t5\",\n",
    "    \"logging_dir\" : \"./t5/log\",\n",
    "    \"include_inputs_for_metrics\" : True\n",
    "}\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(**train_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# trainer = {\n",
    "#     \"peng\" : {},\n",
    "#     \"wan\" : {},\n",
    "#     \"zhang\" : {},\n",
    "#     \"william\" : {}\n",
    "# }\n",
    "\n",
    "decoding_args = {\n",
    "    \"skip_special_tokens\" : False\n",
    "}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    pred_logits = logits[0] if isinstance(logits,tuple) else logits\n",
    "    pred_ids = torch.argmax(pred_logits, dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_predictions(model,tokenizer,tokenized:torch.Tensor,device:torch.device=torch.device(\"cpu\"),batch_size:int=16,max_len:int=512,decoding_args:Dict={}) -> List[str]:\n",
    "    # Data loader\n",
    "    input_ids_data_loader = torch.utils.data.DataLoader(tokenized[\"input_ids\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    attention_mask_data_loader = torch.utils.data.DataLoader(tokenized[\"attention_mask\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    # Predict\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    tensor_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(zip(input_ids_data_loader,attention_mask_data_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            tensor_predictions.extend(model.generate(input_ids=input_ids,attention_mask=attention_mask,max_length=max_len,pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id).cpu())\n",
    "            input_ids = input_ids.cpu()\n",
    "            attention_mask = attention_mask.cpu()\n",
    "    tensor_predictions = [[token for token in row if token != -100] for row in tensor_predictions]\n",
    "    predictions = tokenizer.batch_decode(tensor_predictions,**decoding_args)\n",
    "    return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Laptop 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3624\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2280\n",
      "  Number of trainable parameters = 222903552\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='715' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 715/2280 05:00 < 11:00, 2.37 it/s, Epoch 6.26/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>0.164901</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.375905</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.375905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.145275</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.501767</td>\n",
       "      <td>0.450474</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.501767</td>\n",
       "      <td>0.450474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.146059</td>\n",
       "      <td>0.423188</td>\n",
       "      <td>0.522968</td>\n",
       "      <td>0.467817</td>\n",
       "      <td>0.423188</td>\n",
       "      <td>0.522968</td>\n",
       "      <td>0.467817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.145351</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.535836</td>\n",
       "      <td>0.488753</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.535836</td>\n",
       "      <td>0.488753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.187052</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.463023</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.463023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.131018</td>\n",
       "      <td>0.513043</td>\n",
       "      <td>0.599338</td>\n",
       "      <td>0.552843</td>\n",
       "      <td>0.513043</td>\n",
       "      <td>0.599338</td>\n",
       "      <td>0.552843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-114\n",
      "Configuration saved in ./t5/checkpoint-114/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-114\n",
      "Configuration saved in ./t5/checkpoint-114/config.json\n",
      "Model weights saved in ./t5/checkpoint-114/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-114/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-114/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-114/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-228\n",
      "Configuration saved in ./t5/checkpoint-228/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-228\n",
      "Configuration saved in ./t5/checkpoint-228/config.json\n",
      "Model weights saved in ./t5/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-228/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-228/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-159] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-342\n",
      "Configuration saved in ./t5/checkpoint-342/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-342\n",
      "Configuration saved in ./t5/checkpoint-342/config.json\n",
      "Model weights saved in ./t5/checkpoint-342/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-342/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-342/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-342/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-114] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-456\n",
      "Configuration saved in ./t5/checkpoint-456/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-456\n",
      "Configuration saved in ./t5/checkpoint-456/config.json\n",
      "Model weights saved in ./t5/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-456/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-456/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-228] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-570\n",
      "Configuration saved in ./t5/checkpoint-570/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-570\n",
      "Configuration saved in ./t5/checkpoint-570/config.json\n",
      "Model weights saved in ./t5/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-570/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-570/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-342] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-684\n",
      "Configuration saved in ./t5/checkpoint-684/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-684\n",
      "Configuration saved in ./t5/checkpoint-684/config.json\n",
      "Model weights saved in ./t5/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-684/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-684/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-456] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-798\n",
      "Configuration saved in ./t5/checkpoint-798/config.json\n",
      "Model weights saved in ./t5/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-798/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-798/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-570] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-912\n",
      "Configuration saved in ./t5/checkpoint-912/config.json\n",
      "Model weights saved in ./t5/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-912/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-912/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-798] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1026\n",
      "Configuration saved in ./t5/checkpoint-1026/config.json\n",
      "Model weights saved in ./t5/checkpoint-1026/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1026/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1026/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1026/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-912] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1140\n",
      "Configuration saved in ./t5/checkpoint-1140/config.json\n",
      "Model weights saved in ./t5/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1026] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1254\n",
      "Configuration saved in ./t5/checkpoint-1254/config.json\n",
      "Model weights saved in ./t5/checkpoint-1254/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1254/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1254/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1254/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1368\n",
      "Configuration saved in ./t5/checkpoint-1368/config.json\n",
      "Model weights saved in ./t5/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1368/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1368/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1254] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1482\n",
      "Configuration saved in ./t5/checkpoint-1482/config.json\n",
      "Model weights saved in ./t5/checkpoint-1482/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1482/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1482/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1482/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1368] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1596\n",
      "Configuration saved in ./t5/checkpoint-1596/config.json\n",
      "Model weights saved in ./t5/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1596/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1596/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-684] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1710\n",
      "Configuration saved in ./t5/checkpoint-1710/config.json\n",
      "Model weights saved in ./t5/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1710/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1710/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1482] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1824\n",
      "Configuration saved in ./t5/checkpoint-1824/config.json\n",
      "Model weights saved in ./t5/checkpoint-1824/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1824/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1824/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1824/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1710] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1938\n",
      "Configuration saved in ./t5/checkpoint-1938/config.json\n",
      "Model weights saved in ./t5/checkpoint-1938/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1938/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1938/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1938/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1824] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2052\n",
      "Configuration saved in ./t5/checkpoint-2052/config.json\n",
      "Model weights saved in ./t5/checkpoint-2052/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2052/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2052/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2052/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1938] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2166\n",
      "Configuration saved in ./t5/checkpoint-2166/config.json\n",
      "Model weights saved in ./t5/checkpoint-2166/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2166/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2166/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2166/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2052] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2280\n",
      "Configuration saved in ./t5/checkpoint-2280/config.json\n",
      "Model weights saved in ./t5/checkpoint-2280/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2280/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2280/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2280/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2166] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-1596 (score: 0.5613128311151312).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2280, training_loss=0.03503607097817095, metrics={'train_runtime': 970.8671, 'train_samples_per_second': 74.655, 'train_steps_per_second': 2.348, 'total_flos': 8965382543769600.0, 'train_loss': 0.03503607097817095, 'epoch': 20.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"lap14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"lap14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"lap14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:08,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"lap14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"oas\") for el in peng_2[\"lap14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5027726432532348,\n",
       " 'precision': 0.6570048309178744,\n",
       " 'f1_score': 0.5696335078534032}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed May 31 17:49:08 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    52W / 300W |  32501MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    53W / 300W |  31936MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    68W / 300W |  12828MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    53W / 300W |   1386MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    54W / 300W |  31937MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    54W / 300W |   1390MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    57W / 300W |    927MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   59C    P0   253W / 300W |  16267MiB / 32768MiB |     88%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     35316      C   /usr/bin/python3                  563MiB |\n",
      "|    0   N/A  N/A     41931      C   /usr/bin/python3                31933MiB |\n",
      "|    1   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    1   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    1   N/A  N/A     71915      C   /usr/bin/python3                31009MiB |\n",
      "|    2   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    2   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    2   N/A  N/A     49887      C   ...nda3/envs/absa/bin/python    11901MiB |\n",
      "|    3   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    3   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    3   N/A  N/A     71915      C   /usr/bin/python3                  459MiB |\n",
      "|    4   N/A  N/A     35316      C   /usr/bin/python3                31471MiB |\n",
      "|    4   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    5   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    5   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    5   N/A  N/A     71915      C   /usr/bin/python3                  459MiB |\n",
      "|    6   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    6   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    7   N/A  N/A     15422      C   gmx                               609MiB |\n",
      "|    7   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    7   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    7   N/A  N/A     76179      C   python3.8                       14731MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed May 31 17:49:09 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    52W / 300W |  32501MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    53W / 300W |  31936MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    67W / 300W |   5760MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    53W / 300W |   1386MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    53W / 300W |  31937MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    54W / 300W |   1390MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    57W / 300W |    927MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   58C    P0   172W / 300W |  16267MiB / 32768MiB |     69%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     35316      C   /usr/bin/python3                  563MiB |\n",
      "|    0   N/A  N/A     41931      C   /usr/bin/python3                31933MiB |\n",
      "|    1   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    1   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    1   N/A  N/A     71915      C   /usr/bin/python3                31009MiB |\n",
      "|    2   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    2   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    2   N/A  N/A     49887      C   ...nda3/envs/absa/bin/python     4833MiB |\n",
      "|    3   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    3   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    3   N/A  N/A     71915      C   /usr/bin/python3                  459MiB |\n",
      "|    4   N/A  N/A     35316      C   /usr/bin/python3                31471MiB |\n",
      "|    4   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    5   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    5   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    5   N/A  N/A     71915      C   /usr/bin/python3                  459MiB |\n",
      "|    6   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    6   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    7   N/A  N/A     15422      C   gmx                               609MiB |\n",
      "|    7   N/A  N/A     35316      C   /usr/bin/python3                  461MiB |\n",
      "|    7   N/A  N/A     41931      C   /usr/bin/python3                  461MiB |\n",
      "|    7   N/A  N/A     76179      C   python3.8                       14731MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5064\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/3180 : < :, Epoch 0.01/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-159\n",
      "Configuration saved in ./t5/checkpoint-159/config.json\n",
      "Model weights saved in ./t5/checkpoint-159/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-159/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-159/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-159/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-318\n",
      "Configuration saved in ./t5/checkpoint-318/config.json\n",
      "Model weights saved in ./t5/checkpoint-318/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-318/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-318/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-318/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-477\n",
      "Configuration saved in ./t5/checkpoint-477/config.json\n",
      "Model weights saved in ./t5/checkpoint-477/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-477/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-477/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-477/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-318] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-636\n",
      "Configuration saved in ./t5/checkpoint-636/config.json\n",
      "Model weights saved in ./t5/checkpoint-636/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-636/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-636/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-636/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-477] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-795\n",
      "Configuration saved in ./t5/checkpoint-795/config.json\n",
      "Model weights saved in ./t5/checkpoint-795/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-795/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-795/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-795/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-636] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-954\n",
      "Configuration saved in ./t5/checkpoint-954/config.json\n",
      "Model weights saved in ./t5/checkpoint-954/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-954/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-954/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-954/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-795] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1113\n",
      "Configuration saved in ./t5/checkpoint-1113/config.json\n",
      "Model weights saved in ./t5/checkpoint-1113/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1113/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1113/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1113/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-954] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1272\n",
      "Configuration saved in ./t5/checkpoint-1272/config.json\n",
      "Model weights saved in ./t5/checkpoint-1272/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1272/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1272/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1272/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1113] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1431\n",
      "Configuration saved in ./t5/checkpoint-1431/config.json\n",
      "Model weights saved in ./t5/checkpoint-1431/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1431/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1431/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1431/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-159] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1590\n",
      "Configuration saved in ./t5/checkpoint-1590/config.json\n",
      "Model weights saved in ./t5/checkpoint-1590/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1590/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1590/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1590/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1749\n",
      "Configuration saved in ./t5/checkpoint-1749/config.json\n",
      "Model weights saved in ./t5/checkpoint-1749/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1749/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1749/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1749/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1590] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1908\n",
      "Configuration saved in ./t5/checkpoint-1908/config.json\n",
      "Model weights saved in ./t5/checkpoint-1908/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1908/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1908/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1908/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1749] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2067\n",
      "Configuration saved in ./t5/checkpoint-2067/config.json\n",
      "Model weights saved in ./t5/checkpoint-2067/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2067/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2067/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2067/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1908] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2226\n",
      "Configuration saved in ./t5/checkpoint-2226/config.json\n",
      "Model weights saved in ./t5/checkpoint-2226/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2226/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2226/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2226/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2067] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2385\n",
      "Configuration saved in ./t5/checkpoint-2385/config.json\n",
      "Model weights saved in ./t5/checkpoint-2385/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2385/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2385/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2385/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2226] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2544\n",
      "Configuration saved in ./t5/checkpoint-2544/config.json\n",
      "Model weights saved in ./t5/checkpoint-2544/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2544/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2544/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2544/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1431] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2703\n",
      "Configuration saved in ./t5/checkpoint-2703/config.json\n",
      "Model weights saved in ./t5/checkpoint-2703/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2703/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2703/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2703/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2385] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2862\n",
      "Configuration saved in ./t5/checkpoint-2862/config.json\n",
      "Model weights saved in ./t5/checkpoint-2862/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2862/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2862/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2862/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2703] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3021\n",
      "Configuration saved in ./t5/checkpoint-3021/config.json\n",
      "Model weights saved in ./t5/checkpoint-3021/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3021/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3021/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3021/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2862] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3180\n",
      "Configuration saved in ./t5/checkpoint-3180/config.json\n",
      "Model weights saved in ./t5/checkpoint-3180/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3180/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3180/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3180/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3021] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-2544 (score: 0.534306627258842).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3180, training_loss=0.02443723297156628, metrics={'train_runtime': 1663.3771, 'train_samples_per_second': 60.888, 'train_steps_per_second': 1.912, 'total_flos': 1.397309066698752e+16, 'train_loss': 0.02443723297156628, 'epoch': 20.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:17,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"oas\") for el in peng_2[\"res14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.4305835010060362,\n",
       " 'precision': 0.7210084033613445,\n",
       " 'f1_score': 0.5391742012021976}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2420\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1520\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='1520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  77/1520 00:34 < 10:57, 2.20 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-76\n",
      "Configuration saved in ./t5/checkpoint-76/config.json\n",
      "Model weights saved in ./t5/checkpoint-76/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-76/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-76/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-76/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-152\n",
      "Configuration saved in ./t5/checkpoint-152/config.json\n",
      "Model weights saved in ./t5/checkpoint-152/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-152/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-152/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-152/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-228\n",
      "Configuration saved in ./t5/checkpoint-228/config.json\n",
      "Model weights saved in ./t5/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-228/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-228/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-76] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-304\n",
      "Configuration saved in ./t5/checkpoint-304/config.json\n",
      "Model weights saved in ./t5/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-304/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-304/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-152] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-380\n",
      "Configuration saved in ./t5/checkpoint-380/config.json\n",
      "Model weights saved in ./t5/checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-380/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-380/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-304] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-456\n",
      "Configuration saved in ./t5/checkpoint-456/config.json\n",
      "Model weights saved in ./t5/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-456/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-456/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-532\n",
      "Configuration saved in ./t5/checkpoint-532/config.json\n",
      "Model weights saved in ./t5/checkpoint-532/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-532/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-532/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-532/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-228] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-608\n",
      "Configuration saved in ./t5/checkpoint-608/config.json\n",
      "Model weights saved in ./t5/checkpoint-608/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-608/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-608/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-608/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-456] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-684\n",
      "Configuration saved in ./t5/checkpoint-684/config.json\n",
      "Model weights saved in ./t5/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-684/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-684/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-608] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-760\n",
      "Configuration saved in ./t5/checkpoint-760/config.json\n",
      "Model weights saved in ./t5/checkpoint-760/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-760/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-760/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-760/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-684] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-836\n",
      "Configuration saved in ./t5/checkpoint-836/config.json\n",
      "Model weights saved in ./t5/checkpoint-836/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-836/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-836/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-836/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-760] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-912\n",
      "Configuration saved in ./t5/checkpoint-912/config.json\n",
      "Model weights saved in ./t5/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-912/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-912/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-836] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-988\n",
      "Configuration saved in ./t5/checkpoint-988/config.json\n",
      "Model weights saved in ./t5/checkpoint-988/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-988/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-988/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-988/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-912] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1064\n",
      "Configuration saved in ./t5/checkpoint-1064/config.json\n",
      "Model weights saved in ./t5/checkpoint-1064/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1064/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1064/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1064/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-988] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1140\n",
      "Configuration saved in ./t5/checkpoint-1140/config.json\n",
      "Model weights saved in ./t5/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1064] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1216\n",
      "Configuration saved in ./t5/checkpoint-1216/config.json\n",
      "Model weights saved in ./t5/checkpoint-1216/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1216/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1216/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1216/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1292\n",
      "Configuration saved in ./t5/checkpoint-1292/config.json\n",
      "Model weights saved in ./t5/checkpoint-1292/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1292/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1292/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1292/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1216] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1368\n",
      "Configuration saved in ./t5/checkpoint-1368/config.json\n",
      "Model weights saved in ./t5/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1368/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1368/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1292] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1444\n",
      "Configuration saved in ./t5/checkpoint-1444/config.json\n",
      "Model weights saved in ./t5/checkpoint-1444/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1444/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1444/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1444/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1368] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1520\n",
      "Configuration saved in ./t5/checkpoint-1520/config.json\n",
      "Model weights saved in ./t5/checkpoint-1520/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1520/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1520/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1520/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1444] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-532 (score: 0.6714377654781826).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1520, training_loss=0.03920878225104197, metrics={'train_runtime': 766.7602, 'train_samples_per_second': 63.123, 'train_steps_per_second': 1.982, 'total_flos': 7138128273408000.0, 'train_loss': 0.03920878225104197, 'epoch': 20.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:09,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"oas\") for el in peng_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5298969072164949,\n",
       " 'precision': 0.6277372262773723,\n",
       " 'f1_score': 0.5746824581702831}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3428\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2160\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 109/2160 00:44 < 14:17, 2.39 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-108\n",
      "Configuration saved in ./t5/checkpoint-108/config.json\n",
      "Model weights saved in ./t5/checkpoint-108/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-108/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-108/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-108/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-216\n",
      "Configuration saved in ./t5/checkpoint-216/config.json\n",
      "Model weights saved in ./t5/checkpoint-216/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-216/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-216/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-216/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-324\n",
      "Configuration saved in ./t5/checkpoint-324/config.json\n",
      "Model weights saved in ./t5/checkpoint-324/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-324/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-324/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-324/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-108] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-432\n",
      "Configuration saved in ./t5/checkpoint-432/config.json\n",
      "Model weights saved in ./t5/checkpoint-432/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-432/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-432/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-432/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-324] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-540\n",
      "Configuration saved in ./t5/checkpoint-540/config.json\n",
      "Model weights saved in ./t5/checkpoint-540/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-540/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-540/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-540/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-216] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-648\n",
      "Configuration saved in ./t5/checkpoint-648/config.json\n",
      "Model weights saved in ./t5/checkpoint-648/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-648/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-648/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-648/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-432] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-756\n",
      "Configuration saved in ./t5/checkpoint-756/config.json\n",
      "Model weights saved in ./t5/checkpoint-756/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-756/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-756/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-756/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-648] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-864\n",
      "Configuration saved in ./t5/checkpoint-864/config.json\n",
      "Model weights saved in ./t5/checkpoint-864/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-864/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-864/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-864/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-540] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-972\n",
      "Configuration saved in ./t5/checkpoint-972/config.json\n",
      "Model weights saved in ./t5/checkpoint-972/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-972/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-972/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-972/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-756] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1080\n",
      "Configuration saved in ./t5/checkpoint-1080/config.json\n",
      "Model weights saved in ./t5/checkpoint-1080/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1080/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1080/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1080/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-972] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1188\n",
      "Configuration saved in ./t5/checkpoint-1188/config.json\n",
      "Model weights saved in ./t5/checkpoint-1188/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1188/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1188/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1188/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1080] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1296\n",
      "Configuration saved in ./t5/checkpoint-1296/config.json\n",
      "Model weights saved in ./t5/checkpoint-1296/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1296/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1296/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1296/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1188] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1404\n",
      "Configuration saved in ./t5/checkpoint-1404/config.json\n",
      "Model weights saved in ./t5/checkpoint-1404/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1404/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1404/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1404/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1296] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1512\n",
      "Configuration saved in ./t5/checkpoint-1512/config.json\n",
      "Model weights saved in ./t5/checkpoint-1512/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1512/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1512/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1512/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1404] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1620\n",
      "Configuration saved in ./t5/checkpoint-1620/config.json\n",
      "Model weights saved in ./t5/checkpoint-1620/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1620/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1620/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1620/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1728\n",
      "Configuration saved in ./t5/checkpoint-1728/config.json\n",
      "Model weights saved in ./t5/checkpoint-1728/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1728/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1728/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1728/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1620] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1836\n",
      "Configuration saved in ./t5/checkpoint-1836/config.json\n",
      "Model weights saved in ./t5/checkpoint-1836/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1836/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1836/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1836/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1728] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1944\n",
      "Configuration saved in ./t5/checkpoint-1944/config.json\n",
      "Model weights saved in ./t5/checkpoint-1944/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1944/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1944/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1944/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1836] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2052\n",
      "Configuration saved in ./t5/checkpoint-2052/config.json\n",
      "Model weights saved in ./t5/checkpoint-2052/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2052/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2052/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2052/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1944] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2160\n",
      "Configuration saved in ./t5/checkpoint-2160/config.json\n",
      "Model weights saved in ./t5/checkpoint-2160/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2160/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2160/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2160/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2052] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-864 (score: 0.7025614218504966).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2160, training_loss=0.030442173609992972, metrics={'train_runtime': 1011.8291, 'train_samples_per_second': 67.758, 'train_steps_per_second': 2.135, 'total_flos': 1.011127478188032e+16, 'train_loss': 0.030442173609992972, 'epoch': 20.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:12,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"oas\") for el in peng_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.603112840466926,\n",
       " 'precision': 0.6813186813186813,\n",
       " 'f1_score': 0.6398348813209495}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4480\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2800\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 141/2800 01:03 < 20:18, 2.18 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-140\n",
      "Configuration saved in ./t5/checkpoint-140/config.json\n",
      "Model weights saved in ./t5/checkpoint-140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-140/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-280\n",
      "Configuration saved in ./t5/checkpoint-280/config.json\n",
      "Model weights saved in ./t5/checkpoint-280/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-280/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-280/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-280/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-420\n",
      "Configuration saved in ./t5/checkpoint-420/config.json\n",
      "Model weights saved in ./t5/checkpoint-420/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-420/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-420/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-420/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-560\n",
      "Configuration saved in ./t5/checkpoint-560/config.json\n",
      "Model weights saved in ./t5/checkpoint-560/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-560/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-560/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-560/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-700\n",
      "Configuration saved in ./t5/checkpoint-700/config.json\n",
      "Model weights saved in ./t5/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-700/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-700/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-560] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-840\n",
      "Configuration saved in ./t5/checkpoint-840/config.json\n",
      "Model weights saved in ./t5/checkpoint-840/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-840/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-840/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-840/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-980\n",
      "Configuration saved in ./t5/checkpoint-980/config.json\n",
      "Model weights saved in ./t5/checkpoint-980/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-980/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-980/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-980/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-840] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1120\n",
      "Configuration saved in ./t5/checkpoint-1120/config.json\n",
      "Model weights saved in ./t5/checkpoint-1120/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1120/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1120/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1120/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-980] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1260\n",
      "Configuration saved in ./t5/checkpoint-1260/config.json\n",
      "Model weights saved in ./t5/checkpoint-1260/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1260/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1260/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1260/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1400\n",
      "Configuration saved in ./t5/checkpoint-1400/config.json\n",
      "Model weights saved in ./t5/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1400/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1400/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1540\n",
      "Configuration saved in ./t5/checkpoint-1540/config.json\n",
      "Model weights saved in ./t5/checkpoint-1540/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1540/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1540/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1540/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1680\n",
      "Configuration saved in ./t5/checkpoint-1680/config.json\n",
      "Model weights saved in ./t5/checkpoint-1680/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1680/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1680/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1680/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1540] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1820\n",
      "Configuration saved in ./t5/checkpoint-1820/config.json\n",
      "Model weights saved in ./t5/checkpoint-1820/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1820/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1820/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1820/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1680] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1960\n",
      "Configuration saved in ./t5/checkpoint-1960/config.json\n",
      "Model weights saved in ./t5/checkpoint-1960/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1960/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1960/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1960/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1820] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2100\n",
      "Configuration saved in ./t5/checkpoint-2100/config.json\n",
      "Model weights saved in ./t5/checkpoint-2100/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2100/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2100/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2100/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1960] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2240\n",
      "Configuration saved in ./t5/checkpoint-2240/config.json\n",
      "Model weights saved in ./t5/checkpoint-2240/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2240/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2240/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2240/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2380\n",
      "Configuration saved in ./t5/checkpoint-2380/config.json\n",
      "Model weights saved in ./t5/checkpoint-2380/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2380/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2380/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2380/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2240] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-2520\n",
      "Configuration saved in ./t5/checkpoint-2520/config.json\n",
      "Model weights saved in ./t5/checkpoint-2520/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2520/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2520/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2520/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2380] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-2660\n",
      "Configuration saved in ./t5/checkpoint-2660/config.json\n",
      "Model weights saved in ./t5/checkpoint-2660/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2660/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2660/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2660/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2800\n",
      "Configuration saved in ./t5/checkpoint-2800/config.json\n",
      "Model weights saved in ./t5/checkpoint-2800/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2800/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2800/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2660] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-420 (score: 0.6956521739130435).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2800, training_loss=0.02643859689789159, metrics={'train_runtime': 1378.2213, 'train_samples_per_second': 65.011, 'train_steps_per_second': 2.032, 'total_flos': 1.438663016448e+16, 'train_loss': 0.02643859689789159, 'epoch': 20.0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:08,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"asc\") for el in wan_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.2875739644970414,\n",
       " 'precision': 0.5560640732265446,\n",
       " 'f1_score': 0.3790951638065523}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6832\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4280\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='4280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 215/4280 01:37 < 30:58, 2.19 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-214\n",
      "Configuration saved in ./t5/checkpoint-214/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-214\n",
      "Configuration saved in ./t5/checkpoint-214/config.json\n",
      "Model weights saved in ./t5/checkpoint-214/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-214/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-214/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-214/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-428\n",
      "Configuration saved in ./t5/checkpoint-428/config.json\n",
      "Model weights saved in ./t5/checkpoint-428/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-428/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-428/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-428/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-856\n",
      "Configuration saved in ./t5/checkpoint-856/config.json\n",
      "Model weights saved in ./t5/checkpoint-856/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-856/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-856/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-856/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-428] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1070\n",
      "Configuration saved in ./t5/checkpoint-1070/config.json\n",
      "Model weights saved in ./t5/checkpoint-1070/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1070/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1070/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1070/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-642] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1284\n",
      "Configuration saved in ./t5/checkpoint-1284/config.json\n",
      "Model weights saved in ./t5/checkpoint-1284/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1284/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1284/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1284/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-856] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-1498\n",
      "Configuration saved in ./t5/checkpoint-1498/config.json\n",
      "Model weights saved in ./t5/checkpoint-1498/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1498/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1498/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1498/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1712\n",
      "Configuration saved in ./t5/checkpoint-1712/config.json\n",
      "Model weights saved in ./t5/checkpoint-1712/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1712/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1712/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1712/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1070] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1926\n",
      "Configuration saved in ./t5/checkpoint-1926/config.json\n",
      "Model weights saved in ./t5/checkpoint-1926/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1926/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1926/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1926/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1498] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2140\n",
      "Configuration saved in ./t5/checkpoint-2140/config.json\n",
      "Model weights saved in ./t5/checkpoint-2140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1926] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2354\n",
      "Configuration saved in ./t5/checkpoint-2354/config.json\n",
      "Model weights saved in ./t5/checkpoint-2354/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2354/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2354/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2354/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2568\n",
      "Configuration saved in ./t5/checkpoint-2568/config.json\n",
      "Model weights saved in ./t5/checkpoint-2568/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2568/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2568/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2568/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1712] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-2782\n",
      "Configuration saved in ./t5/checkpoint-2782/config.json\n",
      "Model weights saved in ./t5/checkpoint-2782/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2782/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2782/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2782/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2354] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2996\n",
      "Configuration saved in ./t5/checkpoint-2996/config.json\n",
      "Model weights saved in ./t5/checkpoint-2996/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2996/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2996/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2996/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2782] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3210\n",
      "Configuration saved in ./t5/checkpoint-3210/config.json\n",
      "Model weights saved in ./t5/checkpoint-3210/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3210/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3210/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3210/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2996] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3424\n",
      "Configuration saved in ./t5/checkpoint-3424/config.json\n",
      "Model weights saved in ./t5/checkpoint-3424/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3424/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3424/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3424/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3638\n",
      "Configuration saved in ./t5/checkpoint-3638/config.json\n",
      "Model weights saved in ./t5/checkpoint-3638/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3638/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3638/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3638/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3424] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3852\n",
      "Configuration saved in ./t5/checkpoint-3852/config.json\n",
      "Model weights saved in ./t5/checkpoint-3852/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3852/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3852/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3852/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3638] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4066\n",
      "Configuration saved in ./t5/checkpoint-4066/config.json\n",
      "Model weights saved in ./t5/checkpoint-4066/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4066/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4066/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4066/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3852] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4280\n",
      "Configuration saved in ./t5/checkpoint-4280/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-4280\n",
      "Configuration saved in ./t5/checkpoint-4280/config.json\n",
      "Model weights saved in ./t5/checkpoint-4280/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4280/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4280/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4280/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4066] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-2568 (score: 0.7341772151898734).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4280, training_loss=0.019733498087065798, metrics={'train_runtime': 2028.5889, 'train_samples_per_second': 67.357, 'train_steps_per_second': 2.11, 'total_flos': 2.1939611000832e+16, 'train_loss': 0.019733498087065798, 'epoch': 20.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:11,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"asc\") for el in wan_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.38766006984866125,\n",
       " 'precision': 0.6121323529411765,\n",
       " 'f1_score': 0.4746970776906629}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5004\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3140\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='3140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  22/3140 00:07 < 19:57, 2.60 it/s, Epoch 0.13/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-157\n",
      "Configuration saved in ./t5/checkpoint-157/config.json\n",
      "Model weights saved in ./t5/checkpoint-157/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-157/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-157/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-157/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-314\n",
      "Configuration saved in ./t5/checkpoint-314/config.json\n",
      "Model weights saved in ./t5/checkpoint-314/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-314/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-314/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-314/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-471\n",
      "Configuration saved in ./t5/checkpoint-471/config.json\n",
      "Model weights saved in ./t5/checkpoint-471/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-471/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-471/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-471/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-314] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-628\n",
      "Configuration saved in ./t5/checkpoint-628/config.json\n",
      "Model weights saved in ./t5/checkpoint-628/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-628/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-628/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-628/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-471] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-785\n",
      "Configuration saved in ./t5/checkpoint-785/config.json\n",
      "Model weights saved in ./t5/checkpoint-785/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-785/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-785/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-785/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-628] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-942\n",
      "Configuration saved in ./t5/checkpoint-942/config.json\n",
      "Model weights saved in ./t5/checkpoint-942/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-942/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-942/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-942/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-785] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1099\n",
      "Configuration saved in ./t5/checkpoint-1099/config.json\n",
      "Model weights saved in ./t5/checkpoint-1099/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1099/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1099/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1099/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-942] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1256\n",
      "Configuration saved in ./t5/checkpoint-1256/config.json\n",
      "Model weights saved in ./t5/checkpoint-1256/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1256/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1256/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1256/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1099] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1413\n",
      "Configuration saved in ./t5/checkpoint-1413/config.json\n",
      "Model weights saved in ./t5/checkpoint-1413/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1413/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1413/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1413/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1570\n",
      "Configuration saved in ./t5/checkpoint-1570/config.json\n",
      "Model weights saved in ./t5/checkpoint-1570/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1570/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1570/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1570/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1413] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1727\n",
      "Configuration saved in ./t5/checkpoint-1727/config.json\n",
      "Model weights saved in ./t5/checkpoint-1727/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1727/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1727/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1727/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1570] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1884\n",
      "Configuration saved in ./t5/checkpoint-1884/config.json\n",
      "Model weights saved in ./t5/checkpoint-1884/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1884/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1884/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1884/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1727] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2041\n",
      "Configuration saved in ./t5/checkpoint-2041/config.json\n",
      "Model weights saved in ./t5/checkpoint-2041/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2041/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2041/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2041/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1884] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2198\n",
      "Configuration saved in ./t5/checkpoint-2198/config.json\n",
      "Model weights saved in ./t5/checkpoint-2198/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2198/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2198/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2198/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2041] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2355\n",
      "Configuration saved in ./t5/checkpoint-2355/config.json\n",
      "Model weights saved in ./t5/checkpoint-2355/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2355/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2355/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2355/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2198] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2512\n",
      "Configuration saved in ./t5/checkpoint-2512/config.json\n",
      "Model weights saved in ./t5/checkpoint-2512/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2512/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2512/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2512/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2355] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2669\n",
      "Configuration saved in ./t5/checkpoint-2669/config.json\n",
      "Model weights saved in ./t5/checkpoint-2669/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2669/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2669/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2669/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2826\n",
      "Configuration saved in ./t5/checkpoint-2826/config.json\n",
      "Model weights saved in ./t5/checkpoint-2826/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2826/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2826/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2826/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2669] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2983\n",
      "Configuration saved in ./t5/checkpoint-2983/config.json\n",
      "Model weights saved in ./t5/checkpoint-2983/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2983/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2983/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2983/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2826] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3140\n",
      "Configuration saved in ./t5/checkpoint-3140/config.json\n",
      "Model weights saved in ./t5/checkpoint-3140/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3140/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3140/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3140/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2983] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-157 (score: 0.12403100775193798).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3140, training_loss=0.02554370107212264, metrics={'train_runtime': 1304.0701, 'train_samples_per_second': 76.744, 'train_steps_per_second': 2.408, 'total_flos': 1.08319336280064e+16, 'train_loss': 0.02554370107212264, 'epoch': 20.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:13,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"oasc\") for el in zhang_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.11949685534591195,\n",
       " 'precision': 0.37109375,\n",
       " 'f1_score': 0.1807802093244529}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7584\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4740\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='4740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  76/4740 00:34 < 36:27, 2.13 it/s, Epoch 0.32/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-237\n",
      "Configuration saved in ./t5/checkpoint-237/config.json\n",
      "Model weights saved in ./t5/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-237/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-237/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-474\n",
      "Configuration saved in ./t5/checkpoint-474/config.json\n",
      "Model weights saved in ./t5/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-474/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-474/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-711\n",
      "Configuration saved in ./t5/checkpoint-711/config.json\n",
      "Model weights saved in ./t5/checkpoint-711/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-711/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-711/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-711/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-237] due to args.save_total_limit\n",
      "Saving model checkpoint to ./t5/checkpoint-948\n",
      "Configuration saved in ./t5/checkpoint-948/config.json\n",
      "Model weights saved in ./t5/checkpoint-948/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-948/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-948/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-948/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-711] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1185\n",
      "Configuration saved in ./t5/checkpoint-1185/config.json\n",
      "Model weights saved in ./t5/checkpoint-1185/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1185/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1185/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1185/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-948] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1422\n",
      "Configuration saved in ./t5/checkpoint-1422/config.json\n",
      "Model weights saved in ./t5/checkpoint-1422/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1422/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1422/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1422/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1185] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1659\n",
      "Configuration saved in ./t5/checkpoint-1659/config.json\n",
      "Model weights saved in ./t5/checkpoint-1659/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1659/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1659/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1659/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1422] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1896\n",
      "Configuration saved in ./t5/checkpoint-1896/config.json\n",
      "Model weights saved in ./t5/checkpoint-1896/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1896/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1896/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1896/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1659] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2133\n",
      "Configuration saved in ./t5/checkpoint-2133/config.json\n",
      "Model weights saved in ./t5/checkpoint-2133/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2133/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2133/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2133/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-474] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2370\n",
      "Configuration saved in ./t5/checkpoint-2370/config.json\n",
      "Model weights saved in ./t5/checkpoint-2370/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2370/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2370/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2370/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1896] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2607\n",
      "Configuration saved in ./t5/checkpoint-2607/config.json\n",
      "Model weights saved in ./t5/checkpoint-2607/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2607/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2607/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2607/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2370] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2844\n",
      "Configuration saved in ./t5/checkpoint-2844/config.json\n",
      "Model weights saved in ./t5/checkpoint-2844/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2844/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2844/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2844/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2607] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3081\n",
      "Configuration saved in ./t5/checkpoint-3081/config.json\n",
      "Model weights saved in ./t5/checkpoint-3081/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3081/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3081/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3081/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2844] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3318\n",
      "Configuration saved in ./t5/checkpoint-3318/config.json\n",
      "Model weights saved in ./t5/checkpoint-3318/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3318/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3318/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3318/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3081] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3555\n",
      "Configuration saved in ./t5/checkpoint-3555/config.json\n",
      "Model weights saved in ./t5/checkpoint-3555/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3555/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3555/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3555/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3318] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3792\n",
      "Configuration saved in ./t5/checkpoint-3792/config.json\n",
      "Model weights saved in ./t5/checkpoint-3792/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3792/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3792/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3792/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3555] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4029\n",
      "Configuration saved in ./t5/checkpoint-4029/config.json\n",
      "Model weights saved in ./t5/checkpoint-4029/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4029/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4029/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4029/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2133] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4266\n",
      "Configuration saved in ./t5/checkpoint-4266/config.json\n",
      "Model weights saved in ./t5/checkpoint-4266/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4266/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4266/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4266/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3792] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4503\n",
      "Configuration saved in ./t5/checkpoint-4503/config.json\n",
      "Model weights saved in ./t5/checkpoint-4503/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4503/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4503/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4503/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4266] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4740\n",
      "Configuration saved in ./t5/checkpoint-4740/config.json\n",
      "Model weights saved in ./t5/checkpoint-4740/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4740/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4740/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4740/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4503] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-4029 (score: 0.30809399477806787).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4740, training_loss=0.01968533838568609, metrics={'train_runtime': 2346.1284, 'train_samples_per_second': 64.651, 'train_steps_per_second': 2.02, 'total_flos': 2.4354509635584e+16, 'train_loss': 0.01968533838568609, 'epoch': 20.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:11,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"oasc\") for el in zhang_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.2640801001251564,\n",
       " 'precision': 0.4678492239467849,\n",
       " 'f1_score': 0.3376}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# William Hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--Wikidepia--IndoT5-base/snapshots/da8e5576aff97b6e6e08ffa669e34bbf87ca637c/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"Wikidepia/IndoT5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--Wikidepia--IndoT5-base/snapshots/da8e5576aff97b6e6e08ffa669e34bbf87ca637c/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Wikidepia/IndoT5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7500\n",
      "  Number of trainable parameters = 247577856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='492' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 492/7500 05:39 < 1:20:57, 1.44 it/s, Epoch 1.31/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.200200</td>\n",
       "      <td>0.094703</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-375\n",
      "Configuration saved in ./t5/checkpoint-375/config.json\n",
      "Saving model checkpoint to ./t5/checkpoint-375\n",
      "Configuration saved in ./t5/checkpoint-375/config.json\n",
      "Model weights saved in ./t5/checkpoint-375/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-375/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-375/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-750\n",
      "Configuration saved in ./t5/checkpoint-750/config.json\n",
      "Model weights saved in ./t5/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-750/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-750/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1125\n",
      "Configuration saved in ./t5/checkpoint-1125/config.json\n",
      "Model weights saved in ./t5/checkpoint-1125/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1125/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1125/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1125/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1500\n",
      "Configuration saved in ./t5/checkpoint-1500/config.json\n",
      "Model weights saved in ./t5/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1500/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1125] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-1875\n",
      "Configuration saved in ./t5/checkpoint-1875/config.json\n",
      "Model weights saved in ./t5/checkpoint-1875/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-1875/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-1875/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-1875/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2250\n",
      "Configuration saved in ./t5/checkpoint-2250/config.json\n",
      "Model weights saved in ./t5/checkpoint-2250/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2250/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2250/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2250/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-1875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-2625\n",
      "Configuration saved in ./t5/checkpoint-2625/config.json\n",
      "Model weights saved in ./t5/checkpoint-2625/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-2625/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-2625/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-2625/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3000\n",
      "Configuration saved in ./t5/checkpoint-3000/config.json\n",
      "Model weights saved in ./t5/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3000/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3000/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-2625] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3375\n",
      "Configuration saved in ./t5/checkpoint-3375/config.json\n",
      "Model weights saved in ./t5/checkpoint-3375/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3375/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3375/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3375/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-3750\n",
      "Configuration saved in ./t5/checkpoint-3750/config.json\n",
      "Model weights saved in ./t5/checkpoint-3750/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-3750/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-3750/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3375] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4125\n",
      "Configuration saved in ./t5/checkpoint-4125/config.json\n",
      "Model weights saved in ./t5/checkpoint-4125/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4125/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4125/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4125/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-3750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4500\n",
      "Configuration saved in ./t5/checkpoint-4500/config.json\n",
      "Model weights saved in ./t5/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4500/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4500/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4125] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-4875\n",
      "Configuration saved in ./t5/checkpoint-4875/config.json\n",
      "Model weights saved in ./t5/checkpoint-4875/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-4875/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-4875/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-4875/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-5250\n",
      "Configuration saved in ./t5/checkpoint-5250/config.json\n",
      "Model weights saved in ./t5/checkpoint-5250/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-5250/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-5250/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-5250/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-4875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-5625\n",
      "Configuration saved in ./t5/checkpoint-5625/config.json\n",
      "Model weights saved in ./t5/checkpoint-5625/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-5625/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-5625/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-5625/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-5250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-6000\n",
      "Configuration saved in ./t5/checkpoint-6000/config.json\n",
      "Model weights saved in ./t5/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-6000/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-6000/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-5625] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-6375\n",
      "Configuration saved in ./t5/checkpoint-6375/config.json\n",
      "Model weights saved in ./t5/checkpoint-6375/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-6375/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-6375/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-6375/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-6750\n",
      "Configuration saved in ./t5/checkpoint-6750/config.json\n",
      "Model weights saved in ./t5/checkpoint-6750/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-6750/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-6750/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-6750/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-6375] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-7125\n",
      "Configuration saved in ./t5/checkpoint-7125/config.json\n",
      "Model weights saved in ./t5/checkpoint-7125/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-7125/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-7125/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-7125/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-6750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5/checkpoint-7500\n",
      "Configuration saved in ./t5/checkpoint-7500/config.json\n",
      "Model weights saved in ./t5/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./t5/checkpoint-7500/special_tokens_map.json\n",
      "Copy vocab file to ./t5/checkpoint-7500/spiece.model\n",
      "Deleting older checkpoint [t5/checkpoint-7125] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5/checkpoint-375 (score: 0).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.164492866435647, metrics={'train_runtime': 5288.9667, 'train_samples_per_second': 45.377, 'train_steps_per_second': 1.418, 'total_flos': 4.778348078451917e+16, 'train_loss': 0.164492866435647, 'epoch': 20.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_id,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = william_tok[\"hotel\"][\"train\"],\n",
    "        eval_dataset = william_tok[\"hotel\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,william_2[\"hotel\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:29,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_id, william_tok[\"hotel\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [catch_answer(el,\"oas\") for el in william_2[\"hotel\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.3274270948689553,\n",
       " 'precision': 0.7598290598290598,\n",
       " 'f1_score': 0.4576449083904052}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
