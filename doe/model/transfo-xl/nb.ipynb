{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "sys.path.append(\"../../../src/\")\n",
    "import data_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_dir = dict(\n",
    "    lap14 = \"../../../data/absa/en/peng/14lap\",\n",
    "    res14 = \"../../../data/absa/en/peng/14res\",\n",
    "    res15 = \"../../../data/absa/en/peng/15res\",\n",
    "    res16 = \"../../../data/absa/en/peng/16res\"\n",
    ")\n",
    "\n",
    "wan_dir = dict(\n",
    "    res15 = \"../../../data/absa/en/wan/interim/rest15\",\n",
    "    res16 = \"../../../data/absa/en/wan/interim/rest16\"\n",
    ")\n",
    "    \n",
    "zhang_dir = dict(\n",
    "    res15 = \"../../../data/absa/en/zhang/interim/interim_2/rest15\",\n",
    "    res16 = \"../../../data/absa/en/zhang/interim/interim_2/rest16\"\n",
    ")\n",
    "\n",
    "william_dir = dict(\n",
    "    hotel = \"../../../data/absa/id/william\"\n",
    ")\n",
    "\n",
    "peng = dict(\n",
    "    lap14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res15\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res15\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res15\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res16\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res16\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res16\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")\n",
    "\n",
    "wan = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    )\n",
    ")\n",
    "\n",
    "zhang = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    )\n",
    ")\n",
    "\n",
    "william = dict(\n",
    "    hotel = dict(\n",
    "        train = data_utils.read_data(path=william_dir[\"hotel\"] + \"/train.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=william_dir[\"hotel\"] + \"/dev.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=william_dir[\"hotel\"] + \"/test.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.SENTIMENT_ELEMENT = {'a' : \"aspect\", 'o' : \"opinion\", 's' : \"sentiment\", 'c' : \"category\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AOS (ASTE)\n",
    "    * AO\n",
    "    * AS\n",
    "    * A\n",
    "    * O\n",
    "\n",
    "2. ACS (TASD)\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * C\n",
    "\n",
    "3. ACOS\n",
    "    * AO\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * O\n",
    "    * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oas', 'oa', 'as', 'a', 'o', 'asc', 'sc', 'c', 'oasc']\n"
     ]
    }
   ],
   "source": [
    "task_tree = {\n",
    "    \"oas\" : [\"oas\",\"oa\",\"as\",'a','o'],\n",
    "    \"asc\" : [\"asc\",\"as\",\"sc\",'a','c'],\n",
    "    \"oasc\" : [\"oasc\",\"oa\",\"as\",\"sc\",'a','o','c']\n",
    "}\n",
    "\n",
    "all_task = []\n",
    "for k,v1 in task_tree.items():\n",
    "    if k not in all_task:\n",
    "        all_task.append(k)\n",
    "    for v2 in v1:\n",
    "        if v2 not in all_task:\n",
    "            all_task.append(v2)\n",
    "\n",
    "print(all_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.remove_duplicate_targets(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"ao\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle mix may not be a must, but we'll see it later. Will be problematic if like as (UABSA / E2E ABSA) used for training AOS (ASTE) --> may be for further experiment because we will insert imputing later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good', 'sentiment': 'mixed'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.handle_mix_sentiment(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"aos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Peng (ASTE/AOS)\n",
    "peng_intermediate = dict()\n",
    "\n",
    "for domain, v1 in peng.items():\n",
    "    peng_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        peng_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = peng[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            peng_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wan (TASD/ACS)\n",
    "wan_intermediate = dict()\n",
    "\n",
    "for domain, v1 in wan.items():\n",
    "    wan_intermediate[domain] = dict()\n",
    "    for task in [\"asc\"] + task_tree[\"asc\"]:\n",
    "        wan_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = wan[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            wan_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang (ACOS)\n",
    "zhang_intermediate = dict()\n",
    "\n",
    "for domain, v1 in zhang.items():\n",
    "    zhang_intermediate[domain] = dict()\n",
    "    for task in [\"oasc\"] + task_tree[\"oasc\"]:\n",
    "        zhang_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = zhang[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            zhang_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William (AOS ID)\n",
    "william_intermediate = dict()\n",
    "\n",
    "for domain, v1 in william.items():\n",
    "    william_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        william_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = william[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            william_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"<extra_id_X>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = {\n",
    "    ',' : \"<comma>\",\n",
    "    '(' : \"<open_bracket>\",\n",
    "    ')' : \"<close_bracket>\",\n",
    "    ';' : \"<semicolon>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_answer(targets,se_order):\n",
    "#     result = []\n",
    "#     counter = 0\n",
    "#     for t in targets:\n",
    "#         constructed_t = \"\"\n",
    "#         for se in se_order:\n",
    "#             if counter > 99:\n",
    "#                 raise Exception(\"Extra id more than 99!\")\n",
    "#             constructed_t += ' ' + mask.replace('X',str(counter)) + ' ' + t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "#             counter += 1\n",
    "#         constructed_t = constructed_t.strip()\n",
    "#         result.append(constructed_t)\n",
    "#     result = \" ; \".join(result)\n",
    "#     return result\n",
    "def construct_answer(targets,se_order):\n",
    "    result = []\n",
    "    for t in targets:\n",
    "        constructed_t = []\n",
    "        for se in se_order:\n",
    "            element = t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "            for k, v in added_tokens.items():\n",
    "                element = element.replace(k,v)\n",
    "            constructed_t.append(element)\n",
    "        constructed_t = \" , \".join(constructed_t)\n",
    "        constructed_t = f\"( {constructed_t} )\"\n",
    "        result.append(constructed_t)\n",
    "    result = \" ; \".join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( no , GUI , negative ) ; ( dark , screen , negative ) ; ( steady , power light , neutral ) ; ( steady , hard drive light , negative )'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( <open_bracket> tes3 <semicolon> tes4 <close_bracket> , tes1 <comma> tes2 , positive )'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer([{\"aspect\" : \"tes1 , tes2\", \"opinion\" : \"( tes3 ; tes4 )\", \"sentiment\" : \"positive\"}],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_prompt(text,se_order):\n",
    "#     prompt = []\n",
    "#     for counter, se in enumerate(se_order):\n",
    "#         prompt.append(data_utils.SENTIMENT_ELEMENT[se] + \" : \" + mask.replace('X',str(counter)))\n",
    "#     prompt = \" ,\".join(prompt)\n",
    "#     result = text + \"| \" + prompt\n",
    "#     return result\n",
    "def construct_prompt(text,se_order):\n",
    "    prompt = []\n",
    "    for se in se_order:\n",
    "        prompt.append(data_utils.SENTIMENT_ELEMENT[se])\n",
    "    prompt = \" , \".join(prompt)\n",
    "    prompt = f\"( {prompt} )\"\n",
    "    masked_text = text\n",
    "    for k, v in added_tokens.items():\n",
    "        masked_text = masked_text.replace(k,v)\n",
    "    result = masked_text + \" | \" + prompt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One night I turned the freaking thing off after using it <comma> the next day I turn it on <comma> no GUI <comma> screen all dark <comma> power light steady <comma> hard drive light steady and not flashing as it usually does . | ( opinion , aspect , sentiment )'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_prompt(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"text\"],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def catch_answer(output,se_order):\n",
    "#     output = output.replace(\"<pad>\",'')\n",
    "#     output = output.replace(\"</s>\",'')\n",
    "#     pattern = r\"\"\n",
    "#     for se in se_order:\n",
    "#         if se != 's':\n",
    "#             pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
    "#         else:\n",
    "#             pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
    "#     found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "#     for i in range(len(found)):\n",
    "#         for k, v in found[i].items():\n",
    "#             found[i][k] = found[i][k].strip()\n",
    "#     return found\n",
    "def catch_answer(output,se_order):\n",
    "    # output = output.replace(\"<pad>\",'')\n",
    "    # output = output.replace(\"</s>\",'')\n",
    "    pattern = []\n",
    "    for se in se_order:\n",
    "        if se != 's':\n",
    "            pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\")\n",
    "        else:\n",
    "            pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\")\n",
    "    pattern = ','.join(pattern)\n",
    "    pattern = f\"\\({pattern}\\)\"\n",
    "    found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "    for i in range(len(found)):\n",
    "        for k, v in found[i].items():\n",
    "            found[i][k] = found[i][k].strip()\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'opinion': 'no', 'aspect': 'GUI', 'sentiment': 'negative'},\n",
       " {'opinion': 'dark', 'aspect': 'screen', 'sentiment': 'negative'},\n",
       " {'opinion': 'steady', 'aspect': 'power light', 'sentiment': 'neutral'},\n",
       " {'opinion': 'steady', 'aspect': 'hard drive light', 'sentiment': 'negative'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")\n",
    "se_order = \"oas\"\n",
    "catch_answer(output,se_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( no , GUI , negative ) ; ( dark , screen , negative ) ; ( steady , power light , neutral ) ; ( steady , hard drive light , negative )'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "peng_2 = dict()\n",
    "for domain, v1 in peng_intermediate.items():\n",
    "    peng_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in peng_intermediate[domain][basic_task][\"train\"]:\n",
    "            peng_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        peng_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        peng_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    peng_2[domain][\"train\"] = Dataset.from_list(peng_2[domain][\"train\"])\n",
    "    peng_2[domain][\"val\"] = Dataset.from_list(peng_2[domain][\"val\"])\n",
    "    peng_2[domain][\"test\"] = Dataset.from_list(peng_2[domain][\"test\"])\n",
    "\n",
    "wan_2 = dict()\n",
    "for domain, v1 in wan_intermediate.items():\n",
    "    wan_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"asc\"]:\n",
    "        for el in wan_intermediate[domain][basic_task][\"train\"]:\n",
    "            wan_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"val\"]:\n",
    "        wan_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"test\"]:\n",
    "        wan_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    wan_2[domain][\"train\"] = Dataset.from_list(wan_2[domain][\"train\"])\n",
    "    wan_2[domain][\"val\"] = Dataset.from_list(wan_2[domain][\"val\"])\n",
    "    wan_2[domain][\"test\"] = Dataset.from_list(wan_2[domain][\"test\"])\n",
    "\n",
    "zhang_2 = dict()\n",
    "for domain, v1 in zhang_intermediate.items():\n",
    "    zhang_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oasc\"]:\n",
    "        for el in zhang_intermediate[domain][basic_task][\"train\"]:\n",
    "            zhang_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"val\"]:\n",
    "        zhang_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"test\"]:\n",
    "        zhang_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    zhang_2[domain][\"train\"] = Dataset.from_list(zhang_2[domain][\"train\"])\n",
    "    zhang_2[domain][\"val\"] = Dataset.from_list(zhang_2[domain][\"val\"])\n",
    "    zhang_2[domain][\"test\"] = Dataset.from_list(zhang_2[domain][\"test\"])\n",
    "\n",
    "william_2 = dict()\n",
    "for domain, v1 in william_intermediate.items():\n",
    "    william_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in william_intermediate[domain][basic_task][\"train\"]:\n",
    "            william_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in william_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        william_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in william_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        william_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    william_2[domain][\"train\"] = Dataset.from_list(william_2[domain][\"train\"])\n",
    "    william_2[domain][\"val\"] = Dataset.from_list(william_2[domain][\"val\"])\n",
    "    william_2[domain][\"test\"] = Dataset.from_list(william_2[domain][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tempat yag bagus dan nyaman untuk istirahat tetapi tolong tvnya perlu di perbaiki channelnya karena banyak semutnya digambar dan water heaternya tidak bisa jadi mandi air dingin terus . | ( opinion , aspect , sentiment )',\n",
       " 'output': '( bagus , tempat , positive ) ; ( nyaman , tempat , positive ) ; ( perlu di perbaiki , tvnya , positive ) ; ( tidak bisa , water heaternya , negative )',\n",
       " 'task': 'oas'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "william_2[\"hotel\"][\"train\"][69]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Tokenized Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 560/560 [00:00<00:00, 326kB/s]\n",
      "Downloading: 100%|██████████| 1.01k/1.01k [00:00<00:00, 1.10MB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 3.50MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 2.02MB/s]\n",
      "Downloading: 100%|██████████| 357/357 [00:00<00:00, 262kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained(\"gpt2-large\", padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_en.add_tokens(list(added_tokens.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_args = {\n",
    "    \"max_length\" : 512,\n",
    "    \"padding\" : True,\n",
    "    \"truncation\" : True,\n",
    "    \"return_tensors\" : \"pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "resize_en = False\n",
    "if tokenizer_en.pad_token == None:\n",
    "    pad_token = \"<|pad|>\"\n",
    "    tokenizer_en.add_tokens([pad_token])\n",
    "    tokenizer_en.add_special_tokens({\"pad_token\": pad_token})\n",
    "    resize_en = True\n",
    "\n",
    "if tokenizer_en.sep_token == None:\n",
    "    sep_token = \"<|sep|>\"\n",
    "    tokenizer_en.add_tokens([sep_token])\n",
    "    tokenizer_en.add_special_tokens({\"sep_token\": sep_token})\n",
    "    resize_en = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_en(dataset):\n",
    "    causal_lm_input =[row_input + ' ' + tokenizer_en.sep_token + ' ' + row_output + ' ' + tokenizer_en.eos_token\n",
    "                      for row_input, row_output in zip(dataset[\"input\"],dataset[\"output\"])]\n",
    "    result = tokenizer_en(causal_lm_input, **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "peng_tok = dict()\n",
    "for domain, v1 in peng_2.items():\n",
    "    peng_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            peng_tok[domain][split] = peng_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            test_input = [row_input + ' ' + tokenizer_en.sep_token for row_input in peng_2[domain][split][\"input\"]]\n",
    "            peng_tok[domain][split] = tokenizer_en(test_input, **encoding_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "wan_tok = dict()\n",
    "for domain, v1 in wan_2.items():\n",
    "    wan_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            wan_tok[domain][split] = wan_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            test_input = [row_input + ' ' + tokenizer_en.sep_token for row_input in wan_2[domain][split][\"input\"]]\n",
    "            wan_tok[domain][split] = tokenizer_en(test_input, **encoding_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "zhang_tok = dict()\n",
    "for domain, v1 in zhang_2.items():\n",
    "    zhang_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            zhang_tok[domain][split] = zhang_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            test_input = [row_input + ' ' + tokenizer_en.sep_token for row_input in zhang_2[domain][split][\"input\"]]\n",
    "            zhang_tok[domain][split] = tokenizer_en(test_input, **encoding_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\", padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_id.add_tokens(list(added_tokens.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_id = False\n",
    "if tokenizer_id.pad_token == None:\n",
    "    pad_token = \"<|pad|>\"\n",
    "    tokenizer_id.add_tokens([pad_token])\n",
    "    tokenizer_id.add_special_tokens({\"pad_token\": pad_token})\n",
    "    resize_id = True\n",
    "\n",
    "if tokenizer_id.sep_token == None:\n",
    "    sep_token = \"<|sep|>\"\n",
    "    tokenizer_id.add_tokens([sep_token])\n",
    "    tokenizer_id.add_special_tokens({\"sep_token\": sep_token})\n",
    "    resize_id = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_id(dataset):\n",
    "    causal_lm_input =[row_input + ' ' + tokenizer_en.sep_token + ' ' + row_output + ' ' + tokenizer_en.eos_token\n",
    "                      for row_input, row_output in zip(dataset[\"input\"],dataset[\"output\"])]\n",
    "    result = tokenizer_id(causal_lm_input, **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "william_tok = dict()\n",
    "for domain, v1 in william_2.items():\n",
    "    william_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            william_tok[domain][split] = william_2[domain][split].map(encode_id,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            test_input = [row_input + ' ' + tokenizer_id.sep_token for row_input in william_2[domain][split][\"input\"]]\n",
    "            william_tok[domain][split] = tokenizer_id(test_input, **encoding_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator_en = DataCollatorForLanguageModeling(tokenizer=tokenizer_en,mlm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_id = DataCollatorForLanguageModeling(tokenizer=tokenizer_id,mlm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from evaluation import recall, precision, f1_score, summary_score\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def seperate_target_prediction_per_task(predictions:List[List[Dict]],targets:List[List[Dict]],tasks:List) -> Tuple[Dict[str,List],Dict[str,List]]:\n",
    "    per_task_targets = {}\n",
    "    per_task_predictions = {}\n",
    "    for target, prediction, task in zip(targets,predictions,tasks):\n",
    "        if task not in per_task_targets.keys():\n",
    "            per_task_targets[task] = []\n",
    "        if task not in per_task_predictions.keys():\n",
    "            per_task_predictions[task] = []\n",
    "        per_task_targets[task].append(target)\n",
    "        per_task_predictions[task].append(prediction)\n",
    "    return per_task_targets, per_task_predictions\n",
    "\n",
    "def preprocess_eval_preds(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer):\n",
    "    input_ids = eval_preds.inputs\n",
    "    target_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(input_ids, tuple):\n",
    "        input_ids = input_ids[0]\n",
    "    if isinstance(target_ids, tuple):\n",
    "        target_ids = target_ids[0]\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    input_ids = np.argmax(input_ids,axis=-1) if len(input_ids.shape) == 3 else input_ids # in case not predict with generate\n",
    "    target_ids = np.argmax(target_ids,axis=-1) if len(target_ids.shape) == 3 else target_ids # in case not predict with generate\n",
    "    prediction_ids = np.argmax(pred_ids,axis=-1) if len(pred_ids.shape) == 3 else pred_ids # in case not predict with generate\n",
    "\n",
    "    input_ids = [[token for token in row if token != -100] for row in input_ids]\n",
    "    target_ids = [[token for token in row if token != -100] for row in target_ids]\n",
    "    prediction_ids = [[token for token in row if token != -100] for row in prediction_ids]\n",
    "\n",
    "    inputs = tokenizer.batch_decode(input_ids,**decoding_args)\n",
    "    targets = tokenizer.batch_decode(target_ids,**decoding_args)\n",
    "    predictions = tokenizer.batch_decode(prediction_ids,**decoding_args)\n",
    "\n",
    "    return inputs, targets, predictions\n",
    "\n",
    "def compute_metrics(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer,tasks:List) -> Dict[str,float]: # MAY NOT BE SUFFICIATE FOR CAUSAL LM\n",
    "        \"\"\"\n",
    "        ### DESC\n",
    "            Method to compute the metrics.\n",
    "        ### PARAMS\n",
    "        * eval_preds: EvalPrediction instance from training.\n",
    "        * decoding_args: Decoding arguments.\n",
    "        ### RETURN\n",
    "        * metrics: Dictionary of metrics.\n",
    "        \"\"\"\n",
    "        inputs, targets, predictions = preprocess_eval_preds(eval_preds,decoding_args,tokenizer)\n",
    "\n",
    "        targets = [catch_answer(text,task) for text,task in zip(targets,tasks) if task != \"non_absa\"]\n",
    "        predictions = [catch_answer(text,task) for text,task in zip(predictions,tasks) if task != \"non_absa\"]\n",
    "\n",
    "\n",
    "        per_task_targets, per_task_predictions = seperate_target_prediction_per_task(predictions, targets, tasks)\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        metrics[\"overall_recall\"] = recall(predictions,targets)\n",
    "        metrics[\"overall_precision\"] = precision(predictions,targets)\n",
    "        metrics[\"overall_f1_score\"] = f1_score(predictions,targets)\n",
    "\n",
    "        for task in per_task_targets.keys():\n",
    "            if task == \"non_absa\":\n",
    "                continue\n",
    "            metrics[f\"{task}_recall\"] = recall(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_precision\"] = precision(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_f1_score\"] = f1_score(per_task_predictions[task],per_task_targets[task])\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"per_device_train_batch_size\": 16//n_gpu,\n",
    "    \"per_device_eval_batch_size\": 16//n_gpu,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_strategy\" : \"epoch\",\n",
    "    # \"metric_for_best_model\": \"overall_f1_score\",\n",
    "    # \"load_best_model_at_end\": True,\n",
    "    \"adam_epsilon\": 1e-08,\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"logging_dir\" : \"./output/log\",\n",
    "    \"include_inputs_for_metrics\" : True\n",
    "}\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(**train_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# trainer = {\n",
    "#     \"peng\" : {},\n",
    "#     \"wan\" : {},\n",
    "#     \"zhang\" : {},\n",
    "#     \"william\" : {}\n",
    "# }\n",
    "\n",
    "decoding_args = {\n",
    "    \"skip_special_tokens\" : False\n",
    "}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, targets):\n",
    "    pred_logits = logits[0] if isinstance(logits,tuple) else logits\n",
    "    pred_ids = torch.argmax(pred_logits, dim=-1)\n",
    "    return pred_ids, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_predictions(model,tokenizer,tokenized:torch.Tensor,device:torch.device=torch.device(\"cpu\"),batch_size:int=16,max_len:int=512,decoding_args:Dict={}) -> List[str]:\n",
    "    # Data loader\n",
    "    input_ids_data_loader = torch.utils.data.DataLoader(tokenized[\"input_ids\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    attention_mask_data_loader = torch.utils.data.DataLoader(tokenized[\"attention_mask\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    # Predict\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    tensor_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(zip(input_ids_data_loader,attention_mask_data_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            tensor_predictions.extend(model.generate(input_ids=input_ids,attention_mask=attention_mask,max_length=max_len,pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id).cpu())\n",
    "            input_ids = input_ids.cpu()\n",
    "            attention_mask = attention_mask.cpu()\n",
    "    tensor_predictions = [[token for token in row if token != -100] for row in tensor_predictions]\n",
    "    predictions = tokenizer.batch_decode(tensor_predictions,**decoding_args)\n",
    "    predictions = [el.split(tokenizer.sep_token)[-1] for el in predictions]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_result(str_preds_,preds,targets,filename):\n",
    "    result = []\n",
    "    str_preds = [el.replace(\"<pad>\",'') for el in str_preds_]\n",
    "    assert len(str_preds) == len(preds) == len(targets)\n",
    "    for i in range(len(str_preds)):\n",
    "        result.append({\n",
    "            \"str_pred\" : str_preds[i],\n",
    "            \"pred\" : preds[i],\n",
    "            \"target\" : targets[i]\n",
    "        })\n",
    "    \n",
    "    with open(filename,'w') as fp:\n",
    "        json.dump(result,fp)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Laptop 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4530\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1420\n",
      "  Number of trainable parameters = 125200128\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='143' max='1420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 143/1420 00:54 < 08:11, 2.60 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-142\n",
      "Configuration saved in ./output/checkpoint-142/config.json\n",
      "Model weights saved in ./output/checkpoint-142/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-142/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-142/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-284\n",
      "Configuration saved in ./output/checkpoint-284/config.json\n",
      "Model weights saved in ./output/checkpoint-284/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-284/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-284/special_tokens_map.json\n",
      "Saving model checkpoint to ./output/checkpoint-426\n",
      "Configuration saved in ./output/checkpoint-426/config.json\n",
      "Model weights saved in ./output/checkpoint-426/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-426/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-426/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-142] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-568\n",
      "Configuration saved in ./output/checkpoint-568/config.json\n",
      "Model weights saved in ./output/checkpoint-568/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-568/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-568/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-710\n",
      "Configuration saved in ./output/checkpoint-710/config.json\n",
      "Model weights saved in ./output/checkpoint-710/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-710/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-710/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-426] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-852\n",
      "Configuration saved in ./output/checkpoint-852/config.json\n",
      "Model weights saved in ./output/checkpoint-852/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-852/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-852/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-568] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-994\n",
      "Configuration saved in ./output/checkpoint-994/config.json\n",
      "Model weights saved in ./output/checkpoint-994/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-994/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-994/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-710] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1136\n",
      "Configuration saved in ./output/checkpoint-1136/config.json\n",
      "Model weights saved in ./output/checkpoint-1136/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1136/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1136/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-852] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1278\n",
      "Configuration saved in ./output/checkpoint-1278/config.json\n",
      "Model weights saved in ./output/checkpoint-1278/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1278/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1278/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-994] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1420\n",
      "Configuration saved in ./output/checkpoint-1420/config.json\n",
      "Model weights saved in ./output/checkpoint-1420/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1420/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1420/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1136] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1420, training_loss=6.237962491747359, metrics={'train_runtime': 578.5929, 'train_samples_per_second': 78.293, 'train_steps_per_second': 2.454, 'total_flos': 3346180256102400.0, 'train_loss': 6.237962491747359, 'epoch': 10.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"lap14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"lap14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"lap14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:02,  3.69it/s]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"lap14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"lap14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.0, 'precision': 0, 'f1_score': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_lap14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/b983397156c0991016feccfbcbe1fe2746d47b29/config.json\n",
      "Model config GPTNeoConfig {\n",
      "  \"_name_or_path\": \"EleutherAI/gpt-neo-125m\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/b983397156c0991016feccfbcbe1fe2746d47b29/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6330\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1980\n",
      "  Number of trainable parameters = 125200128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='1980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 198/1980 01:27 < 13:17, 2.24 it/s, Epoch 0.99/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1980, training_loss=20.77789368292298, metrics={'train_runtime': 926.9541, 'train_samples_per_second': 68.288, 'train_steps_per_second': 2.136, 'total_flos': 5437342036684800.0, 'train_loss': 20.77789368292298, 'epoch': 10.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:37,  6.12s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.0, 'precision': 0, 'f1_score': 0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/b983397156c0991016feccfbcbe1fe2746d47b29/config.json\n",
      "Model config GPTNeoConfig {\n",
      "  \"_name_or_path\": \"EleutherAI/gpt-neo-125m\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/b983397156c0991016feccfbcbe1fe2746d47b29/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3025\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 950\n",
      "  Number of trainable parameters = 125200128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 95/950 00:40 < 06:11, 2.30 it/s, Epoch 0.99/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=950, training_loss=6.7153996196546055, metrics={'train_runtime': 441.5222, 'train_samples_per_second': 68.513, 'train_steps_per_second': 2.152, 'total_flos': 2592211071320064.0, 'train_loss': 6.7153996196546055, 'epoch': 10.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:03,  5.77s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.0, 'precision': 0, 'f1_score': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/b983397156c0991016feccfbcbe1fe2746d47b29/config.json\n",
      "Model config GPTNeoConfig {\n",
      "  \"_name_or_path\": \"EleutherAI/gpt-neo-125m\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/b983397156c0991016feccfbcbe1fe2746d47b29/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4285\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1340\n",
      "  Number of trainable parameters = 125200128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='134' max='1340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 134/1340 00:54 < 08:17, 2.42 it/s, Epoch 0.99/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:05,  5.92s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.005836575875486381,\n",
       " 'precision': 0.0182648401826484,\n",
       " 'f1_score': 0.008846295613711757}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5600\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3500\n",
      "  Number of trainable parameters = 124441344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 176/3500 01:28 < 28:05, 1.97 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-175\n",
      "Configuration saved in ./output/checkpoint-175/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-175\n",
      "Configuration saved in ./output/checkpoint-175/config.json\n",
      "Model weights saved in ./output/checkpoint-175/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-175/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-175/special_tokens_map.json\n",
      "Saving model checkpoint to ./output/checkpoint-350\n",
      "Configuration saved in ./output/checkpoint-350/config.json\n",
      "Model weights saved in ./output/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-350/special_tokens_map.json\n",
      "Saving model checkpoint to ./output/checkpoint-525\n",
      "Configuration saved in ./output/checkpoint-525/config.json\n",
      "Model weights saved in ./output/checkpoint-525/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-525/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-525/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-175] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-700\n",
      "Configuration saved in ./output/checkpoint-700/config.json\n",
      "Model weights saved in ./output/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-875\n",
      "Configuration saved in ./output/checkpoint-875/config.json\n",
      "Model weights saved in ./output/checkpoint-875/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-875/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-875/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-525] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1050\n",
      "Configuration saved in ./output/checkpoint-1050/config.json\n",
      "Model weights saved in ./output/checkpoint-1050/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1225\n",
      "Configuration saved in ./output/checkpoint-1225/config.json\n",
      "Model weights saved in ./output/checkpoint-1225/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1225/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1225/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-875] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1400\n",
      "Configuration saved in ./output/checkpoint-1400/config.json\n",
      "Model weights saved in ./output/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1575\n",
      "Configuration saved in ./output/checkpoint-1575/config.json\n",
      "Model weights saved in ./output/checkpoint-1575/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1575/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1575/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1225] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1750\n",
      "Configuration saved in ./output/checkpoint-1750/config.json\n",
      "Model weights saved in ./output/checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1925\n",
      "Configuration saved in ./output/checkpoint-1925/config.json\n",
      "Model weights saved in ./output/checkpoint-1925/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1925/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1925/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1575] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2100\n",
      "Configuration saved in ./output/checkpoint-2100/config.json\n",
      "Model weights saved in ./output/checkpoint-2100/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2275\n",
      "Configuration saved in ./output/checkpoint-2275/config.json\n",
      "Model weights saved in ./output/checkpoint-2275/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2275/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2275/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1925] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2450\n",
      "Configuration saved in ./output/checkpoint-2450/config.json\n",
      "Model weights saved in ./output/checkpoint-2450/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2625\n",
      "Configuration saved in ./output/checkpoint-2625/config.json\n",
      "Model weights saved in ./output/checkpoint-2625/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2625/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2625/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2275] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2800\n",
      "Configuration saved in ./output/checkpoint-2800/config.json\n",
      "Model weights saved in ./output/checkpoint-2800/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2975\n",
      "Configuration saved in ./output/checkpoint-2975/config.json\n",
      "Model weights saved in ./output/checkpoint-2975/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2975/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2975/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2625] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3150\n",
      "Configuration saved in ./output/checkpoint-3150/config.json\n",
      "Model weights saved in ./output/checkpoint-3150/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3325\n",
      "Configuration saved in ./output/checkpoint-3325/config.json\n",
      "Model weights saved in ./output/checkpoint-3325/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3325/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3325/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2975] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3500\n",
      "Configuration saved in ./output/checkpoint-3500/config.json\n",
      "Model weights saved in ./output/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3150] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=2.9796246425083703, metrics={'train_runtime': 1834.0091, 'train_samples_per_second': 61.068, 'train_steps_per_second': 1.908, 'total_flos': 1.0602748901376e+16, 'train_loss': 2.9796246425083703, 'epoch': 20.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [02:16,  7.20s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.11952662721893491,\n",
       " 'precision': 0.14647577092511013,\n",
       " 'f1_score': 0.13163606787101959}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8540\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 5340\n",
      "  Number of trainable parameters = 124441344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 267/5340 02:13 < 42:27, 1.99 it/s, Epoch 1.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5340, training_loss=4.955751294768258, metrics={'train_runtime': 2770.7722, 'train_samples_per_second': 61.643, 'train_steps_per_second': 1.927, 'total_flos': 1.6023935955456e+16, 'train_loss': 4.955751294768258, 'epoch': 20.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [01:47,  5.65s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.0, 'precision': 0, 'f1_score': 0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5838\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3640\n",
      "  Number of trainable parameters = 124441344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='3640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 183/3640 01:33 < 29:36, 1.95 it/s, Epoch 1.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/14 00:01 < 00:00, 9.79 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3640, training_loss=3.3605101784506997, metrics={'train_runtime': 1986.8205, 'train_samples_per_second': 58.767, 'train_steps_per_second': 1.832, 'total_flos': 1.1273477382144e+16, 'train_loss': 3.3605101784506997, 'epoch': 20.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [02:11,  7.73s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.0, 'precision': 0.0, 'f1_score': 0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8848\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 5520\n",
      "  Number of trainable parameters = 124441344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='277' max='5520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 277/5520 02:37 < 50:07, 1.74 it/s, Epoch 1.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-276\n",
      "Configuration saved in ./output/checkpoint-276/config.json\n",
      "Model weights saved in ./output/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-276/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-552\n",
      "Configuration saved in ./output/checkpoint-552/config.json\n",
      "Model weights saved in ./output/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-552/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-828\n",
      "Configuration saved in ./output/checkpoint-828/config.json\n",
      "Model weights saved in ./output/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-276] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1104\n",
      "Configuration saved in ./output/checkpoint-1104/config.json\n",
      "Model weights saved in ./output/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-552] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1380\n",
      "Configuration saved in ./output/checkpoint-1380/config.json\n",
      "Model weights saved in ./output/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-828] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1656\n",
      "Configuration saved in ./output/checkpoint-1656/config.json\n",
      "Model weights saved in ./output/checkpoint-1656/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1656/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1656/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1932\n",
      "Configuration saved in ./output/checkpoint-1932/config.json\n",
      "Model weights saved in ./output/checkpoint-1932/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1932/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1932/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1380] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2208\n",
      "Configuration saved in ./output/checkpoint-2208/config.json\n",
      "Model weights saved in ./output/checkpoint-2208/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2208/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2208/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1656] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2484\n",
      "Configuration saved in ./output/checkpoint-2484/config.json\n",
      "Model weights saved in ./output/checkpoint-2484/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2484/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2484/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1932] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2760\n",
      "Configuration saved in ./output/checkpoint-2760/config.json\n",
      "Model weights saved in ./output/checkpoint-2760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2208] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3036\n",
      "Configuration saved in ./output/checkpoint-3036/config.json\n",
      "Model weights saved in ./output/checkpoint-3036/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3036/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3036/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2484] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3312\n",
      "Configuration saved in ./output/checkpoint-3312/config.json\n",
      "Model weights saved in ./output/checkpoint-3312/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3312/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3312/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2760] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3588\n",
      "Configuration saved in ./output/checkpoint-3588/config.json\n",
      "Model weights saved in ./output/checkpoint-3588/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3588/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3588/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3036] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3864\n",
      "Configuration saved in ./output/checkpoint-3864/config.json\n",
      "Model weights saved in ./output/checkpoint-3864/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3864/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3864/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3312] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4140\n",
      "Configuration saved in ./output/checkpoint-4140/config.json\n",
      "Model weights saved in ./output/checkpoint-4140/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4140/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4140/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3588] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4416\n",
      "Configuration saved in ./output/checkpoint-4416/config.json\n",
      "Model weights saved in ./output/checkpoint-4416/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4416/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4416/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3864] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4692\n",
      "Configuration saved in ./output/checkpoint-4692/config.json\n",
      "Model weights saved in ./output/checkpoint-4692/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4692/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4692/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4140] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4968\n",
      "Configuration saved in ./output/checkpoint-4968/config.json\n",
      "Model weights saved in ./output/checkpoint-4968/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4968/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4968/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-5244\n",
      "Configuration saved in ./output/checkpoint-5244/config.json\n",
      "Model weights saved in ./output/checkpoint-5244/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5244/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5244/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4692] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5520\n",
      "Configuration saved in ./output/checkpoint-5520/config.json\n",
      "Model weights saved in ./output/checkpoint-5520/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5520/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5520/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4968] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5520, training_loss=4.20832051816194, metrics={'train_runtime': 3280.4895, 'train_samples_per_second': 53.943, 'train_steps_per_second': 1.683, 'total_flos': 1.910844960768e+16, 'train_loss': 4.20832051816194, 'epoch': 20.0})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:30,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.0, 'precision': 0.0, 'f1_score': 0}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# William Hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 4690\n",
      "  Number of trainable parameters = 125198592\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [63,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [57,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 16\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      6\u001b[0m         model \u001b[39m=\u001b[39m model,\n\u001b[1;32m      7\u001b[0m         args \u001b[39m=\u001b[39m train_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         preprocess_logits_for_metrics \u001b[39m=\u001b[39m preprocess_logits_for_metrics\n\u001b[1;32m     14\u001b[0m     )\n\u001b[0;32m---> 16\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2507\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2508\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2511\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2539\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2540\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2541\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2542\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:744\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    742\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 744\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    745\u001b[0m     input_ids,\n\u001b[1;32m    746\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    747\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    748\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    749\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    750\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    751\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    752\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    753\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    754\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    755\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    756\u001b[0m )\n\u001b[1;32m    757\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    759\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:623\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    615\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    616\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    617\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m         head_mask[i],\n\u001b[1;32m    621\u001b[0m     )\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    624\u001b[0m         hidden_states,\n\u001b[1;32m    625\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    626\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    627\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    628\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    629\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    630\u001b[0m     )\n\u001b[1;32m    632\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:328\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    326\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    327\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 328\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    329\u001b[0m     hidden_states,\n\u001b[1;32m    330\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    331\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    332\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    333\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    334\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    335\u001b[0m )\n\u001b[1;32m    336\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    337\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:280\u001b[0m, in \u001b[0;36mGPTNeoAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    272\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    273\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m ):\n\u001b[0;32m--> 280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    281\u001b[0m         hidden_states,\n\u001b[1;32m    282\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    283\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    284\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    285\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    286\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    287\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:224\u001b[0m, in \u001b[0;36mGPTNeoSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    216\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    222\u001b[0m ):\n\u001b[0;32m--> 224\u001b[0m     query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj(hidden_states)\n\u001b[1;32m    225\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    226\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n",
    "if resize_id:\n",
    "    model.resize_token_embeddings(len(tokenizer_id))\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_id,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = william_tok[\"hotel\"][\"train\"],\n",
    "        eval_dataset = william_tok[\"hotel\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_id,william_2[\"hotel\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [08:06, 15.19s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_id, william_tok[\"hotel\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in william_2[\"hotel\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.0, 'precision': 0.0, 'f1_score': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"william_hotel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
