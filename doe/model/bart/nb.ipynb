{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,6\"\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "sys.path.append(\"../../../src/\")\n",
    "import data_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_dir = dict(\n",
    "    lap14 = \"../../../data/absa/en/peng/14lap\",\n",
    "    res14 = \"../../../data/absa/en/peng/14res\",\n",
    "    res15 = \"../../../data/absa/en/peng/15res\",\n",
    "    res16 = \"../../../data/absa/en/peng/16res\"\n",
    ")\n",
    "\n",
    "wan_dir = dict(\n",
    "    res15 = \"../../../data/absa/en/wan/interim/rest15\",\n",
    "    res16 = \"../../../data/absa/en/wan/interim/rest16\"\n",
    ")\n",
    "    \n",
    "zhang_dir = dict(\n",
    "    res15 = \"../../../data/absa/en/zhang/interim/interim_2/rest15\",\n",
    "    res16 = \"../../../data/absa/en/zhang/interim/interim_2/rest16\"\n",
    ")\n",
    "\n",
    "william_dir = dict(\n",
    "    hotel = \"../../../data/absa/id/william\"\n",
    ")\n",
    "\n",
    "peng = dict(\n",
    "    lap14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res15\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res15\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res15\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res16\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res16\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res16\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")\n",
    "\n",
    "wan = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    )\n",
    ")\n",
    "\n",
    "zhang = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    )\n",
    ")\n",
    "\n",
    "william = dict(\n",
    "    hotel = dict(\n",
    "        train = data_utils.read_data(path=william_dir[\"hotel\"] + \"/train.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=william_dir[\"hotel\"] + \"/dev.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=william_dir[\"hotel\"] + \"/test.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.SENTIMENT_ELEMENT = {'a' : \"aspect\", 'o' : \"opinion\", 's' : \"sentiment\", 'c' : \"category\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AOS (ASTE)\n",
    "    * AO\n",
    "    * AS\n",
    "    * A\n",
    "    * O\n",
    "\n",
    "2. ACS (TASD)\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * C\n",
    "\n",
    "3. ACOS\n",
    "    * AO\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * O\n",
    "    * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oas', 'oa', 'as', 'a', 'o', 'asc', 'sc', 'c', 'oasc']\n"
     ]
    }
   ],
   "source": [
    "task_tree = {\n",
    "    \"oas\" : [\"oas\",\"oa\",\"as\",'a','o'],\n",
    "    \"asc\" : [\"asc\",\"as\",\"sc\",'a','c'],\n",
    "    \"oasc\" : [\"oasc\",\"oa\",\"as\",\"sc\",'a','o','c']\n",
    "}\n",
    "\n",
    "all_task = []\n",
    "for k,v1 in task_tree.items():\n",
    "    if k not in all_task:\n",
    "        all_task.append(k)\n",
    "    for v2 in v1:\n",
    "        if v2 not in all_task:\n",
    "            all_task.append(v2)\n",
    "\n",
    "print(all_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.remove_duplicate_targets(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"ao\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle mix may not be a must, but we'll see it later. Will be problematic if like as (UABSA / E2E ABSA) used for training AOS (ASTE) --> may be for further experiment because we will insert imputing later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good', 'sentiment': 'mixed'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.handle_mix_sentiment(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"aos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Peng (ASTE/AOS)\n",
    "peng_intermediate = dict()\n",
    "\n",
    "for domain, v1 in peng.items():\n",
    "    peng_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        peng_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = peng[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            peng_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wan (TASD/ACS)\n",
    "wan_intermediate = dict()\n",
    "\n",
    "for domain, v1 in wan.items():\n",
    "    wan_intermediate[domain] = dict()\n",
    "    for task in [\"asc\"] + task_tree[\"asc\"]:\n",
    "        wan_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = wan[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            wan_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang (ACOS)\n",
    "zhang_intermediate = dict()\n",
    "\n",
    "for domain, v1 in zhang.items():\n",
    "    zhang_intermediate[domain] = dict()\n",
    "    for task in [\"oasc\"] + task_tree[\"oasc\"]:\n",
    "        zhang_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = zhang[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            zhang_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William (AOS ID)\n",
    "william_intermediate = dict()\n",
    "\n",
    "for domain, v1 in william.items():\n",
    "    william_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        william_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = william[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            william_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"<extra_id_X>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = {\n",
    "    ',' : \"<comma>\",\n",
    "    '(' : \"<open_bracket>\",\n",
    "    ')' : \"<close_bracket>\",\n",
    "    ';' : \"<semicolon>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_answer(targets,se_order):\n",
    "#     result = []\n",
    "#     counter = 0\n",
    "#     for t in targets:\n",
    "#         constructed_t = \"\"\n",
    "#         for se in se_order:\n",
    "#             if counter > 99:\n",
    "#                 raise Exception(\"Extra id more than 99!\")\n",
    "#             constructed_t += ' ' + mask.replace('X',str(counter)) + ' ' + t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "#             counter += 1\n",
    "#         constructed_t = constructed_t.strip()\n",
    "#         result.append(constructed_t)\n",
    "#     result = \" ; \".join(result)\n",
    "#     return result\n",
    "def construct_answer(targets,se_order):\n",
    "    result = []\n",
    "    for t in targets:\n",
    "        constructed_t = []\n",
    "        for se in se_order:\n",
    "            element = t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "            for k, v in added_tokens.items():\n",
    "                element = element.replace(k,v)\n",
    "            constructed_t.append(element)\n",
    "        constructed_t = \" , \".join(constructed_t)\n",
    "        constructed_t = f\"( {constructed_t} )\"\n",
    "        result.append(constructed_t)\n",
    "    result = \" ; \".join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( no , GUI , negative ) ; ( dark , screen , negative ) ; ( steady , power light , neutral ) ; ( steady , hard drive light , negative )'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( <open_bracket> tes3 <semicolon> tes4 <close_bracket> , tes1 <comma> tes2 , positive )'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer([{\"aspect\" : \"tes1 , tes2\", \"opinion\" : \"( tes3 ; tes4 )\", \"sentiment\" : \"positive\"}],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_prompt(text,se_order):\n",
    "#     prompt = []\n",
    "#     for counter, se in enumerate(se_order):\n",
    "#         prompt.append(data_utils.SENTIMENT_ELEMENT[se] + \" : \" + mask.replace('X',str(counter)))\n",
    "#     prompt = \" ,\".join(prompt)\n",
    "#     result = text + \"| \" + prompt\n",
    "#     return result\n",
    "def construct_prompt(text,se_order):\n",
    "    prompt = []\n",
    "    for se in se_order:\n",
    "        prompt.append(data_utils.SENTIMENT_ELEMENT[se])\n",
    "    prompt = \" , \".join(prompt)\n",
    "    prompt = f\"( {prompt} )\"\n",
    "    masked_text = text\n",
    "    for k, v in added_tokens.items():\n",
    "        masked_text = masked_text.replace(k,v)\n",
    "    result = masked_text + \" | \" + prompt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One night I turned the freaking thing off after using it <comma> the next day I turn it on <comma> no GUI <comma> screen all dark <comma> power light steady <comma> hard drive light steady and not flashing as it usually does . | ( opinion , aspect , sentiment )'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_prompt(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"text\"],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def catch_answer(output,se_order):\n",
    "#     output = output.replace(\"<pad>\",'')\n",
    "#     output = output.replace(\"</s>\",'')\n",
    "#     pattern = r\"\"\n",
    "#     for se in se_order:\n",
    "#         if se != 's':\n",
    "#             pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
    "#         else:\n",
    "#             pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
    "#     found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "#     for i in range(len(found)):\n",
    "#         for k, v in found[i].items():\n",
    "#             found[i][k] = found[i][k].strip()\n",
    "#     return found\n",
    "def catch_answer(output,se_order):\n",
    "    output = output.replace(\"<pad>\",'')\n",
    "    output = output.replace(\"</s>\",'')\n",
    "    pattern = []\n",
    "    for se in se_order:\n",
    "        if se != 's':\n",
    "            pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\")\n",
    "        else:\n",
    "            pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\")\n",
    "    pattern = ','.join(pattern)\n",
    "    pattern = f\"\\({pattern}\\)\"\n",
    "    found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "    for i in range(len(found)):\n",
    "        for k, v in found[i].items():\n",
    "            found[i][k] = found[i][k].strip()\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'opinion': 'no', 'aspect': 'GUI', 'sentiment': 'negative'},\n",
       " {'opinion': 'dark', 'aspect': 'screen', 'sentiment': 'negative'},\n",
       " {'opinion': 'steady', 'aspect': 'power light', 'sentiment': 'neutral'},\n",
       " {'opinion': 'steady', 'aspect': 'hard drive light', 'sentiment': 'negative'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")\n",
    "se_order = \"oas\"\n",
    "catch_answer(output,se_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( no , GUI , negative ) ; ( dark , screen , negative ) ; ( steady , power light , neutral ) ; ( steady , hard drive light , negative )'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "peng_2 = dict()\n",
    "for domain, v1 in peng_intermediate.items():\n",
    "    peng_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in peng_intermediate[domain][basic_task][\"train\"]:\n",
    "            peng_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        peng_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        peng_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    peng_2[domain][\"train\"] = Dataset.from_list(peng_2[domain][\"train\"])\n",
    "    peng_2[domain][\"val\"] = Dataset.from_list(peng_2[domain][\"val\"])\n",
    "    peng_2[domain][\"test\"] = Dataset.from_list(peng_2[domain][\"test\"])\n",
    "\n",
    "wan_2 = dict()\n",
    "for domain, v1 in wan_intermediate.items():\n",
    "    wan_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"asc\"]:\n",
    "        for el in wan_intermediate[domain][basic_task][\"train\"]:\n",
    "            wan_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"val\"]:\n",
    "        wan_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"test\"]:\n",
    "        wan_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    wan_2[domain][\"train\"] = Dataset.from_list(wan_2[domain][\"train\"])\n",
    "    wan_2[domain][\"val\"] = Dataset.from_list(wan_2[domain][\"val\"])\n",
    "    wan_2[domain][\"test\"] = Dataset.from_list(wan_2[domain][\"test\"])\n",
    "\n",
    "zhang_2 = dict()\n",
    "for domain, v1 in zhang_intermediate.items():\n",
    "    zhang_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oasc\"]:\n",
    "        for el in zhang_intermediate[domain][basic_task][\"train\"]:\n",
    "            zhang_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"val\"]:\n",
    "        zhang_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"test\"]:\n",
    "        zhang_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    zhang_2[domain][\"train\"] = Dataset.from_list(zhang_2[domain][\"train\"])\n",
    "    zhang_2[domain][\"val\"] = Dataset.from_list(zhang_2[domain][\"val\"])\n",
    "    zhang_2[domain][\"test\"] = Dataset.from_list(zhang_2[domain][\"test\"])\n",
    "\n",
    "william_2 = dict()\n",
    "for domain, v1 in william_intermediate.items():\n",
    "    william_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in william_intermediate[domain][basic_task][\"train\"]:\n",
    "            william_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in william_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        william_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in william_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        william_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    william_2[domain][\"train\"] = Dataset.from_list(william_2[domain][\"train\"])\n",
    "    william_2[domain][\"val\"] = Dataset.from_list(william_2[domain][\"val\"])\n",
    "    william_2[domain][\"test\"] = Dataset.from_list(william_2[domain][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tempat yag bagus dan nyaman untuk istirahat tetapi tolong tvnya perlu di perbaiki channelnya karena banyak semutnya digambar dan water heaternya tidak bisa jadi mandi air dingin terus . | ( opinion , aspect , sentiment )',\n",
       " 'output': '( bagus , tempat , positive ) ; ( nyaman , tempat , positive ) ; ( perlu di perbaiki , tvnya , positive ) ; ( tidak bisa , water heaternya , negative )',\n",
       " 'task': 'oas'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "william_2[\"hotel\"][\"train\"][69]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Tokenized Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_en.add_tokens(list(added_tokens.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_args = {\n",
    "    \"max_length\" : 512,\n",
    "    \"padding\" : True,\n",
    "    \"truncation\" : True,\n",
    "    \"return_tensors\" : \"pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_en(dataset):\n",
    "    result = tokenizer_en(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "peng_tok = dict()\n",
    "for domain, v1 in peng_2.items():\n",
    "    peng_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            peng_tok[domain][split] = peng_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            peng_tok[domain][split] = encode_en(peng_2[domain][split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "wan_tok = dict()\n",
    "for domain, v1 in wan_2.items():\n",
    "    wan_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            wan_tok[domain][split] = wan_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            wan_tok[domain][split] = encode_en(wan_2[domain][split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "zhang_tok = dict()\n",
    "for domain, v1 in zhang_2.items():\n",
    "    zhang_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            zhang_tok[domain][split] = zhang_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            zhang_tok[domain][split] = encode_en(zhang_2[domain][split])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"id_ID\", tgt_lang=\"id_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_id(dataset):\n",
    "    result = tokenizer_id(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "william_tok = dict()\n",
    "for domain, v1 in william_2.items():\n",
    "    william_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            william_tok[domain][split] = william_2[domain][split].map(encode_id,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            william_tok[domain][split] = encode_id(william_2[domain][split])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator_en = DataCollatorForSeq2Seq(tokenizer=tokenizer_en)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_id = DataCollatorForSeq2Seq(tokenizer=tokenizer_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from evaluation import recall, precision, f1_score, summary_score\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def seperate_target_prediction_per_task(predictions:List[List[Dict]],targets:List[List[Dict]],tasks:List) -> Tuple[Dict[str,List],Dict[str,List]]:\n",
    "    per_task_targets = {}\n",
    "    per_task_predictions = {}\n",
    "    for target, prediction, task in zip(targets,predictions,tasks):\n",
    "        if task not in per_task_targets.keys():\n",
    "            per_task_targets[task] = []\n",
    "        if task not in per_task_predictions.keys():\n",
    "            per_task_predictions[task] = []\n",
    "        per_task_targets[task].append(target)\n",
    "        per_task_predictions[task].append(prediction)\n",
    "    return per_task_targets, per_task_predictions\n",
    "\n",
    "def preprocess_eval_preds(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer):\n",
    "    input_ids = eval_preds.inputs\n",
    "    target_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(input_ids, tuple):\n",
    "        input_ids = input_ids[0]\n",
    "    if isinstance(target_ids, tuple):\n",
    "        target_ids = target_ids[0]\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    input_ids = np.argmax(input_ids,axis=-1) if len(input_ids.shape) == 3 else input_ids # in case not predict with generate\n",
    "    target_ids = np.argmax(target_ids,axis=-1) if len(target_ids.shape) == 3 else target_ids # in case not predict with generate\n",
    "    prediction_ids = np.argmax(pred_ids,axis=-1) if len(pred_ids.shape) == 3 else pred_ids # in case not predict with generate\n",
    "\n",
    "    input_ids = [[token for token in row if token != -100] for row in input_ids]\n",
    "    target_ids = [[token for token in row if token != -100] for row in target_ids]\n",
    "    prediction_ids = [[token for token in row if token != -100] for row in prediction_ids]\n",
    "\n",
    "    inputs = tokenizer.batch_decode(input_ids,**decoding_args)\n",
    "    targets = tokenizer.batch_decode(target_ids,**decoding_args)\n",
    "    predictions = tokenizer.batch_decode(prediction_ids,**decoding_args)\n",
    "\n",
    "    return inputs, targets, predictions\n",
    "\n",
    "def compute_metrics(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer,tasks:List) -> Dict[str,float]: # MAY NOT BE SUFFICIATE FOR CAUSAL LM\n",
    "        \"\"\"\n",
    "        ### DESC\n",
    "            Method to compute the metrics.\n",
    "        ### PARAMS\n",
    "        * eval_preds: EvalPrediction instance from training.\n",
    "        * decoding_args: Decoding arguments.\n",
    "        ### RETURN\n",
    "        * metrics: Dictionary of metrics.\n",
    "        \"\"\"\n",
    "        inputs, targets, predictions = preprocess_eval_preds(eval_preds,decoding_args,tokenizer)\n",
    "\n",
    "        targets = [catch_answer(text,task) for text,task in zip(targets,tasks) if task != \"non_absa\"]\n",
    "        predictions = [catch_answer(text,task) for text,task in zip(predictions,tasks) if task != \"non_absa\"]\n",
    "\n",
    "\n",
    "        per_task_targets, per_task_predictions = seperate_target_prediction_per_task(predictions, targets, tasks)\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        metrics[\"overall_recall\"] = recall(predictions,targets)\n",
    "        metrics[\"overall_precision\"] = precision(predictions,targets)\n",
    "        metrics[\"overall_f1_score\"] = f1_score(predictions,targets)\n",
    "\n",
    "        for task in per_task_targets.keys():\n",
    "            if task == \"non_absa\":\n",
    "                continue\n",
    "            metrics[f\"{task}_recall\"] = recall(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_precision\"] = precision(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_f1_score\"] = f1_score(per_task_predictions[task],per_task_targets[task])\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"per_device_train_batch_size\": 16//n_gpu,\n",
    "    \"per_device_eval_batch_size\": 16//n_gpu,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_strategy\" : \"epoch\",\n",
    "    \"metric_for_best_model\": \"overall_f1_score\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"adam_epsilon\": 1e-08,\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"logging_dir\" : \"./output/log\",\n",
    "    \"include_inputs_for_metrics\" : True\n",
    "}\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(**train_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# trainer = {\n",
    "#     \"peng\" : {},\n",
    "#     \"wan\" : {},\n",
    "#     \"zhang\" : {},\n",
    "#     \"william\" : {}\n",
    "# }\n",
    "\n",
    "decoding_args = {\n",
    "    \"skip_special_tokens\" : False\n",
    "}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, targets):\n",
    "    pred_logits = logits[0] if isinstance(logits,tuple) else logits\n",
    "    pred_ids = torch.argmax(pred_logits, dim=-1)\n",
    "    return pred_ids, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_predictions(model,tokenizer,tokenized:torch.Tensor,device:torch.device=torch.device(\"cpu\"),batch_size:int=16,max_len:int=512,decoding_args:Dict={}) -> List[str]:\n",
    "    # Data loader\n",
    "    input_ids_data_loader = torch.utils.data.DataLoader(tokenized[\"input_ids\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    attention_mask_data_loader = torch.utils.data.DataLoader(tokenized[\"attention_mask\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    # Predict\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    tensor_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(zip(input_ids_data_loader,attention_mask_data_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            tensor_predictions.extend(model.generate(input_ids=input_ids,attention_mask=attention_mask,max_length=max_len,pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id).cpu())\n",
    "            input_ids = input_ids.cpu()\n",
    "            attention_mask = attention_mask.cpu()\n",
    "    tensor_predictions = [[token for token in row if token != -100] for row in tensor_predictions]\n",
    "    predictions = tokenizer.batch_decode(tensor_predictions,**decoding_args)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_result(str_preds_,preds,targets,filename):\n",
    "    result = []\n",
    "    str_preds = [el.replace(\"<pad>\",'').replace(\"</s>\",'') for el in str_preds_]\n",
    "    assert len(str_preds) == len(preds) == len(targets)\n",
    "    for i in range(len(str_preds)):\n",
    "        result.append({\n",
    "            \"str_pred\" : str_preds[i],\n",
    "            \"pred\" : preds[i],\n",
    "            \"target\" : targets[i]\n",
    "        })\n",
    "    \n",
    "    with open(filename,'w') as fp:\n",
    "        json.dump(result,fp)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Laptop 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4530\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2840\n",
      "  Number of trainable parameters = 139420416\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='2840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  61/2840 00:14 < 11:19, 4.09 it/s, Epoch 0.42/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-142\n",
      "Configuration saved in ./output/checkpoint-142/config.json\n",
      "Model weights saved in ./output/checkpoint-142/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-142/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-142/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-284\n",
      "Configuration saved in ./output/checkpoint-284/config.json\n",
      "Model weights saved in ./output/checkpoint-284/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-284/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-284/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-426\n",
      "Configuration saved in ./output/checkpoint-426/config.json\n",
      "Model weights saved in ./output/checkpoint-426/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-426/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-426/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-142] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-568\n",
      "Configuration saved in ./output/checkpoint-568/config.json\n",
      "Model weights saved in ./output/checkpoint-568/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-568/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-568/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-710\n",
      "Configuration saved in ./output/checkpoint-710/config.json\n",
      "Model weights saved in ./output/checkpoint-710/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-710/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-710/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-426] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-852\n",
      "Configuration saved in ./output/checkpoint-852/config.json\n",
      "Model weights saved in ./output/checkpoint-852/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-852/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-852/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-710] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-994\n",
      "Configuration saved in ./output/checkpoint-994/config.json\n",
      "Model weights saved in ./output/checkpoint-994/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-994/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-994/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-852] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1136\n",
      "Configuration saved in ./output/checkpoint-1136/config.json\n",
      "Model weights saved in ./output/checkpoint-1136/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1136/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1136/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-994] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1278\n",
      "Configuration saved in ./output/checkpoint-1278/config.json\n",
      "Model weights saved in ./output/checkpoint-1278/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1278/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1278/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1136] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1420\n",
      "Configuration saved in ./output/checkpoint-1420/config.json\n",
      "Model weights saved in ./output/checkpoint-1420/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1420/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1420/special_tokens_map.json\n",
      "Saving model checkpoint to ./output/checkpoint-1562\n",
      "Configuration saved in ./output/checkpoint-1562/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-1704\n",
      "Configuration saved in ./output/checkpoint-1704/config.json\n",
      "Model weights saved in ./output/checkpoint-1704/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1704/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1704/special_tokens_map.json\n",
      "Saving model checkpoint to ./output/checkpoint-1846\n",
      "Configuration saved in ./output/checkpoint-1846/config.json\n",
      "Model weights saved in ./output/checkpoint-1846/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1846/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1846/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1562] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1988\n",
      "Configuration saved in ./output/checkpoint-1988/config.json\n",
      "Model weights saved in ./output/checkpoint-1988/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1988/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1988/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1846] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2130\n",
      "Configuration saved in ./output/checkpoint-2130/config.json\n",
      "Model weights saved in ./output/checkpoint-2130/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2130/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2130/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1988] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2272\n",
      "Configuration saved in ./output/checkpoint-2272/config.json\n",
      "Model weights saved in ./output/checkpoint-2272/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2272/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2272/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2130] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2414\n",
      "Configuration saved in ./output/checkpoint-2414/config.json\n",
      "Model weights saved in ./output/checkpoint-2414/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2414/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2414/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2272] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2556\n",
      "Configuration saved in ./output/checkpoint-2556/config.json\n",
      "Model weights saved in ./output/checkpoint-2556/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2556/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2556/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2414] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2698\n",
      "Configuration saved in ./output/checkpoint-2698/config.json\n",
      "Model weights saved in ./output/checkpoint-2698/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2698/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2698/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2556] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2840, training_loss=0.07642156593763912, metrics={'train_runtime': 778.8109, 'train_samples_per_second': 116.331, 'train_steps_per_second': 3.647, 'total_flos': 5610490020495360.0, 'train_loss': 0.07642156593763912, 'epoch': 20.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"lap14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"lap14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"lap14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:13,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"lap14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"lap14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.2846580406654344,\n",
       " 'precision': 0.4196185286103542,\n",
       " 'f1_score': 0.33920704845814975}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_lap14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6330\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3960\n",
      "  Number of trainable parameters = 139420416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='3960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 288/3960 01:29 < 19:14, 3.18 it/s, Epoch 1.45/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.732200</td>\n",
       "      <td>0.134896</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.413721</td>\n",
       "      <td>0.369202</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.413721</td>\n",
       "      <td>0.369202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-198\n",
      "Configuration saved in ./output/checkpoint-198/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-198\n",
      "Configuration saved in ./output/checkpoint-198/config.json\n",
      "Model weights saved in ./output/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-198/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-396\n",
      "Configuration saved in ./output/checkpoint-396/config.json\n",
      "Model weights saved in ./output/checkpoint-396/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-396/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-396/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-594\n",
      "Configuration saved in ./output/checkpoint-594/config.json\n",
      "Model weights saved in ./output/checkpoint-594/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-594/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-594/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-198] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-792\n",
      "Configuration saved in ./output/checkpoint-792/config.json\n",
      "Model weights saved in ./output/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-792/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-396] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-990\n",
      "Configuration saved in ./output/checkpoint-990/config.json\n",
      "Model weights saved in ./output/checkpoint-990/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-990/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-990/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-594] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1188\n",
      "Configuration saved in ./output/checkpoint-1188/config.json\n",
      "Model weights saved in ./output/checkpoint-1188/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1188/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1188/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-990] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1386\n",
      "Configuration saved in ./output/checkpoint-1386/config.json\n",
      "Model weights saved in ./output/checkpoint-1386/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1386/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1386/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1188] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1584\n",
      "Configuration saved in ./output/checkpoint-1584/config.json\n",
      "Model weights saved in ./output/checkpoint-1584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1386] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1782\n",
      "Configuration saved in ./output/checkpoint-1782/config.json\n",
      "Model weights saved in ./output/checkpoint-1782/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1782/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1782/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1584] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1980\n",
      "Configuration saved in ./output/checkpoint-1980/config.json\n",
      "Model weights saved in ./output/checkpoint-1980/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1980/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1980/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-792] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2178\n",
      "Configuration saved in ./output/checkpoint-2178/config.json\n",
      "Model weights saved in ./output/checkpoint-2178/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2178/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2178/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1782] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2376\n",
      "Configuration saved in ./output/checkpoint-2376/config.json\n",
      "Model weights saved in ./output/checkpoint-2376/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2376/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2376/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2178] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2574\n",
      "Configuration saved in ./output/checkpoint-2574/config.json\n",
      "Model weights saved in ./output/checkpoint-2574/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2574/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2574/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2376] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2772\n",
      "Configuration saved in ./output/checkpoint-2772/config.json\n",
      "Model weights saved in ./output/checkpoint-2772/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2772/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2772/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2574] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2970\n",
      "Configuration saved in ./output/checkpoint-2970/config.json\n",
      "Model weights saved in ./output/checkpoint-2970/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2970/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2970/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2772] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3168\n",
      "Configuration saved in ./output/checkpoint-3168/config.json\n",
      "Model weights saved in ./output/checkpoint-3168/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3168/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3168/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2970] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3366\n",
      "Configuration saved in ./output/checkpoint-3366/config.json\n",
      "Model weights saved in ./output/checkpoint-3366/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3366/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3366/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3168] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3564\n",
      "Configuration saved in ./output/checkpoint-3564/config.json\n",
      "Model weights saved in ./output/checkpoint-3564/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3564/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3564/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3366] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3762\n",
      "Configuration saved in ./output/checkpoint-3762/config.json\n",
      "Model weights saved in ./output/checkpoint-3762/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3762/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3762/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3564] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3960\n",
      "Configuration saved in ./output/checkpoint-3960/config.json\n",
      "Model weights saved in ./output/checkpoint-3960/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3960/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3960/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3762] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/checkpoint-1980 (score: 0.49037606494363334).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3960, training_loss=0.04736596604225912, metrics={'train_runtime': 1277.6034, 'train_samples_per_second': 99.092, 'train_steps_per_second': 3.1, 'total_flos': 9497740957286400.0, 'train_loss': 0.04736596604225912, 'epoch': 20.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:37,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res14\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.34909456740442657,\n",
       " 'precision': 0.4928977272727273,\n",
       " 'f1_score': 0.408716136631331}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3025\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1900\n",
      "  Number of trainable parameters = 139420416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  96/1900 00:26 < 08:36, 3.49 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/5 00:00 < 00:00, 15.49 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-95\n",
      "Configuration saved in ./output/checkpoint-95/config.json\n",
      "Model weights saved in ./output/checkpoint-95/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-95/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-95/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-190\n",
      "Configuration saved in ./output/checkpoint-190/config.json\n",
      "Model weights saved in ./output/checkpoint-190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-190/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-285\n",
      "Configuration saved in ./output/checkpoint-285/config.json\n",
      "Model weights saved in ./output/checkpoint-285/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-285/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-285/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-95] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-380\n",
      "Configuration saved in ./output/checkpoint-380/config.json\n",
      "Model weights saved in ./output/checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-475\n",
      "Configuration saved in ./output/checkpoint-475/config.json\n",
      "Model weights saved in ./output/checkpoint-475/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-475/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-475/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-285] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-570\n",
      "Configuration saved in ./output/checkpoint-570/config.json\n",
      "Model weights saved in ./output/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-570/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-665\n",
      "Configuration saved in ./output/checkpoint-665/config.json\n",
      "Model weights saved in ./output/checkpoint-665/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-665/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-665/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-475] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-760\n",
      "Configuration saved in ./output/checkpoint-760/config.json\n",
      "Model weights saved in ./output/checkpoint-760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-665] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-855\n",
      "Configuration saved in ./output/checkpoint-855/config.json\n",
      "Model weights saved in ./output/checkpoint-855/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-855/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-855/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-570] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-950\n",
      "Configuration saved in ./output/checkpoint-950/config.json\n",
      "Model weights saved in ./output/checkpoint-950/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-760] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1045\n",
      "Configuration saved in ./output/checkpoint-1045/config.json\n",
      "Model weights saved in ./output/checkpoint-1045/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1045/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1045/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-855] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1140\n",
      "Configuration saved in ./output/checkpoint-1140/config.json\n",
      "Model weights saved in ./output/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1140/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-950] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1235\n",
      "Configuration saved in ./output/checkpoint-1235/config.json\n",
      "Model weights saved in ./output/checkpoint-1235/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1235/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1235/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1330\n",
      "Configuration saved in ./output/checkpoint-1330/config.json\n",
      "Model weights saved in ./output/checkpoint-1330/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1330/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1330/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1235] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1425\n",
      "Configuration saved in ./output/checkpoint-1425/config.json\n",
      "Model weights saved in ./output/checkpoint-1425/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1425/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1425/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1520\n",
      "Configuration saved in ./output/checkpoint-1520/config.json\n",
      "Model weights saved in ./output/checkpoint-1520/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1520/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1520/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1425] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1615\n",
      "Configuration saved in ./output/checkpoint-1615/config.json\n",
      "Model weights saved in ./output/checkpoint-1615/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1615/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1615/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1520] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1710\n",
      "Configuration saved in ./output/checkpoint-1710/config.json\n",
      "Model weights saved in ./output/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1710/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1615] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1805\n",
      "Configuration saved in ./output/checkpoint-1805/config.json\n",
      "Model weights saved in ./output/checkpoint-1805/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1805/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1805/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1710] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1900\n",
      "Configuration saved in ./output/checkpoint-1900/config.json\n",
      "Model weights saved in ./output/checkpoint-1900/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1805] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/checkpoint-1045 (score: 0.5339073543619384).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1900, training_loss=0.07466605609184818, metrics={'train_runtime': 631.2337, 'train_samples_per_second': 95.844, 'train_steps_per_second': 3.01, 'total_flos': 4070764938240000.0, 'train_loss': 0.07466605609184818, 'epoch': 20.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:13,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.37938144329896906,\n",
       " 'precision': 0.4754521963824289,\n",
       " 'f1_score': 0.4220183486238532}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4285\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2680\n",
      "  Number of trainable parameters = 139420416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='2680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 101/2680 00:27 < 12:00, 3.58 it/s, Epoch 0.75/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-134\n",
      "Configuration saved in ./output/checkpoint-134/config.json\n",
      "Model weights saved in ./output/checkpoint-134/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-134/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-134/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-268\n",
      "Configuration saved in ./output/checkpoint-268/config.json\n",
      "Model weights saved in ./output/checkpoint-268/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-268/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-268/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-402\n",
      "Configuration saved in ./output/checkpoint-402/config.json\n",
      "Model weights saved in ./output/checkpoint-402/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-402/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-402/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-134] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-536\n",
      "Configuration saved in ./output/checkpoint-536/config.json\n",
      "Model weights saved in ./output/checkpoint-536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-536/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-670\n",
      "Configuration saved in ./output/checkpoint-670/config.json\n",
      "Model weights saved in ./output/checkpoint-670/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-670/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-670/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-402] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-804\n",
      "Configuration saved in ./output/checkpoint-804/config.json\n",
      "Model weights saved in ./output/checkpoint-804/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-804/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-804/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-670] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-938\n",
      "Configuration saved in ./output/checkpoint-938/config.json\n",
      "Model weights saved in ./output/checkpoint-938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-938/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-536] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1072\n",
      "Configuration saved in ./output/checkpoint-1072/config.json\n",
      "Model weights saved in ./output/checkpoint-1072/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1072/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1072/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-804] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1206\n",
      "Configuration saved in ./output/checkpoint-1206/config.json\n",
      "Model weights saved in ./output/checkpoint-1206/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1206/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1206/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-938] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1340\n",
      "Configuration saved in ./output/checkpoint-1340/config.json\n",
      "Model weights saved in ./output/checkpoint-1340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1072] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1474\n",
      "Configuration saved in ./output/checkpoint-1474/config.json\n",
      "Model weights saved in ./output/checkpoint-1474/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1474/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1474/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1206] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1608\n",
      "Configuration saved in ./output/checkpoint-1608/config.json\n",
      "Model weights saved in ./output/checkpoint-1608/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1608/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1608/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1474] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1742\n",
      "Configuration saved in ./output/checkpoint-1742/config.json\n",
      "Model weights saved in ./output/checkpoint-1742/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1742/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1742/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1608] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1876\n",
      "Configuration saved in ./output/checkpoint-1876/config.json\n",
      "Model weights saved in ./output/checkpoint-1876/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1876/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1876/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1742] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2010\n",
      "Configuration saved in ./output/checkpoint-2010/config.json\n",
      "Model weights saved in ./output/checkpoint-2010/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2010/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2010/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1340] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2144\n",
      "Configuration saved in ./output/checkpoint-2144/config.json\n",
      "Model weights saved in ./output/checkpoint-2144/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2144/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2144/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1876] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2278\n",
      "Configuration saved in ./output/checkpoint-2278/config.json\n",
      "Model weights saved in ./output/checkpoint-2278/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2278/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2278/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2010] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2412\n",
      "Configuration saved in ./output/checkpoint-2412/config.json\n",
      "Model weights saved in ./output/checkpoint-2412/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2412/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2412/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2278] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2546\n",
      "Configuration saved in ./output/checkpoint-2546/config.json\n",
      "Model weights saved in ./output/checkpoint-2546/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2546/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2546/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2144] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2680\n",
      "Configuration saved in ./output/checkpoint-2680/config.json\n",
      "Model weights saved in ./output/checkpoint-2680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2412] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/checkpoint-2546 (score: 0.5706988199587028).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2680, training_loss=0.06290284601915906, metrics={'train_runtime': 822.7626, 'train_samples_per_second': 104.161, 'train_steps_per_second': 3.257, 'total_flos': 5766356284416000.0, 'train_loss': 0.06290284601915906, 'epoch': 20.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:57,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, peng_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.4396887159533074,\n",
       " 'precision': 0.517162471395881,\n",
       " 'f1_score': 0.4752891692954785}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5600\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3500\n",
      "  Number of trainable parameters = 139420416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/3500 00:52 < 16:53, 3.28 it/s, Epoch 0.99/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=0.043617513888648575, metrics={'train_runtime': 1135.8677, 'train_samples_per_second': 98.603, 'train_steps_per_second': 3.081, 'total_flos': 7802492587868160.0, 'train_loss': 0.043617513888648575, 'epoch': 20.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:27,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.3727810650887574,\n",
       " 'precision': 0.4474431818181818,\n",
       " 'f1_score': 0.40671400903808913}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8540\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n",
      "  Number of trainable parameters = 139420416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  53/5340 00:15 < 26:25, 3.33 it/s, Epoch 0.19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-267\n",
      "Configuration saved in ./output/checkpoint-267/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-267\n",
      "Configuration saved in ./output/checkpoint-267/config.json\n",
      "Model weights saved in ./output/checkpoint-267/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-267/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-267/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-534\n",
      "Configuration saved in ./output/checkpoint-534/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-534\n",
      "Configuration saved in ./output/checkpoint-534/config.json\n",
      "Model weights saved in ./output/checkpoint-534/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-534/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-534/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-801\n",
      "Configuration saved in ./output/checkpoint-801/config.json\n",
      "Model weights saved in ./output/checkpoint-801/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-801/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-801/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-267] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1068\n",
      "Configuration saved in ./output/checkpoint-1068/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-1068\n",
      "Configuration saved in ./output/checkpoint-1068/config.json\n",
      "Model weights saved in ./output/checkpoint-1068/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1068/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1068/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-534] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1335\n",
      "Configuration saved in ./output/checkpoint-1335/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-1335\n",
      "Configuration saved in ./output/checkpoint-1335/config.json\n",
      "Model weights saved in ./output/checkpoint-1335/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1335/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1335/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-801] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1602\n",
      "Configuration saved in ./output/checkpoint-1602/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-1602\n",
      "Configuration saved in ./output/checkpoint-1602/config.json\n",
      "Model weights saved in ./output/checkpoint-1602/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1602/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1602/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1068] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1869\n",
      "Configuration saved in ./output/checkpoint-1869/config.json\n",
      "Model weights saved in ./output/checkpoint-1869/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1869/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1869/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1602] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2136\n",
      "Configuration saved in ./output/checkpoint-2136/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-2136\n",
      "Configuration saved in ./output/checkpoint-2136/config.json\n",
      "Model weights saved in ./output/checkpoint-2136/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2136/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2136/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1335] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2403\n",
      "Configuration saved in ./output/checkpoint-2403/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-2403\n",
      "Configuration saved in ./output/checkpoint-2403/config.json\n",
      "Model weights saved in ./output/checkpoint-2403/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2403/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2403/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1869] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2670\n",
      "Configuration saved in ./output/checkpoint-2670/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-2670\n",
      "Configuration saved in ./output/checkpoint-2670/config.json\n",
      "Model weights saved in ./output/checkpoint-2670/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2670/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2670/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2403] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2937\n",
      "Configuration saved in ./output/checkpoint-2937/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-2937\n",
      "Configuration saved in ./output/checkpoint-2937/config.json\n",
      "Model weights saved in ./output/checkpoint-2937/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2937/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2937/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2136] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3204\n",
      "Configuration saved in ./output/checkpoint-3204/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-3204\n",
      "Configuration saved in ./output/checkpoint-3204/config.json\n",
      "Model weights saved in ./output/checkpoint-3204/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3204/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3204/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2670] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3471\n",
      "Configuration saved in ./output/checkpoint-3471/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-3471\n",
      "Configuration saved in ./output/checkpoint-3471/config.json\n",
      "Model weights saved in ./output/checkpoint-3471/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3471/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3471/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3204] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3738\n",
      "Configuration saved in ./output/checkpoint-3738/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-3738\n",
      "Configuration saved in ./output/checkpoint-3738/config.json\n",
      "Model weights saved in ./output/checkpoint-3738/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3738/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3738/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3471] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-4005\n",
      "Configuration saved in ./output/checkpoint-4005/config.json\n",
      "Model weights saved in ./output/checkpoint-4005/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4005/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4005/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3738] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-4272\n",
      "Configuration saved in ./output/checkpoint-4272/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-4272\n",
      "Configuration saved in ./output/checkpoint-4272/config.json\n",
      "Model weights saved in ./output/checkpoint-4272/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4272/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4272/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4005] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-4539\n",
      "Configuration saved in ./output/checkpoint-4539/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-4539\n",
      "Configuration saved in ./output/checkpoint-4539/config.json\n",
      "Model weights saved in ./output/checkpoint-4539/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4539/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4539/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4272] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4806\n",
      "Configuration saved in ./output/checkpoint-4806/config.json\n",
      "Model weights saved in ./output/checkpoint-4806/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4806/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4806/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4539] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-5073\n",
      "Configuration saved in ./output/checkpoint-5073/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-5073\n",
      "Configuration saved in ./output/checkpoint-5073/config.json\n",
      "Model weights saved in ./output/checkpoint-5073/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5073/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5073/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4806] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-5340\n",
      "Configuration saved in ./output/checkpoint-5340/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-5340\n",
      "Configuration saved in ./output/checkpoint-5340/config.json\n",
      "Model weights saved in ./output/checkpoint-5340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5073] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/checkpoint-2937 (score: 1.0).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5340, training_loss=0.03400218944005975, metrics={'train_runtime': 1672.9629, 'train_samples_per_second': 102.094, 'train_steps_per_second': 3.192, 'total_flos': 1.189491984285696e+16, 'train_loss': 0.03400218944005975, 'epoch': 20.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:34,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, wan_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.3969732246798603,\n",
       " 'precision': 0.4949201741654572,\n",
       " 'f1_score': 0.44056847545219635}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5838\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3660\n",
      "  Number of trainable parameters = 139420416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='107' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 107/3660 00:31 < 17:37, 3.36 it/s, Epoch 0.58/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-183\n",
      "Configuration saved in ./output/checkpoint-183/config.json\n",
      "Model weights saved in ./output/checkpoint-183/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-183/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-183/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-366\n",
      "Configuration saved in ./output/checkpoint-366/config.json\n",
      "Model weights saved in ./output/checkpoint-366/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-366/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-366/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-549\n",
      "Configuration saved in ./output/checkpoint-549/config.json\n",
      "Model weights saved in ./output/checkpoint-549/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-549/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-549/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-183] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-732\n",
      "Configuration saved in ./output/checkpoint-732/config.json\n",
      "Model weights saved in ./output/checkpoint-732/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-732/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-732/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-366] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-915\n",
      "Configuration saved in ./output/checkpoint-915/config.json\n",
      "Model weights saved in ./output/checkpoint-915/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-915/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-915/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-549] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1098\n",
      "Configuration saved in ./output/checkpoint-1098/config.json\n",
      "Model weights saved in ./output/checkpoint-1098/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1098/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1098/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-732] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1281\n",
      "Configuration saved in ./output/checkpoint-1281/config.json\n",
      "Model weights saved in ./output/checkpoint-1281/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1281/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1281/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-915] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1464\n",
      "Configuration saved in ./output/checkpoint-1464/config.json\n",
      "Model weights saved in ./output/checkpoint-1464/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1464/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1464/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1098] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1647\n",
      "Configuration saved in ./output/checkpoint-1647/config.json\n",
      "Model weights saved in ./output/checkpoint-1647/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1647/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1647/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1464] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1830\n",
      "Configuration saved in ./output/checkpoint-1830/config.json\n",
      "Model weights saved in ./output/checkpoint-1830/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1830/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1830/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1647] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2013\n",
      "Configuration saved in ./output/checkpoint-2013/config.json\n",
      "Model weights saved in ./output/checkpoint-2013/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2013/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2013/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1830] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2196\n",
      "Configuration saved in ./output/checkpoint-2196/config.json\n",
      "Model weights saved in ./output/checkpoint-2196/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2196/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2196/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2013] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2379\n",
      "Configuration saved in ./output/checkpoint-2379/config.json\n",
      "Model weights saved in ./output/checkpoint-2379/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2379/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2379/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1281] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2562\n",
      "Configuration saved in ./output/checkpoint-2562/config.json\n",
      "Model weights saved in ./output/checkpoint-2562/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2562/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2562/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2196] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2745\n",
      "Configuration saved in ./output/checkpoint-2745/config.json\n",
      "Model weights saved in ./output/checkpoint-2745/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2745/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2745/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2379] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2928\n",
      "Configuration saved in ./output/checkpoint-2928/config.json\n",
      "Model weights saved in ./output/checkpoint-2928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2745] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3111\n",
      "Configuration saved in ./output/checkpoint-3111/config.json\n",
      "Model weights saved in ./output/checkpoint-3111/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3111/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3111/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2928] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3294\n",
      "Configuration saved in ./output/checkpoint-3294/config.json\n",
      "Model weights saved in ./output/checkpoint-3294/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3294/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3294/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3111] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3477\n",
      "Configuration saved in ./output/checkpoint-3477/config.json\n",
      "Model weights saved in ./output/checkpoint-3477/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3477/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3477/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3294] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3660\n",
      "Configuration saved in ./output/checkpoint-3660/config.json\n",
      "Model weights saved in ./output/checkpoint-3660/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3660/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3660/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3477] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/checkpoint-2562 (score: 0.37966739880765615).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3660, training_loss=0.046242999769950825, metrics={'train_runtime': 1179.7427, 'train_samples_per_second': 98.971, 'train_steps_per_second': 3.102, 'total_flos': 6325996699975680.0, 'train_loss': 0.046242999769950825, 'epoch': 20.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:36,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res15\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.27547169811320754,\n",
       " 'precision': 0.365,\n",
       " 'f1_score': 0.31397849462365596}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8848\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5540\n",
      "  Number of trainable parameters = 139420416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='555' max='5540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 555/5540 03:04 < 27:46, 2.99 it/s, Epoch 2/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oasc Recall</th>\n",
       "      <th>Oasc Precision</th>\n",
       "      <th>Oasc F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.160848</td>\n",
       "      <td>0.219368</td>\n",
       "      <td>0.308743</td>\n",
       "      <td>0.256493</td>\n",
       "      <td>0.219368</td>\n",
       "      <td>0.308743</td>\n",
       "      <td>0.256493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/10 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-277\n",
      "Configuration saved in ./output/checkpoint-277/config.json\n",
      "Saving model checkpoint to ./output/checkpoint-277\n",
      "Configuration saved in ./output/checkpoint-277/config.json\n",
      "Model weights saved in ./output/checkpoint-277/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-277/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-277/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-554\n",
      "Configuration saved in ./output/checkpoint-554/config.json\n",
      "Model weights saved in ./output/checkpoint-554/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-554/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-554/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-831\n",
      "Configuration saved in ./output/checkpoint-831/config.json\n",
      "Model weights saved in ./output/checkpoint-831/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-831/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-831/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-277] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1108\n",
      "Configuration saved in ./output/checkpoint-1108/config.json\n",
      "Model weights saved in ./output/checkpoint-1108/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1108/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1108/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-554] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1385\n",
      "Configuration saved in ./output/checkpoint-1385/config.json\n",
      "Model weights saved in ./output/checkpoint-1385/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1385/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1385/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-831] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1662\n",
      "Configuration saved in ./output/checkpoint-1662/config.json\n",
      "Model weights saved in ./output/checkpoint-1662/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1662/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1662/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1108] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-1939\n",
      "Configuration saved in ./output/checkpoint-1939/config.json\n",
      "Model weights saved in ./output/checkpoint-1939/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1939/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1939/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1662] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2216\n",
      "Configuration saved in ./output/checkpoint-2216/config.json\n",
      "Model weights saved in ./output/checkpoint-2216/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2216/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2216/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1385] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2493\n",
      "Configuration saved in ./output/checkpoint-2493/config.json\n",
      "Model weights saved in ./output/checkpoint-2493/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2493/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2493/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1939] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-2770\n",
      "Configuration saved in ./output/checkpoint-2770/config.json\n",
      "Model weights saved in ./output/checkpoint-2770/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2770/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2770/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2493] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3047\n",
      "Configuration saved in ./output/checkpoint-3047/config.json\n",
      "Model weights saved in ./output/checkpoint-3047/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3047/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3047/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2216] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3324\n",
      "Configuration saved in ./output/checkpoint-3324/config.json\n",
      "Model weights saved in ./output/checkpoint-3324/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3324/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3324/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2770] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3601\n",
      "Configuration saved in ./output/checkpoint-3601/config.json\n",
      "Model weights saved in ./output/checkpoint-3601/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3601/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3601/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3324] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-3878\n",
      "Configuration saved in ./output/checkpoint-3878/config.json\n",
      "Model weights saved in ./output/checkpoint-3878/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3878/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3878/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3601] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-4155\n",
      "Configuration saved in ./output/checkpoint-4155/config.json\n",
      "Model weights saved in ./output/checkpoint-4155/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4155/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4155/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3878] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-4432\n",
      "Configuration saved in ./output/checkpoint-4432/config.json\n",
      "Model weights saved in ./output/checkpoint-4432/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4432/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4432/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4155] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-4709\n",
      "Configuration saved in ./output/checkpoint-4709/config.json\n",
      "Model weights saved in ./output/checkpoint-4709/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4709/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4709/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3047] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4986\n",
      "Configuration saved in ./output/checkpoint-4986/config.json\n",
      "Model weights saved in ./output/checkpoint-4986/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4986/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4986/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4432] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-5263\n",
      "Configuration saved in ./output/checkpoint-5263/config.json\n",
      "Model weights saved in ./output/checkpoint-5263/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5263/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5263/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4986] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./output/checkpoint-5540\n",
      "Configuration saved in ./output/checkpoint-5540/config.json\n",
      "Model weights saved in ./output/checkpoint-5540/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5540/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5540/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5263] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/checkpoint-4709 (score: 0.42255177369830793).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5540, training_loss=0.037715545970448947, metrics={'train_runtime': 1897.7976, 'train_samples_per_second': 93.245, 'train_steps_per_second': 2.919, 'total_flos': 1.252974124007424e+16, 'train_loss': 0.037715545970448947, 'epoch': 20.0})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:42,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_en, zhang_tok[\"res16\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.3241551939924906,\n",
       " 'precision': 0.40279937791601866,\n",
       " 'f1_score': 0.35922330097087374}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# William Hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 9380\n",
      "  Number of trainable parameters = 610879488\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='470' max='9380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 470/9380 11:59 < 3:48:21, 0.65 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/63 00:07 < 00:14, 2.90 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-469\n",
      "Configuration saved in ./output/checkpoint-469/config.json\n",
      "Model weights saved in ./output/checkpoint-469/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-469/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-469/special_tokens_map.json\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-938\n",
      "Configuration saved in ./output/checkpoint-938/config.json\n",
      "Model weights saved in ./output/checkpoint-938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-938/special_tokens_map.json\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1407\n",
      "Configuration saved in ./output/checkpoint-1407/config.json\n",
      "Model weights saved in ./output/checkpoint-1407/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1407/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1407/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-469] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1876\n",
      "Configuration saved in ./output/checkpoint-1876/config.json\n",
      "Model weights saved in ./output/checkpoint-1876/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1876/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1876/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-938] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-2345\n",
      "Configuration saved in ./output/checkpoint-2345/config.json\n",
      "Model weights saved in ./output/checkpoint-2345/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2345/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2345/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1407] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-2814\n",
      "Configuration saved in ./output/checkpoint-2814/config.json\n",
      "Model weights saved in ./output/checkpoint-2814/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2814/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2814/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1876] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3283\n",
      "Configuration saved in ./output/checkpoint-3283/config.json\n",
      "Model weights saved in ./output/checkpoint-3283/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3283/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3283/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2345] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3752\n",
      "Configuration saved in ./output/checkpoint-3752/config.json\n",
      "Model weights saved in ./output/checkpoint-3752/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3752/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3752/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2814] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-4221\n",
      "Configuration saved in ./output/checkpoint-4221/config.json\n",
      "Model weights saved in ./output/checkpoint-4221/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4221/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4221/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3283] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-4690\n",
      "Configuration saved in ./output/checkpoint-4690/config.json\n",
      "Model weights saved in ./output/checkpoint-4690/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4690/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4690/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3752] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-5159\n",
      "Configuration saved in ./output/checkpoint-5159/config.json\n",
      "Model weights saved in ./output/checkpoint-5159/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5159/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5159/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4221] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-5628\n",
      "Configuration saved in ./output/checkpoint-5628/config.json\n",
      "Model weights saved in ./output/checkpoint-5628/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-5628/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5628/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4690] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-6097\n",
      "Configuration saved in ./output/checkpoint-6097/config.json\n",
      "Model weights saved in ./output/checkpoint-6097/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-6097/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6097/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5159] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-6566\n",
      "Configuration saved in ./output/checkpoint-6566/config.json\n",
      "Model weights saved in ./output/checkpoint-6566/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-6566/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6566/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5628] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-7035\n",
      "Configuration saved in ./output/checkpoint-7035/config.json\n",
      "Model weights saved in ./output/checkpoint-7035/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-7035/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7035/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6097] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-7504\n",
      "Configuration saved in ./output/checkpoint-7504/config.json\n",
      "Model weights saved in ./output/checkpoint-7504/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-7504/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7504/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6566] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-7973\n",
      "Configuration saved in ./output/checkpoint-7973/config.json\n",
      "Model weights saved in ./output/checkpoint-7973/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-7973/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7973/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7035] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-8442\n",
      "Configuration saved in ./output/checkpoint-8442/config.json\n",
      "Model weights saved in ./output/checkpoint-8442/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-8442/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8442/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7504] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-8911\n",
      "Configuration saved in ./output/checkpoint-8911/config.json\n",
      "Model weights saved in ./output/checkpoint-8911/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-8911/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8911/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7973] due to args.save_total_limit\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-9380\n",
      "Configuration saved in ./output/checkpoint-9380/config.json\n",
      "Model weights saved in ./output/checkpoint-9380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-9380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8442] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/checkpoint-8911 (score: 0.4109452997625535).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9380, training_loss=0.0810542557539462, metrics={'train_runtime': 15225.7737, 'train_samples_per_second': 19.703, 'train_steps_per_second': 0.616, 'total_flos': 1.0597212080032973e+17, 'train_loss': 0.0810542557539462, 'epoch': 20.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50\")\n",
    "model.to(device)\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_id,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = william_tok[\"hotel\"][\"train\"],\n",
    "        eval_dataset = william_tok[\"hotel\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_id,william_2[\"hotel\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:38,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "str_preds = generate_predictions(model, tokenizer_id, william_tok[\"hotel\"][\"test\"], device, 32, 512, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in william_2[\"hotel\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.3672942045035068,\n",
       " 'precision': 0.41181262729124235,\n",
       " 'f1_score': 0.3882815171752558}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"william_hotel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
