{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "sys.path.append(\"../../../src/\")\n",
    "import data_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_dir = dict(\n",
    "    lap14 = \"../../../data/absa/en/peng/14lap\",\n",
    "    res14 = \"../../../data/absa/en/peng/14res\",\n",
    "    res15 = \"../../../data/absa/en/peng/15res\",\n",
    "    res16 = \"../../../data/absa/en/peng/16res\"\n",
    ")\n",
    "\n",
    "wan_dir = dict(\n",
    "    res15 = \"../../../data/absa/en/wan/interim/rest15\",\n",
    "    res16 = \"../../../data/absa/en/wan/interim/rest16\"\n",
    ")\n",
    "    \n",
    "zhang_dir = dict(\n",
    "    res15 = \"../../../data/absa/en/zhang/interim/interim_2/rest15\",\n",
    "    res16 = \"../../../data/absa/en/zhang/interim/interim_2/rest16\"\n",
    ")\n",
    "\n",
    "william_dir = dict(\n",
    "    hotel = \"../../../data/absa/id/william\"\n",
    ")\n",
    "\n",
    "peng = dict(\n",
    "    lap14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"lap14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res14 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res14\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res14\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res14\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res15\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res15\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res15\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=peng_dir[\"res16\"] + \"/train_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=peng_dir[\"res16\"] + \"/dev_triplets.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=peng_dir[\"res16\"] + \"/test_triplets.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")\n",
    "\n",
    "wan = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=wan_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        val = data_utils.read_data(path=wan_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acs\"),\n",
    "        test = data_utils.read_data(path=wan_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acs\")\n",
    "    )\n",
    ")\n",
    "\n",
    "zhang = dict(\n",
    "    res15 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res15\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    ),\n",
    "    res16 = dict(\n",
    "        train = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/train.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        val = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acso\"),\n",
    "        test = data_utils.read_data(path=zhang_dir[\"res16\"] + \"/test.txt\",\n",
    "                                     target_format=\"acso\")\n",
    "    )\n",
    ")\n",
    "\n",
    "william = dict(\n",
    "    hotel = dict(\n",
    "        train = data_utils.read_data(path=william_dir[\"hotel\"] + \"/train.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        val = data_utils.read_data(path=william_dir[\"hotel\"] + \"/dev.txt\",\n",
    "                                     target_format=\"aos\"),\n",
    "        test = data_utils.read_data(path=william_dir[\"hotel\"] + \"/test.txt\",\n",
    "                                     target_format=\"aos\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.SENTIMENT_ELEMENT = {'a' : \"aspect\", 'o' : \"opinion\", 's' : \"sentiment\", 'c' : \"category\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AOS (ASTE)\n",
    "    * AO\n",
    "    * AS\n",
    "    * A\n",
    "    * O\n",
    "\n",
    "2. ACS (TASD)\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * C\n",
    "\n",
    "3. ACOS\n",
    "    * AO\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * O\n",
    "    * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oas', 'oa', 'as', 'a', 'o', 'asc', 'sc', 'c', 'oasc']\n"
     ]
    }
   ],
   "source": [
    "task_tree = {\n",
    "    \"oas\" : [\"oas\",\"oa\",\"as\",'a','o'],\n",
    "    \"asc\" : [\"asc\",\"as\",\"sc\",'a','c'],\n",
    "    \"oasc\" : [\"oasc\",\"oa\",\"as\",\"sc\",'a','o','c']\n",
    "}\n",
    "\n",
    "all_task = []\n",
    "for k,v1 in task_tree.items():\n",
    "    if k not in all_task:\n",
    "        all_task.append(k)\n",
    "    for v2 in v1:\n",
    "        if v2 not in all_task:\n",
    "            all_task.append(v2)\n",
    "\n",
    "print(all_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.remove_duplicate_targets(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"ao\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle mix may not be a must, but we'll see it later. Will be problematic if like as (UABSA / E2E ABSA) used for training AOS (ASTE) --> may be for further experiment because we will insert imputing later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'battery life', 'opinion': 'good', 'sentiment': 'mixed'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.handle_mix_sentiment(data_utils.reduce_targets([{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"positive\"},{'aspect': 'battery life', 'opinion': 'good', \"sentiment\" : \"negative\"}],\"aos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Peng (ASTE/AOS)\n",
    "peng_intermediate = dict()\n",
    "\n",
    "for domain, v1 in peng.items():\n",
    "    peng_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        peng_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = peng[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            peng_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wan (TASD/ACS)\n",
    "wan_intermediate = dict()\n",
    "\n",
    "for domain, v1 in wan.items():\n",
    "    wan_intermediate[domain] = dict()\n",
    "    for task in [\"asc\"] + task_tree[\"asc\"]:\n",
    "        wan_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = wan[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            wan_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang (ACOS)\n",
    "zhang_intermediate = dict()\n",
    "\n",
    "for domain, v1 in zhang.items():\n",
    "    zhang_intermediate[domain] = dict()\n",
    "    for task in [\"oasc\"] + task_tree[\"oasc\"]:\n",
    "        zhang_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = zhang[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            zhang_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William (AOS ID)\n",
    "william_intermediate = dict()\n",
    "\n",
    "for domain, v1 in william.items():\n",
    "    william_intermediate[domain] = dict()\n",
    "    for task in [\"oas\"] + task_tree[\"oas\"]:\n",
    "        william_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = william[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            william_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"<extra_id_X>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = {\n",
    "    ',' : \"<comma>\",\n",
    "    '(' : \"<open_bracket>\",\n",
    "    ')' : \"<close_bracket>\",\n",
    "    ';' : \"<semicolon>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_answer(targets,se_order):\n",
    "#     result = []\n",
    "#     counter = 0\n",
    "#     for t in targets:\n",
    "#         constructed_t = \"\"\n",
    "#         for se in se_order:\n",
    "#             if counter > 99:\n",
    "#                 raise Exception(\"Extra id more than 99!\")\n",
    "#             constructed_t += ' ' + mask.replace('X',str(counter)) + ' ' + t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "#             counter += 1\n",
    "#         constructed_t = constructed_t.strip()\n",
    "#         result.append(constructed_t)\n",
    "#     result = \" ; \".join(result)\n",
    "#     return result\n",
    "def construct_answer(targets,se_order):\n",
    "    result = []\n",
    "    for t in targets:\n",
    "        constructed_t = []\n",
    "        for se in se_order:\n",
    "            element = t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "            for k, v in added_tokens.items():\n",
    "                element = element.replace(k,v)\n",
    "            constructed_t.append(element)\n",
    "        constructed_t = \" , \".join(constructed_t)\n",
    "        constructed_t = f\"( {constructed_t} )\"\n",
    "        result.append(constructed_t)\n",
    "    result = \" ; \".join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( no , GUI , negative ) ; ( dark , screen , negative ) ; ( steady , power light , neutral ) ; ( steady , hard drive light , negative )'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( <open_bracket> tes3 <semicolon> tes4 <close_bracket> , tes1 <comma> tes2 , positive )'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_answer([{\"aspect\" : \"tes1 , tes2\", \"opinion\" : \"( tes3 ; tes4 )\", \"sentiment\" : \"positive\"}],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_prompt(text,se_order):\n",
    "#     prompt = []\n",
    "#     for counter, se in enumerate(se_order):\n",
    "#         prompt.append(data_utils.SENTIMENT_ELEMENT[se] + \" : \" + mask.replace('X',str(counter)))\n",
    "#     prompt = \" ,\".join(prompt)\n",
    "#     result = text + \"| \" + prompt\n",
    "#     return result\n",
    "def construct_prompt(text,se_order):\n",
    "    prompt = []\n",
    "    for se in se_order:\n",
    "        prompt.append(data_utils.SENTIMENT_ELEMENT[se])\n",
    "    prompt = \" , \".join(prompt)\n",
    "    prompt = f\"( {prompt} )\"\n",
    "    masked_text = text\n",
    "    for k, v in added_tokens.items():\n",
    "        masked_text = masked_text.replace(k,v)\n",
    "    result = masked_text + \" | \" + prompt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One night I turned the freaking thing off after using it <comma> the next day I turn it on <comma> no GUI <comma> screen all dark <comma> power light steady <comma> hard drive light steady and not flashing as it usually does . | ( opinion , aspect , sentiment )'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_prompt(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"text\"],\"oas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def catch_answer(output,se_order):\n",
    "#     output = output.replace(\"<pad>\",'')\n",
    "#     output = output.replace(\"</s>\",'')\n",
    "#     pattern = r\"\"\n",
    "#     for se in se_order:\n",
    "#         if se != 's':\n",
    "#             pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
    "#         else:\n",
    "#             pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
    "#     found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "#     for i in range(len(found)):\n",
    "#         for k, v in found[i].items():\n",
    "#             found[i][k] = found[i][k].strip()\n",
    "#     return found\n",
    "def catch_answer(output,se_order):\n",
    "    # output = output.replace(\"<pad>\",'')\n",
    "    # output = output.replace(\"</s>\",'')\n",
    "    # output = output.split(\"<|sep|>\")[-1]\n",
    "    pattern = []\n",
    "    for se in se_order:\n",
    "        if se != 's':\n",
    "            pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\")\n",
    "        else:\n",
    "            pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\")\n",
    "    pattern = ','.join(pattern)\n",
    "    pattern = f\"\\({pattern}\\)\"\n",
    "    found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "    for i in range(len(found)):\n",
    "        for k, v in found[i].items():\n",
    "            found[i][k] = found[i][k].strip()\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'opinion': 'no', 'aspect': 'GUI', 'sentiment': 'negative'},\n",
       " {'opinion': 'dark', 'aspect': 'screen', 'sentiment': 'negative'},\n",
       " {'opinion': 'steady', 'aspect': 'power light', 'sentiment': 'neutral'},\n",
       " {'opinion': 'steady', 'aspect': 'hard drive light', 'sentiment': 'negative'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = construct_answer(peng_intermediate[\"lap14\"][\"oas\"][\"train\"][4][\"target\"],\"oas\")\n",
    "se_order = \"oas\"\n",
    "catch_answer(output,se_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( no , GUI , negative ) ; ( dark , screen , negative ) ; ( steady , power light , neutral ) ; ( steady , hard drive light , negative )'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "peng_2 = dict()\n",
    "for domain, v1 in peng_intermediate.items():\n",
    "    peng_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in peng_intermediate[domain][basic_task][\"train\"]:\n",
    "            peng_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        peng_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in peng_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        peng_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    peng_2[domain][\"train\"] = Dataset.from_list(peng_2[domain][\"train\"])\n",
    "    peng_2[domain][\"val\"] = Dataset.from_list(peng_2[domain][\"val\"])\n",
    "    peng_2[domain][\"test\"] = Dataset.from_list(peng_2[domain][\"test\"])\n",
    "\n",
    "wan_2 = dict()\n",
    "for domain, v1 in wan_intermediate.items():\n",
    "    wan_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"asc\"]:\n",
    "        for el in wan_intermediate[domain][basic_task][\"train\"]:\n",
    "            wan_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"val\"]:\n",
    "        wan_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in wan_intermediate[domain][\"asc\"][\"test\"]:\n",
    "        wan_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"asc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"asc\"),\n",
    "                \"task\" : \"asc\"\n",
    "            })\n",
    "    wan_2[domain][\"train\"] = Dataset.from_list(wan_2[domain][\"train\"])\n",
    "    wan_2[domain][\"val\"] = Dataset.from_list(wan_2[domain][\"val\"])\n",
    "    wan_2[domain][\"test\"] = Dataset.from_list(wan_2[domain][\"test\"])\n",
    "\n",
    "zhang_2 = dict()\n",
    "for domain, v1 in zhang_intermediate.items():\n",
    "    zhang_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oasc\"]:\n",
    "        for el in zhang_intermediate[domain][basic_task][\"train\"]:\n",
    "            zhang_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"val\"]:\n",
    "        zhang_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in zhang_intermediate[domain][\"oasc\"][\"test\"]:\n",
    "        zhang_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oasc\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oasc\"),\n",
    "                \"task\" : \"oasc\"\n",
    "            })\n",
    "    zhang_2[domain][\"train\"] = Dataset.from_list(zhang_2[domain][\"train\"])\n",
    "    zhang_2[domain][\"val\"] = Dataset.from_list(zhang_2[domain][\"val\"])\n",
    "    zhang_2[domain][\"test\"] = Dataset.from_list(zhang_2[domain][\"test\"])\n",
    "\n",
    "william_2 = dict()\n",
    "for domain, v1 in william_intermediate.items():\n",
    "    william_2[domain] = {\n",
    "        \"train\" : [], # basic task\n",
    "        \"val\" : [], # complex task\n",
    "        \"test\" : [] # complex task\n",
    "    }\n",
    "    # TRAIN\n",
    "    for basic_task in task_tree[\"oas\"]:\n",
    "        for el in william_intermediate[domain][basic_task][\"train\"]:\n",
    "            william_2[domain][\"train\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                    \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                    \"task\" : basic_task\n",
    "                })\n",
    "    # VAL\n",
    "    for el in william_intermediate[domain][\"oas\"][\"val\"]:\n",
    "        william_2[domain][\"val\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    # TEST\n",
    "    for el in william_intermediate[domain][\"oas\"][\"test\"]:\n",
    "        william_2[domain][\"test\"].append({\n",
    "                \"input\" : construct_prompt(el[\"text\"],\"oas\"),\n",
    "                \"output\" : construct_answer(el[\"target\"],\"oas\"),\n",
    "                \"task\" : \"oas\"\n",
    "            })\n",
    "    william_2[domain][\"train\"] = Dataset.from_list(william_2[domain][\"train\"])\n",
    "    william_2[domain][\"val\"] = Dataset.from_list(william_2[domain][\"val\"])\n",
    "    william_2[domain][\"test\"] = Dataset.from_list(william_2[domain][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tempat yag bagus dan nyaman untuk istirahat tetapi tolong tvnya perlu di perbaiki channelnya karena banyak semutnya digambar dan water heaternya tidak bisa jadi mandi air dingin terus . | ( opinion , aspect , sentiment )',\n",
       " 'output': '( bagus , tempat , positive ) ; ( nyaman , tempat , positive ) ; ( perlu di perbaiki , tvnya , positive ) ; ( tidak bisa , water heaternya , negative )',\n",
       " 'task': 'oas'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "william_2[\"hotel\"][\"train\"][69]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Tokenized Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained(\"gpt2-large\", padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_args = {\n",
    "    \"max_length\" : 128,\n",
    "    \"padding\" : True,\n",
    "    \"truncation\" : True,\n",
    "    # \"return_tensors\" : \"pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "resize_en = False\n",
    "if tokenizer_en.pad_token == None:\n",
    "    pad_token = \"<|pad|>\"\n",
    "    tokenizer_en.add_tokens([pad_token])\n",
    "    tokenizer_en.add_special_tokens({\"pad_token\": pad_token})\n",
    "    resize_en = True\n",
    "\n",
    "if tokenizer_en.sep_token == None:\n",
    "    sep_token = \"<|sep|>\"\n",
    "    tokenizer_en.add_tokens([sep_token])\n",
    "    tokenizer_en.add_special_tokens({\"sep_token\": sep_token})\n",
    "    resize_en = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_en(dataset):\n",
    "    causal_lm_input =[row_input + ' ' + tokenizer_en.sep_token + ' ' + row_output + ' ' + tokenizer_en.eos_token\n",
    "                      for row_input, row_output in zip(dataset[\"input\"],dataset[\"output\"])]\n",
    "    result = tokenizer_en(causal_lm_input, **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "peng_tok = dict()\n",
    "for domain, v1 in peng_2.items():\n",
    "    peng_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            peng_tok[domain][split] = peng_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            test_input = [row_input + ' ' + tokenizer_en.sep_token for row_input in peng_2[domain][split][\"input\"]]\n",
    "            peng_tok[domain][split] = tokenizer_en(test_input, max_length=86, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "wan_tok = dict()\n",
    "for domain, v1 in wan_2.items():\n",
    "    wan_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            wan_tok[domain][split] = wan_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            test_input = [row_input + ' ' + tokenizer_en.sep_token for row_input in wan_2[domain][split][\"input\"]]\n",
    "            wan_tok[domain][split] = tokenizer_en(test_input, max_length=86, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "zhang_tok = dict()\n",
    "for domain, v1 in zhang_2.items():\n",
    "    zhang_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            zhang_tok[domain][split] = zhang_2[domain][split].map(encode_en,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            test_input = [row_input + ' ' + tokenizer_en.sep_token for row_input in zhang_2[domain][split][\"input\"]]\n",
    "            zhang_tok[domain][split] = tokenizer_en(test_input, max_length=86, padding=True, truncation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\", padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_id = False\n",
    "if tokenizer_id.pad_token == None:\n",
    "    pad_token = \"<|pad|>\"\n",
    "    tokenizer_id.add_tokens([pad_token])\n",
    "    tokenizer_id.add_special_tokens({\"pad_token\": pad_token})\n",
    "    resize_id = True\n",
    "\n",
    "if tokenizer_id.sep_token == None:\n",
    "    sep_token = \"<|sep|>\"\n",
    "    tokenizer_id.add_tokens([sep_token])\n",
    "    tokenizer_id.add_special_tokens({\"sep_token\": sep_token})\n",
    "    resize_id = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_id(dataset):\n",
    "    causal_lm_input =[row_input + ' ' + tokenizer_id.sep_token + ' ' + row_output + ' ' + tokenizer_id.eos_token\n",
    "                      for row_input, row_output in zip(dataset[\"input\"],dataset[\"output\"])]\n",
    "    result = tokenizer_id(causal_lm_input, **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    }
   ],
   "source": [
    "william_tok = dict()\n",
    "for domain, v1 in william_2.items():\n",
    "    william_tok[domain] = dict()\n",
    "    for split, v2 in v1.items():\n",
    "        if split != \"test\":\n",
    "            william_tok[domain][split] = william_2[domain][split].map(encode_id,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "        else:\n",
    "            test_input = [row_input + ' ' + tokenizer_id.sep_token for row_input in william_2[domain][split][\"input\"]]\n",
    "            william_tok[domain][split] = tokenizer_id(test_input, max_length=86, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator_en = DataCollatorForLanguageModeling(tokenizer=tokenizer_en,mlm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_id = DataCollatorForLanguageModeling(tokenizer=tokenizer_id,mlm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from evaluation import recall, precision, f1_score, summary_score\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def seperate_target_prediction_per_task(predictions:List[List[Dict]],targets:List[List[Dict]],tasks:List) -> Tuple[Dict[str,List],Dict[str,List]]:\n",
    "    per_task_targets = {}\n",
    "    per_task_predictions = {}\n",
    "    for target, prediction, task in zip(targets,predictions,tasks):\n",
    "        if task not in per_task_targets.keys():\n",
    "            per_task_targets[task] = []\n",
    "        if task not in per_task_predictions.keys():\n",
    "            per_task_predictions[task] = []\n",
    "        per_task_targets[task].append(target)\n",
    "        per_task_predictions[task].append(prediction)\n",
    "    return per_task_targets, per_task_predictions\n",
    "\n",
    "def preprocess_eval_preds(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer):\n",
    "    input_ids = eval_preds.inputs\n",
    "    target_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(input_ids, tuple):\n",
    "        input_ids = input_ids[0]\n",
    "    if isinstance(target_ids, tuple):\n",
    "        target_ids = target_ids[0]\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    input_ids = np.argmax(input_ids,axis=-1) if len(input_ids.shape) == 3 else input_ids # in case not predict with generate\n",
    "    target_ids = np.argmax(target_ids,axis=-1) if len(target_ids.shape) == 3 else target_ids # in case not predict with generate\n",
    "    prediction_ids = np.argmax(pred_ids,axis=-1) if len(pred_ids.shape) == 3 else pred_ids # in case not predict with generate\n",
    "\n",
    "    input_ids = [[token for token in row if token != -100] for row in input_ids]\n",
    "    target_ids = [[token for token in row if token != -100] for row in target_ids]\n",
    "    prediction_ids = [[token for token in row if token != -100] for row in prediction_ids]\n",
    "\n",
    "    inputs = tokenizer.batch_decode(input_ids,**decoding_args)\n",
    "    targets = tokenizer.batch_decode(target_ids,**decoding_args)\n",
    "    predictions = tokenizer.batch_decode(prediction_ids,**decoding_args)\n",
    "\n",
    "    return inputs, targets, predictions\n",
    "\n",
    "def compute_metrics(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer,tasks:List) -> Dict[str,float]: # MAY NOT BE SUFFICIATE FOR CAUSAL LM\n",
    "        \"\"\"\n",
    "        ### DESC\n",
    "            Method to compute the metrics.\n",
    "        ### PARAMS\n",
    "        * eval_preds: EvalPrediction instance from training.\n",
    "        * decoding_args: Decoding arguments.\n",
    "        ### RETURN\n",
    "        * metrics: Dictionary of metrics.\n",
    "        \"\"\"\n",
    "        inputs, targets, predictions = preprocess_eval_preds(eval_preds,decoding_args,tokenizer)\n",
    "\n",
    "        print(\">> INPUTS :\",inputs[0])\n",
    "        print(\">> TARGETS :\",targets[0])\n",
    "        print(\">> PREDS :\", predictions[0])\n",
    "\n",
    "        targets = [catch_answer(text,task) for text,task in zip(targets,tasks) if task != \"non_absa\"]\n",
    "        predictions = [catch_answer(text,task) for text,task in zip(predictions,tasks) if task != \"non_absa\"]\n",
    "\n",
    "\n",
    "        per_task_targets, per_task_predictions = seperate_target_prediction_per_task(predictions, targets, tasks)\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        metrics[\"overall_recall\"] = recall(predictions,targets)\n",
    "        metrics[\"overall_precision\"] = precision(predictions,targets)\n",
    "        metrics[\"overall_f1_score\"] = f1_score(predictions,targets)\n",
    "\n",
    "        for task in per_task_targets.keys():\n",
    "            if task == \"non_absa\":\n",
    "                continue\n",
    "            metrics[f\"{task}_recall\"] = recall(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_precision\"] = precision(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_f1_score\"] = f1_score(per_task_predictions[task],per_task_targets[task])\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"per_device_train_batch_size\": 16//n_gpu,\n",
    "    \"per_device_eval_batch_size\": 16//n_gpu,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_strategy\" : \"epoch\",\n",
    "    # \"metric_for_best_model\": \"overall_f1_score\",\n",
    "    # \"load_best_model_at_end\": True,\n",
    "    \"adam_epsilon\": 1e-08,\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"logging_dir\" : \"./output/log\",\n",
    "    \"include_inputs_for_metrics\" : True\n",
    "}\n",
    "\n",
    "train_args = TrainingArguments(**train_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# trainer = {\n",
    "#     \"peng\" : {},\n",
    "#     \"wan\" : {},\n",
    "#     \"zhang\" : {},\n",
    "#     \"william\" : {}\n",
    "# }\n",
    "\n",
    "decoding_args = {\n",
    "    \"skip_special_tokens\" : False\n",
    "}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, targets):\n",
    "    pred_logits = logits[0] if isinstance(logits,tuple) else logits\n",
    "    pred_ids = torch.argmax(pred_logits, dim=-1)\n",
    "    return pred_ids, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_predictions(model,tokenizer,data,device=torch.device(\"cuda:0\"),decoding_args:Dict={}) -> List[str]:\n",
    "    # Data loader\n",
    "    # input_ids_data_loader = torch.utils.data.DataLoader(tokenized[\"input_ids\"],\n",
    "    #                     batch_size=batch_size,shuffle=False)\n",
    "    # attention_mask_data_loader = torch.utils.data.DataLoader(tokenized[\"attention_mask\"],\n",
    "    #                     batch_size=batch_size,shuffle=False)\n",
    "    # Predict\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    tensor_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(data):\n",
    "            # input_ids = input_ids.to(device)\n",
    "            # attention_mask = attention_mask.to(device)\n",
    "            input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "            tensor_predictions.extend(model.generate(input_ids=input_ids, pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id,max_length=128).cpu())\n",
    "            input_ids = input_ids.cpu()\n",
    "            # attention_mask = attention_mask.cpu()\n",
    "    tensor_predictions = [[token for token in row if token != -100] for row in tensor_predictions]\n",
    "    predictions = tokenizer.batch_decode(tensor_predictions,**decoding_args)\n",
    "    predictions = [el for el in predictions]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_result(str_preds_,preds,targets,filename):\n",
    "    result = []\n",
    "    str_preds = [el.replace(\"<|pad|>\",'').split(\"<|sep|>\")[-1] for el in str_preds_]\n",
    "    assert len(str_preds) == len(preds) == len(targets)\n",
    "    for i in range(len(str_preds)):\n",
    "        result.append({\n",
    "            \"str_pred\" : str_preds[i],\n",
    "            \"pred\" : preds[i],\n",
    "            \"target\" : targets[i]\n",
    "        })\n",
    "    \n",
    "    with open(filename,'w') as fp:\n",
    "        json.dump(result,fp)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Laptop 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4530\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1420\n",
      "  Number of trainable parameters = 774032640\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1420' max='1420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1420/1420 44:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.059400</td>\n",
       "      <td>2.435000</td>\n",
       "      <td>0.296512</td>\n",
       "      <td>0.361404</td>\n",
       "      <td>0.325757</td>\n",
       "      <td>0.296512</td>\n",
       "      <td>0.361404</td>\n",
       "      <td>0.325757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>2.560602</td>\n",
       "      <td>0.322674</td>\n",
       "      <td>0.413919</td>\n",
       "      <td>0.362645</td>\n",
       "      <td>0.322674</td>\n",
       "      <td>0.413919</td>\n",
       "      <td>0.362645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>2.688858</td>\n",
       "      <td>0.360465</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.387365</td>\n",
       "      <td>0.360465</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.387365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>2.851537</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.378549</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.378549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>2.925708</td>\n",
       "      <td>0.340116</td>\n",
       "      <td>0.398649</td>\n",
       "      <td>0.367064</td>\n",
       "      <td>0.340116</td>\n",
       "      <td>0.398649</td>\n",
       "      <td>0.367064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>2.935990</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.418118</td>\n",
       "      <td>0.380349</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.418118</td>\n",
       "      <td>0.380349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>2.963400</td>\n",
       "      <td>0.363372</td>\n",
       "      <td>0.430034</td>\n",
       "      <td>0.393903</td>\n",
       "      <td>0.363372</td>\n",
       "      <td>0.430034</td>\n",
       "      <td>0.393903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>3.018411</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.438776</td>\n",
       "      <td>0.402692</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.438776</td>\n",
       "      <td>0.402692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>3.039836</td>\n",
       "      <td>0.360465</td>\n",
       "      <td>0.428082</td>\n",
       "      <td>0.391375</td>\n",
       "      <td>0.360465</td>\n",
       "      <td>0.428082</td>\n",
       "      <td>0.391375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>3.064008</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.440273</td>\n",
       "      <td>0.403322</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.440273</td>\n",
       "      <td>0.403322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-142\n",
      "Configuration saved in ./output/checkpoint-142/config.json\n",
      "Configuration saved in ./output/checkpoint-142/config.json\n",
      "Model weights saved in ./output/checkpoint-142/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-142/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-142/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-284\n",
      "Configuration saved in ./output/checkpoint-284/config.json\n",
      "Configuration saved in ./output/checkpoint-284/config.json\n",
      "Model weights saved in ./output/checkpoint-284/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-284/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-284/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-426\n",
      "Configuration saved in ./output/checkpoint-426/config.json\n",
      "Configuration saved in ./output/checkpoint-426/config.json\n",
      "Model weights saved in ./output/checkpoint-426/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-426/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-426/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-142] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-568\n",
      "Configuration saved in ./output/checkpoint-568/config.json\n",
      "Configuration saved in ./output/checkpoint-568/config.json\n",
      "Model weights saved in ./output/checkpoint-568/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-568/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-568/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-710\n",
      "Configuration saved in ./output/checkpoint-710/config.json\n",
      "Configuration saved in ./output/checkpoint-710/config.json\n",
      "Model weights saved in ./output/checkpoint-710/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-710/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-710/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-426] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-852\n",
      "Configuration saved in ./output/checkpoint-852/config.json\n",
      "Configuration saved in ./output/checkpoint-852/config.json\n",
      "Model weights saved in ./output/checkpoint-852/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-852/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-852/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-568] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-994\n",
      "Configuration saved in ./output/checkpoint-994/config.json\n",
      "Configuration saved in ./output/checkpoint-994/config.json\n",
      "Model weights saved in ./output/checkpoint-994/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-994/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-994/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-710] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1136\n",
      "Configuration saved in ./output/checkpoint-1136/config.json\n",
      "Configuration saved in ./output/checkpoint-1136/config.json\n",
      "Model weights saved in ./output/checkpoint-1136/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1136/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1136/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-852] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1278\n",
      "Configuration saved in ./output/checkpoint-1278/config.json\n",
      "Configuration saved in ./output/checkpoint-1278/config.json\n",
      "Model weights saved in ./output/checkpoint-1278/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1278/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1278/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-994] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1420\n",
      "Configuration saved in ./output/checkpoint-1420/config.json\n",
      "Configuration saved in ./output/checkpoint-1420/config.json\n",
      "Model weights saved in ./output/checkpoint-1420/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1420/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1420/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1136] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1420, training_loss=0.23611598887913662, metrics={'train_runtime': 2646.867, 'train_samples_per_second': 17.115, 'train_steps_per_second': 0.536, 'total_flos': 2.46447943406592e+16, 'train_loss': 0.23611598887913662, 'epoch': 10.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"lap14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"lap14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"lap14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boot time is super fast <comma> around anywhere from 35 seconds to 1 minute . | ( opinion , aspect , sentiment ) <|sep|>'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el + ' ' + tokenizer_en.sep_token for el in peng_2[\"lap14\"][\"test\"][\"input\"]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:34<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_en.sep_token for el in peng_2[\"lap14\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_en, test_data, device, decoding_args)\n",
    "str_preds = [el.split(tokenizer_en.sep_token)[-1] for el in str_preds]\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"lap14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.38817005545286504,\n",
       " 'precision': 0.44421487603305787,\n",
       " 'f1_score': 0.4143057053061412}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_lap14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6330\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1980\n",
      "  Number of trainable parameters = 774032640\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='199' max='1980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 199/1980 05:47 < 52:22, 0.57 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/20 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-198\n",
      "Configuration saved in ./output/checkpoint-198/config.json\n",
      "Configuration saved in ./output/checkpoint-198/config.json\n",
      "Model weights saved in ./output/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-198/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-396\n",
      "Configuration saved in ./output/checkpoint-396/config.json\n",
      "Configuration saved in ./output/checkpoint-396/config.json\n",
      "Model weights saved in ./output/checkpoint-396/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-396/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-396/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-594\n",
      "Configuration saved in ./output/checkpoint-594/config.json\n",
      "Configuration saved in ./output/checkpoint-594/config.json\n",
      "Model weights saved in ./output/checkpoint-594/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-594/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-594/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-198] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-792\n",
      "Configuration saved in ./output/checkpoint-792/config.json\n",
      "Configuration saved in ./output/checkpoint-792/config.json\n",
      "Model weights saved in ./output/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-792/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-396] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-990\n",
      "Configuration saved in ./output/checkpoint-990/config.json\n",
      "Configuration saved in ./output/checkpoint-990/config.json\n",
      "Model weights saved in ./output/checkpoint-990/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-990/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-990/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-594] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1188\n",
      "Configuration saved in ./output/checkpoint-1188/config.json\n",
      "Configuration saved in ./output/checkpoint-1188/config.json\n",
      "Model weights saved in ./output/checkpoint-1188/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1188/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1188/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-792] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1386\n",
      "Configuration saved in ./output/checkpoint-1386/config.json\n",
      "Configuration saved in ./output/checkpoint-1386/config.json\n",
      "Model weights saved in ./output/checkpoint-1386/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1386/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1386/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-990] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1584\n",
      "Configuration saved in ./output/checkpoint-1584/config.json\n",
      "Configuration saved in ./output/checkpoint-1584/config.json\n",
      "Model weights saved in ./output/checkpoint-1584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1188] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1782\n",
      "Configuration saved in ./output/checkpoint-1782/config.json\n",
      "Configuration saved in ./output/checkpoint-1782/config.json\n",
      "Model weights saved in ./output/checkpoint-1782/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1782/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1782/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1386] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 310\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1980\n",
      "Configuration saved in ./output/checkpoint-1980/config.json\n",
      "Configuration saved in ./output/checkpoint-1980/config.json\n",
      "Model weights saved in ./output/checkpoint-1980/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1980/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1980/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1584] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1980, training_loss=0.23776443173186948, metrics={'train_runtime': 3658.1665, 'train_samples_per_second': 17.304, 'train_steps_per_second': 0.541, 'total_flos': 3.443797426176e+16, 'train_loss': 0.23776443173186948, 'epoch': 10.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res14\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res14\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res14\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 492/492 [04:49<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_en.sep_token for el in peng_2[\"res14\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_en, test_data, device, decoding_args)\n",
    "str_preds = [el.split(tokenizer_en.sep_token)[-1] for el in str_preds]\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res14\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5392354124748491,\n",
       " 'precision': 0.5795698924731183,\n",
       " 'f1_score': 0.558675595554636}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res14.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3025\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 950\n",
      "  Number of trainable parameters = 774032640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 96/950 02:45 < 25:00, 0.57 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/10 00:01 < 00:00, 3.66 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-95\n",
      "Configuration saved in ./output/checkpoint-95/config.json\n",
      "Configuration saved in ./output/checkpoint-95/config.json\n",
      "Model weights saved in ./output/checkpoint-95/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-95/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-95/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-190\n",
      "Configuration saved in ./output/checkpoint-190/config.json\n",
      "Configuration saved in ./output/checkpoint-190/config.json\n",
      "Model weights saved in ./output/checkpoint-190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-190/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-285\n",
      "Configuration saved in ./output/checkpoint-285/config.json\n",
      "Configuration saved in ./output/checkpoint-285/config.json\n",
      "Model weights saved in ./output/checkpoint-285/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-285/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-285/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-95] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-380\n",
      "Configuration saved in ./output/checkpoint-380/config.json\n",
      "Configuration saved in ./output/checkpoint-380/config.json\n",
      "Model weights saved in ./output/checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-475\n",
      "Configuration saved in ./output/checkpoint-475/config.json\n",
      "Configuration saved in ./output/checkpoint-475/config.json\n",
      "Model weights saved in ./output/checkpoint-475/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-475/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-475/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-285] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-570\n",
      "Configuration saved in ./output/checkpoint-570/config.json\n",
      "Configuration saved in ./output/checkpoint-570/config.json\n",
      "Model weights saved in ./output/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-570/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-665\n",
      "Configuration saved in ./output/checkpoint-665/config.json\n",
      "Configuration saved in ./output/checkpoint-665/config.json\n",
      "Model weights saved in ./output/checkpoint-665/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-665/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-665/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-475] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-760\n",
      "Configuration saved in ./output/checkpoint-760/config.json\n",
      "Configuration saved in ./output/checkpoint-760/config.json\n",
      "Model weights saved in ./output/checkpoint-760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-570] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-855\n",
      "Configuration saved in ./output/checkpoint-855/config.json\n",
      "Configuration saved in ./output/checkpoint-855/config.json\n",
      "Model weights saved in ./output/checkpoint-855/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-855/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-855/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-665] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 148\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-950\n",
      "Configuration saved in ./output/checkpoint-950/config.json\n",
      "Configuration saved in ./output/checkpoint-950/config.json\n",
      "Model weights saved in ./output/checkpoint-950/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-760] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=950, training_loss=0.24644181100945722, metrics={'train_runtime': 1804.9995, 'train_samples_per_second': 16.759, 'train_steps_per_second': 0.526, 'total_flos': 1.64573257728e+16, 'train_loss': 0.24644181100945722, 'epoch': 10.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [02:47<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_en.sep_token for el in peng_2[\"res15\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_en, test_data, device, decoding_args)\n",
    "str_preds = [el.split(tokenizer_en.sep_token)[-1] for el in str_preds]\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.4556701030927835,\n",
       " 'precision': 0.4324853228962818,\n",
       " 'f1_score': 0.4437751004016064}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peng Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4285\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1340\n",
      "  Number of trainable parameters = 774032640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='1340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 135/1340 03:54 < 35:25, 0.57 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/14 00:02 < 00:00, 4.00 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-134\n",
      "Configuration saved in ./output/checkpoint-134/config.json\n",
      "Configuration saved in ./output/checkpoint-134/config.json\n",
      "Model weights saved in ./output/checkpoint-134/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-134/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-134/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-268\n",
      "Configuration saved in ./output/checkpoint-268/config.json\n",
      "Configuration saved in ./output/checkpoint-268/config.json\n",
      "Model weights saved in ./output/checkpoint-268/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-268/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-268/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-402\n",
      "Configuration saved in ./output/checkpoint-402/config.json\n",
      "Configuration saved in ./output/checkpoint-402/config.json\n",
      "Model weights saved in ./output/checkpoint-402/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-402/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-402/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-134] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-536\n",
      "Configuration saved in ./output/checkpoint-536/config.json\n",
      "Configuration saved in ./output/checkpoint-536/config.json\n",
      "Model weights saved in ./output/checkpoint-536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-536/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-670\n",
      "Configuration saved in ./output/checkpoint-670/config.json\n",
      "Configuration saved in ./output/checkpoint-670/config.json\n",
      "Model weights saved in ./output/checkpoint-670/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-670/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-670/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-402] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-804\n",
      "Configuration saved in ./output/checkpoint-804/config.json\n",
      "Configuration saved in ./output/checkpoint-804/config.json\n",
      "Model weights saved in ./output/checkpoint-804/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-804/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-804/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-536] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-938\n",
      "Configuration saved in ./output/checkpoint-938/config.json\n",
      "Configuration saved in ./output/checkpoint-938/config.json\n",
      "Model weights saved in ./output/checkpoint-938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-938/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-670] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1072\n",
      "Configuration saved in ./output/checkpoint-1072/config.json\n",
      "Configuration saved in ./output/checkpoint-1072/config.json\n",
      "Model weights saved in ./output/checkpoint-1072/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1072/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1072/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-804] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1206\n",
      "Configuration saved in ./output/checkpoint-1206/config.json\n",
      "Configuration saved in ./output/checkpoint-1206/config.json\n",
      "Model weights saved in ./output/checkpoint-1206/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1206/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1206/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-938] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1340\n",
      "Configuration saved in ./output/checkpoint-1340/config.json\n",
      "Configuration saved in ./output/checkpoint-1340/config.json\n",
      "Model weights saved in ./output/checkpoint-1340/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1340/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1340/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1072] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1340, training_loss=0.24749831726301963, metrics={'train_runtime': 2506.1685, 'train_samples_per_second': 17.098, 'train_steps_per_second': 0.535, 'total_flos': 2.331227799552e+16, 'train_loss': 0.24749831726301963, 'epoch': 10.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = peng_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = peng_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,peng_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 169/326 [01:23<01:01,  2.54it/s]Input length of input_ids is 181, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "100%|██████████| 326/326 [02:43<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_en.sep_token for el in peng_2[\"res16\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_en, test_data, device, decoding_args)\n",
    "str_preds = [el.split(tokenizer_en.sep_token)[-1] for el in str_preds]\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in peng_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5603112840466926,\n",
       " 'precision': 0.5722772277227722,\n",
       " 'f1_score': 0.5662310450157491}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"peng_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5600\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1750\n",
      "  Number of trainable parameters = 774032640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 176/1750 05:07 < 46:20, 0.57 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-175\n",
      "Configuration saved in ./output/checkpoint-175/config.json\n",
      "Configuration saved in ./output/checkpoint-175/config.json\n",
      "Model weights saved in ./output/checkpoint-175/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-175/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-175/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-350\n",
      "Configuration saved in ./output/checkpoint-350/config.json\n",
      "Model weights saved in ./output/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-350/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-525\n",
      "Configuration saved in ./output/checkpoint-525/config.json\n",
      "Configuration saved in ./output/checkpoint-525/config.json\n",
      "Model weights saved in ./output/checkpoint-525/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-525/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-525/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-175] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-700\n",
      "Configuration saved in ./output/checkpoint-700/config.json\n",
      "Configuration saved in ./output/checkpoint-700/config.json\n",
      "Model weights saved in ./output/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-875\n",
      "Configuration saved in ./output/checkpoint-875/config.json\n",
      "Model weights saved in ./output/checkpoint-875/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-875/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-875/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-525] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1050\n",
      "Configuration saved in ./output/checkpoint-1050/config.json\n",
      "Configuration saved in ./output/checkpoint-1050/config.json\n",
      "Model weights saved in ./output/checkpoint-1050/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-700] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1225\n",
      "Configuration saved in ./output/checkpoint-1225/config.json\n",
      "Configuration saved in ./output/checkpoint-1225/config.json\n",
      "Model weights saved in ./output/checkpoint-1225/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1225/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1225/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1400\n",
      "Configuration saved in ./output/checkpoint-1400/config.json\n",
      "Model weights saved in ./output/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1050] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1575\n",
      "Configuration saved in ./output/checkpoint-1575/config.json\n",
      "Configuration saved in ./output/checkpoint-1575/config.json\n",
      "Model weights saved in ./output/checkpoint-1575/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1575/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1575/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1225] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1750\n",
      "Configuration saved in ./output/checkpoint-1750/config.json\n",
      "Configuration saved in ./output/checkpoint-1750/config.json\n",
      "Model weights saved in ./output/checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1400] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1750, training_loss=0.2484765657697405, metrics={'train_runtime': 3198.6644, 'train_samples_per_second': 17.507, 'train_steps_per_second': 0.547, 'total_flos': 3.04664543232e+16, 'train_loss': 0.2484765657697405, 'epoch': 10.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582/582 [04:42<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_en.sep_token for el in wan_2[\"res15\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_en, test_data, device, decoding_args)\n",
    "str_preds = [el.split(tokenizer_en.sep_token)[-1] for el in str_preds]\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.463905325443787,\n",
       " 'precision': 0.49936628643852976,\n",
       " 'f1_score': 0.4809830929684312}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8540\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2670\n",
      "  Number of trainable parameters = 774032640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='268' max='2670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 268/2670 07:49 < 1:10:41, 0.57 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/2 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-267\n",
      "Configuration saved in ./output/checkpoint-267/config.json\n",
      "Configuration saved in ./output/checkpoint-267/config.json\n",
      "Model weights saved in ./output/checkpoint-267/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-267/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-267/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-534\n",
      "Configuration saved in ./output/checkpoint-534/config.json\n",
      "Configuration saved in ./output/checkpoint-534/config.json\n",
      "Model weights saved in ./output/checkpoint-534/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-534/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-534/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-801\n",
      "Configuration saved in ./output/checkpoint-801/config.json\n",
      "Configuration saved in ./output/checkpoint-801/config.json\n",
      "Model weights saved in ./output/checkpoint-801/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-801/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-801/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-267] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1068\n",
      "Configuration saved in ./output/checkpoint-1068/config.json\n",
      "Configuration saved in ./output/checkpoint-1068/config.json\n",
      "Model weights saved in ./output/checkpoint-1068/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1068/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1068/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-534] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1335\n",
      "Configuration saved in ./output/checkpoint-1335/config.json\n",
      "Configuration saved in ./output/checkpoint-1335/config.json\n",
      "Model weights saved in ./output/checkpoint-1335/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1335/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1335/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-801] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1602\n",
      "Configuration saved in ./output/checkpoint-1602/config.json\n",
      "Configuration saved in ./output/checkpoint-1602/config.json\n",
      "Model weights saved in ./output/checkpoint-1602/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1602/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1602/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1068] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1869\n",
      "Configuration saved in ./output/checkpoint-1869/config.json\n",
      "Configuration saved in ./output/checkpoint-1869/config.json\n",
      "Model weights saved in ./output/checkpoint-1869/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1869/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1869/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1335] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-2136\n",
      "Configuration saved in ./output/checkpoint-2136/config.json\n",
      "Configuration saved in ./output/checkpoint-2136/config.json\n",
      "Model weights saved in ./output/checkpoint-2136/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2136/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2136/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1602] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-2403\n",
      "Configuration saved in ./output/checkpoint-2403/config.json\n",
      "Configuration saved in ./output/checkpoint-2403/config.json\n",
      "Model weights saved in ./output/checkpoint-2403/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2403/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2403/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1869] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 29\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-2670\n",
      "Configuration saved in ./output/checkpoint-2670/config.json\n",
      "Configuration saved in ./output/checkpoint-2670/config.json\n",
      "Model weights saved in ./output/checkpoint-2670/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2670/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2670/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2136] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2670, training_loss=0.25343111874012464, metrics={'train_runtime': 4829.9114, 'train_samples_per_second': 17.681, 'train_steps_per_second': 0.553, 'total_flos': 4.646134284288e+16, 'train_loss': 0.25343111874012464, 'epoch': 10.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = wan_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = wan_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,wan_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 294/587 [02:17<02:06,  2.32it/s]Input length of input_ids is 181, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "100%|██████████| 587/587 [04:32<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_en.sep_token for el in wan_2[\"res16\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_en, test_data, device, decoding_args)\n",
    "str_preds = [el.split(tokenizer_en.sep_token)[-1] for el in str_preds]\n",
    "preds = [catch_answer(el,\"asc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"asc\") for el in wan_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.5273573923166472,\n",
       " 'precision': 0.568039950062422,\n",
       " 'f1_score': 0.5469432053687286}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"wan_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5838\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1820\n",
      "  Number of trainable parameters = 774032640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='1820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 183/1820 05:19 < 48:12, 0.57 it/s, Epoch 1.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/14 00:03 < 00:00, 3.58 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-182\n",
      "Configuration saved in ./output/checkpoint-182/config.json\n",
      "Configuration saved in ./output/checkpoint-182/config.json\n",
      "Model weights saved in ./output/checkpoint-182/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-182/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-182/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-364\n",
      "Configuration saved in ./output/checkpoint-364/config.json\n",
      "Configuration saved in ./output/checkpoint-364/config.json\n",
      "Model weights saved in ./output/checkpoint-364/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-364/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-364/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-546\n",
      "Configuration saved in ./output/checkpoint-546/config.json\n",
      "Configuration saved in ./output/checkpoint-546/config.json\n",
      "Model weights saved in ./output/checkpoint-546/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-546/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-546/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-182] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-728\n",
      "Configuration saved in ./output/checkpoint-728/config.json\n",
      "Configuration saved in ./output/checkpoint-728/config.json\n",
      "Model weights saved in ./output/checkpoint-728/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-728/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-728/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-364] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-910\n",
      "Configuration saved in ./output/checkpoint-910/config.json\n",
      "Configuration saved in ./output/checkpoint-910/config.json\n",
      "Model weights saved in ./output/checkpoint-910/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-910/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-910/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-546] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1092\n",
      "Configuration saved in ./output/checkpoint-1092/config.json\n",
      "Configuration saved in ./output/checkpoint-1092/config.json\n",
      "Model weights saved in ./output/checkpoint-1092/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1092/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1092/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-728] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1274\n",
      "Configuration saved in ./output/checkpoint-1274/config.json\n",
      "Configuration saved in ./output/checkpoint-1274/config.json\n",
      "Model weights saved in ./output/checkpoint-1274/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1274/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1274/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-910] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1456\n",
      "Configuration saved in ./output/checkpoint-1456/config.json\n",
      "Configuration saved in ./output/checkpoint-1456/config.json\n",
      "Model weights saved in ./output/checkpoint-1456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1456/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1092] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1638\n",
      "Configuration saved in ./output/checkpoint-1638/config.json\n",
      "Configuration saved in ./output/checkpoint-1638/config.json\n",
      "Model weights saved in ./output/checkpoint-1638/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1638/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1638/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1274] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 209\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1820\n",
      "Configuration saved in ./output/checkpoint-1820/config.json\n",
      "Configuration saved in ./output/checkpoint-1820/config.json\n",
      "Model weights saved in ./output/checkpoint-1820/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1820/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1820/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1456] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1820, training_loss=0.23226662206125784, metrics={'train_runtime': 3372.8296, 'train_samples_per_second': 17.309, 'train_steps_per_second': 0.54, 'total_flos': 3.17536620183552e+16, 'train_loss': 0.23226662206125784, 'epoch': 10.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res15\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res15\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res15\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [05:46<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_en.sep_token for el in zhang_2[\"res15\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_en, test_data, device, decoding_args)\n",
    "str_preds = [el.split(tokenizer_en.sep_token)[-1] for el in str_preds]\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res15\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.3270440251572327,\n",
       " 'precision': 0.33207070707070707,\n",
       " 'f1_score': 0.3295381983349598}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res15.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhang Restaurant 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--gpt2-large/snapshots/212095d5832abbf9926672e1c1e8d14312a3be20/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8848\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2760\n",
      "  Number of trainable parameters = 774032640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1223' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1223/2760 37:09 < 46:46, 0.55 it/s, Epoch 4.43/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oasc Recall</th>\n",
       "      <th>Oasc Precision</th>\n",
       "      <th>Oasc F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.809900</td>\n",
       "      <td>1.841892</td>\n",
       "      <td>0.353175</td>\n",
       "      <td>0.410835</td>\n",
       "      <td>0.379829</td>\n",
       "      <td>0.353175</td>\n",
       "      <td>0.410835</td>\n",
       "      <td>0.379829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>2.062810</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.346667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>2.154689</td>\n",
       "      <td>0.335317</td>\n",
       "      <td>0.381166</td>\n",
       "      <td>0.356775</td>\n",
       "      <td>0.335317</td>\n",
       "      <td>0.381166</td>\n",
       "      <td>0.356775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175900</td>\n",
       "      <td>2.152942</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.387540</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.387540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-276\n",
      "Configuration saved in ./output/checkpoint-276/config.json\n",
      "Configuration saved in ./output/checkpoint-276/config.json\n",
      "Model weights saved in ./output/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-276/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-552\n",
      "Configuration saved in ./output/checkpoint-552/config.json\n",
      "Configuration saved in ./output/checkpoint-552/config.json\n",
      "Model weights saved in ./output/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-552/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-828\n",
      "Configuration saved in ./output/checkpoint-828/config.json\n",
      "Configuration saved in ./output/checkpoint-828/config.json\n",
      "Model weights saved in ./output/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-276] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1104\n",
      "Configuration saved in ./output/checkpoint-1104/config.json\n",
      "Configuration saved in ./output/checkpoint-1104/config.json\n",
      "Model weights saved in ./output/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-552] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1380\n",
      "Configuration saved in ./output/checkpoint-1380/config.json\n",
      "Configuration saved in ./output/checkpoint-1380/config.json\n",
      "Model weights saved in ./output/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-828] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1656\n",
      "Configuration saved in ./output/checkpoint-1656/config.json\n",
      "Configuration saved in ./output/checkpoint-1656/config.json\n",
      "Model weights saved in ./output/checkpoint-1656/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1656/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1656/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1932\n",
      "Configuration saved in ./output/checkpoint-1932/config.json\n",
      "Configuration saved in ./output/checkpoint-1932/config.json\n",
      "Model weights saved in ./output/checkpoint-1932/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1932/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1932/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-2208\n",
      "Configuration saved in ./output/checkpoint-2208/config.json\n",
      "Configuration saved in ./output/checkpoint-2208/config.json\n",
      "Model weights saved in ./output/checkpoint-2208/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2208/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2208/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1656] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-2484\n",
      "Configuration saved in ./output/checkpoint-2484/config.json\n",
      "Configuration saved in ./output/checkpoint-2484/config.json\n",
      "Model weights saved in ./output/checkpoint-2484/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2484/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2484/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1932] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 316\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-2760\n",
      "Configuration saved in ./output/checkpoint-2760/config.json\n",
      "Configuration saved in ./output/checkpoint-2760/config.json\n",
      "Model weights saved in ./output/checkpoint-2760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2208] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2760, training_loss=0.23229833409406137, metrics={'train_runtime': 5057.8658, 'train_samples_per_second': 17.494, 'train_steps_per_second': 0.546, 'total_flos': 4.81282931294208e+16, 'train_loss': 0.23229833409406137, 'epoch': 10.0})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "if resize_en:\n",
    "    model.resize_token_embeddings(len(tokenizer_en))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_en,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = zhang_tok[\"res16\"][\"train\"],\n",
    "        eval_dataset = zhang_tok[\"res16\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_en,zhang_2[\"res16\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 474/544 [05:09<00:48,  1.46it/s]Input length of input_ids is 183, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "100%|██████████| 544/544 [05:55<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_en.sep_token for el in zhang_2[\"res16\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_en, test_data, device, decoding_args)\n",
    "str_preds = [el.split(tokenizer_en.sep_token)[-1] for el in str_preds]\n",
    "preds = [catch_answer(el,\"oasc\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oasc\") for el in zhang_2[\"res16\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.3967459324155194,\n",
       " 'precision': 0.3878048780487805,\n",
       " 'f1_score': 0.39222445731894745}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = save_result(str_preds, preds, targets, \"zhang_res16.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# William Hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--xglm-564M/snapshots/f3059f01b98ccc877c673149e0178c0e957660f9/config.json\n",
      "Model config XGLMConfig {\n",
      "  \"_name_or_path\": \"facebook/xglm-564M\",\n",
      "  \"activation_dropout\": 0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"XGLMForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_heads\": 16,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"xglm\",\n",
      "  \"num_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256008\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--facebook--xglm-564M/snapshots/f3059f01b98ccc877c673149e0178c0e957660f9/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XGLMForCausalLM.\n",
      "\n",
      "All the weights of XGLMForCausalLM were initialized from the model checkpoint at facebook/xglm-564M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XGLMForCausalLM for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 4690\n",
      "  Number of trainable parameters = 564463616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='4690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 939/4690 20:36 < 1:22:30, 0.76 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall F1 Score</th>\n",
       "      <th>Oas Recall</th>\n",
       "      <th>Oas Precision</th>\n",
       "      <th>Oas F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>0.662201</td>\n",
       "      <td>0.699697</td>\n",
       "      <td>0.721778</td>\n",
       "      <td>0.710566</td>\n",
       "      <td>0.699697</td>\n",
       "      <td>0.721778</td>\n",
       "      <td>0.710566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/63 00:05 < 00:06, 5.00 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> INPUTS : </s> pelayanan sudah bagus <comma> tempat juga bagus <comma> akan lebih bagus lagi jika tempat untuk mencharger hp di tambah. terimakasih. | ( opinion, aspect, sentiment ) </s> ( bagus, pelayanan, positive ) ; ( bagus, tempat, positive ) </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      ">> TARGETS : </s> pelayanan sudah bagus <comma> tempat juga bagus <comma> akan lebih bagus lagi jika tempat untuk mencharger hp di tambah. terimakasih. | ( opinion, aspect, sentiment ) </s> ( bagus, pelayanan, positive ) ; ( bagus, tempat, positive ) </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      ">> PREDS : kamar  baik <comma> kamar strategis nyaman untukcomma> dekat kembali baik jika kalau ada di acaraampung yang  lantaikan. erimakasih. | ( opinion, aspect, sentiment ) </s> ( bagus, tempat, positive ) ; ( bagus, tempat, positive ) </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-469\n",
      "Configuration saved in ./output/checkpoint-469/config.json\n",
      "Configuration saved in ./output/checkpoint-469/config.json\n",
      "Model weights saved in ./output/checkpoint-469/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-469/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-469/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-938\n",
      "Configuration saved in ./output/checkpoint-938/config.json\n",
      "Configuration saved in ./output/checkpoint-938/config.json\n",
      "Model weights saved in ./output/checkpoint-938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-938/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1407\n",
      "Configuration saved in ./output/checkpoint-1407/config.json\n",
      "Configuration saved in ./output/checkpoint-1407/config.json\n",
      "Model weights saved in ./output/checkpoint-1407/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1407/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1407/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-469] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> INPUTS : </s> pelayanan sudah bagus <comma> tempat juga bagus <comma> akan lebih bagus lagi jika tempat untuk mencharger hp di tambah. terimakasih. | ( opinion, aspect, sentiment ) </s> ( bagus, pelayanan, positive ) ; ( bagus, tempat, positive ) </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      ">> TARGETS : </s> pelayanan sudah bagus <comma> tempat juga bagus <comma> akan lebih bagus lagi jika tempat untuk mencharger hp di tambah. terimakasih. | ( opinion, aspect, sentiment ) </s> ( bagus, pelayanan, positive ) ; ( bagus, tempat, positive ) </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      ">> PREDS : kamar  baik tetapicomma> tetapinya nyaman untukcomma> tetapi kembali bagus kalau kalau ada tersebut resepampung alat tidak lengkap ke. erimakasih. | ( opinion, aspect, sentiment ) </s> ( bagus, pelayanan, positive ) ; ( bagus, tempat, positive ) </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-1876\n",
      "Configuration saved in ./output/checkpoint-1876/config.json\n",
      "Configuration saved in ./output/checkpoint-1876/config.json\n",
      "Model weights saved in ./output/checkpoint-1876/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-1876/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1876/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-938] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-2345\n",
      "Configuration saved in ./output/checkpoint-2345/config.json\n",
      "Configuration saved in ./output/checkpoint-2345/config.json\n",
      "Model weights saved in ./output/checkpoint-2345/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2345/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2345/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1407] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-2814\n",
      "Configuration saved in ./output/checkpoint-2814/config.json\n",
      "Configuration saved in ./output/checkpoint-2814/config.json\n",
      "Model weights saved in ./output/checkpoint-2814/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-2814/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2814/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1876] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-3283\n",
      "Configuration saved in ./output/checkpoint-3283/config.json\n",
      "Configuration saved in ./output/checkpoint-3283/config.json\n",
      "Model weights saved in ./output/checkpoint-3283/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3283/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3283/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2345] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-3752\n",
      "Configuration saved in ./output/checkpoint-3752/config.json\n",
      "Configuration saved in ./output/checkpoint-3752/config.json\n",
      "Model weights saved in ./output/checkpoint-3752/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-3752/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3752/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2814] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-4221\n",
      "Configuration saved in ./output/checkpoint-4221/config.json\n",
      "Configuration saved in ./output/checkpoint-4221/config.json\n",
      "Model weights saved in ./output/checkpoint-4221/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4221/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4221/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3283] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-4690\n",
      "Configuration saved in ./output/checkpoint-4690/config.json\n",
      "Configuration saved in ./output/checkpoint-4690/config.json\n",
      "Model weights saved in ./output/checkpoint-4690/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/checkpoint-4690/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4690/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3752] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4690, training_loss=0.18513852019808186, metrics={'train_runtime': 6293.9512, 'train_samples_per_second': 23.832, 'train_steps_per_second': 0.745, 'total_flos': 3.48262760448e+16, 'train_loss': 0.18513852019808186, 'epoch': 10.0})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n",
    "if resize_id:\n",
    "    model.resize_token_embeddings(len(tokenizer_id))\n",
    "model.to(device)\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_id,\n",
    "        data_collator = data_collator_en,\n",
    "        train_dataset = william_tok[\"hotel\"][\"train\"],\n",
    "        eval_dataset = william_tok[\"hotel\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_id,william_2[\"hotel\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:07<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [el + ' ' + tokenizer_id.sep_token for el in william_2[\"hotel\"][\"test\"][\"input\"]]\n",
    "str_preds = generate_predictions(model, tokenizer_id, test_data, device, decoding_args)\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_preds = [el.split(tokenizer_id.sep_token)[2] for el in str_preds]\n",
    "preds = [catch_answer(el,\"oas\") for el in str_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [catch_answer(el,\"oas\") for el in william_2[\"hotel\"][\"test\"][\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.6902916205241787,\n",
       " 'precision': 0.7961783439490446,\n",
       " 'f1_score': 0.7394636318341071}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_score(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result = save_result(str_preds, preds, targets, \"william_hotel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
